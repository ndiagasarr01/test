{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92214ccd",
   "metadata": {},
   "source": [
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.16.0\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---\n",
    "\n",
    "<!-- #region id=\"b5f7b35a\" -->\n",
    "# Table des matières\n",
    "1. [Classification par forêt aléatoire](#classification-par-forêt-aléatoire)\n",
    "  1. [Applications](#applications)\n",
    "1. [Voir l'arbre dans la forêt](#voir-larbre-dans-la-forêt)\n",
    "  1. [Exemple général en 2D](#exemple-général-en-2d)\n",
    "  1. [Exemple réel en 2D](#exemple-réel-en-2d)\n",
    "  1. [Principe de construction d'un arbre décisionnel en classification](#principe-de-construction-dun-arbre-décisionnel-en-classification)\n",
    "    1. [Qu'est-ce que l'entropie?](#quest-ce-que-lentropie)\n",
    "    1. [Le gain d'information](#le-gain-dinformation)\n",
    "    1. [Pourquoi est-ce que ça marche?](#pourquoi-est-ce-que-ça-marche)\n",
    "  1. [Forces et faiblesses des arbres décisionnels en classification](#forces-et-faiblesses-des-arbres-décisionnels-en-classification)\n",
    "1. [L'union fait la force: la forêt aléatoire](#lunion-fait-la-force-la-forêt-aléatoire)\n",
    "  1. [Le principe du classificateur par forêt aléatoire](#le-principe-du-classificateur-par-forêt-aléatoire)\n",
    "  1. [Forces et faiblesses des forêts aléatoires en classification](#forces-et-faiblesses-des-forêts-aléatoires-en-classification)\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"1b1c50bb\" -->\n",
    "Il y a beaucoup de similarité entre les forêts aléatoires utilisées dans les tâches de régression et celles\n",
    "utilisées en classification. Nous allons brièvement résumer ce qu'elles ont en commun avant de nous intéresser\n",
    "à celles utilisées en classification.\n",
    "\n",
    "Les forêts aléatoires constituent une méthode d'apprentissage par ensemble. Elles sont formées en combinant une\n",
    "multitude d'arbres décisionnels entraînés sur des jeux de données aléatoires tirés du jeu de données d'entraînement\n",
    "original $(X,y)$. L'aspect heuristique est doublement important dans cette approche pour les raisons suivantes;\n",
    "\n",
    "\n",
    "- les jeux de données, de tailles identiques, sont générés avec la méthode Bootstrap,\n",
    "- chaque jeu de données contient un sous-ensemble aléatoire, de taille constante, des facteurs\n",
    "$x_{i}$ utilisés dans l'ensemble original.\n",
    "\n",
    "\n",
    "En mode inférence, c'est-à-dire, lorsque l'on veut effectuer une prédiction avec des données jamais utilisées lors d'un\n",
    "entraînement, les prédictions des $n$ arbres sont combinées. La façon de combiner les réponses\n",
    "individuelles dépend du type d'application:\n",
    "\n",
    "\n",
    "- tâches de régression: la réponse $y$ est la moyenne des $n$ réponses prédites,\n",
    "- tâches de classification: la réponse $y$ est la classe majoritaire parmi les $n$ classes prédites.\n",
    "\n",
    "\n",
    "\n",
    "Beaucoup de problèmes de classification peuvent être abordés en utilisant la régression logistique\n",
    "décrite dans une section précédente. Elle est couramment utilisée en sciences sociales et en sciences de la vie.\n",
    "Ses performances sont bonnes lorsque les hypothèses sur lesquelles elle est\n",
    "basée sont vérifiées. Néanmoins, celles-ci sont plutôt restrictives dans la pratique.\n",
    "On pense ainsi à l'impossibilité de modéliser certains phénomènes à partir de facteurs $x_{i}$ hétéroclites\n",
    "variant de façon non continue et pouvant interagir entre eux. Le nombre d'interactions à considérer peut devenir\n",
    "très important. Les méthodes classiques comme la régression logistique requièrent de les spécifier dans\n",
    "leurs modèles, ce qui est rarement connu *a priori*. Comme on le verra plus loin, les forêts aléatoires\n",
    "peuvent gérer ces situations problématiques.\n",
    "\n",
    "> À noter que ce module ne contient pas d'exemple en Python. Le prochain module\n",
    "présente un exemple concret utilisant les forêts aléatoires.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"1b5f9900\" -->\n",
    "# <a id=applications>Applications</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"1f480464\" -->\n",
    "Les forêts aléatoires sont particulièrement utiles dans les applications de classification où l'on ne peut pas modéliser\n",
    "la probabilité d'appartenance à une classe donnée. C'est souvent le cas lorsque les facteurs mesurés $x_{i}$ sont hétéroclites.\n",
    "Par exemple en médecine, on essaie de diagnostiquer précocement la maladie d'Alzheimer chez un patient à partir\n",
    "d'échantillons de liquide céphalo-rachidien complétés par des tests d'évaluation moteurs et cognitifs. Chaque test\n",
    "permet de mesurer de multiples facteurs $x_{i}$, difficiles à relier entre eux, mais pouvant\n",
    "idéalement lever le voile sur une facette de la maladie.\n",
    "\n",
    "Voici quelques applications et problèmes où les forêts aléatoires sont utilisées:\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"dd084f15\" -->\n",
    "## Technologie:\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"5595ca81\" -->\n",
    "- Considérant les concentrations bactériennes et celles d'éléments traces dans un échantillon d'eau,\n",
    "celle-ci est-elle potable (OMS)?\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"82a21aa0\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/children-drinking-water.jpeg\"  width=\"400\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://pxhere.com/en/photo/878077</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"e17bcd0f\" -->\n",
    "- Identification du type de culture céréalière à partir d'images satellites (U.S. Geological Survey).\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"044d8193\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/vegetation-map.gif\"  width=\"400\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://seos-project.eu/agriculture/agriculture-c03-p05.html</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"857faab3\" -->\n",
    "## Sciences:\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"770ee4bc\" -->\n",
    "- Selon leur morphologie, on peut classifier chaque type de galaxie observée dans un immense région du ciel photographiée par un télescope (NASA).\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"3ad3a6e0\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/telescope-image.jpeg\"  width=\"400\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://www.flickr.com/photos/24354425@N03/44022429911/in/faves-jonathanospina/</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"f7c22de5\" -->\n",
    "## Médecine:\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"cf3d4a80\" -->\n",
    "- D'après les résultats d'un test d'urine, on peut déterminer si un patient est diabétique (NIH).\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"fc0a8fad\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/urine-test.jpeg\"  width=\"400\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://pxhere.com/en/photo/928823</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"089b3731\" -->\n",
    "## Finances:\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"20c3a83a\" -->\n",
    "- Un client va-t-il rembourser un emprunt d'ici cinq ans?\n",
    "- Un client sera-t-il satisfait de son achat?\n",
    "- Un achat sur une carte de crédit est-il frauduleux (BMO, Scotia Bank, etc.)?\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"7d062822\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/scamming-complaints.png\"  width=\"400\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://www.stonystratford.gov.uk/news/blog-article/scamming-complaints-on-the-rise-</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"b79fee0a\" -->\n",
    "# <a id=voir-larbre-dans-la-forêt>Voir l'arbre dans la forêt</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"a1528188\" -->\n",
    "Les forêts aléatoires qu'elles soient utilisées pour des applications en régression ou en\n",
    "classification sont basées sur le concept d'arbres décisionnels. Ceux-ci ont déjà été\n",
    "présentés dans le module sur la régression par forêt aléatoire.\n",
    "\n",
    "Dans ce qui suit, nous allons voir la simplicité d'utilisation des arbres décisionnels en classification.\n",
    "Toutefois, dans bien des situations, cette simplicité mène à des résultats trop peu précis pour être utilisée.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"de991f84\" -->\n",
    "## <a id=exemple-général-en-2d>Exemple général en 2D</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"3ef4da08\" -->\n",
    "Le panneau de droite de la figure suivante\n",
    "montre un exemple d'arbre décisionnel où l'on veut prédire la classe $y \\in \\{A, B, C, D\\}$ d'un item ou d'une\n",
    "observation en fonction de deux facteurs $x_{1}$ et $x_{2}$ mesurés.\n",
    "\n",
    "Le panneau du centre montre la zone d'influence de chaque classe dans le plan $x_{1}, x_{2}$. On remarque que\n",
    "les frontières de décisions séparant les zones d'influence sont parallèles aux axes des facteurs $X$. Chaque décision\n",
    "implique une séparation en deux de l'espace dans une zone rectangulaire précédente.\n",
    "\n",
    "Le panneau de gauche montre un résultat potentiellement plus utile, car plus complexe. Il est néanmoins\n",
    "impossible à obtenir avec un arbre décisionnel puisqu'il contredit l'observation précédente. Cela limite\n",
    "les possibilités des arbres décisionnels.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"cc5ed26b\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/decision-tree-example2.png\"  width=\"600\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: http://cedric.cnam.fr/vertigo/Cours/ml2/coursArbresDecision.html</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"6f626e3c\" -->\n",
    "## <a id=exemple-réel-en-2d>Exemple réel en 2D</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"2e913901\" -->\n",
    "La figure suivante montre un exemple d'arbre décisionnel servant à classifier deux types d'insectes, des sauterelles\n",
    "(*grasshopper*) et des katydides, en fonction de la longueur de l'abdomen $x_{1}$ et\n",
    "de la longueur des antennes $x_{2}$.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"20e1d607\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src=\"../images/decision-tree-example3.png\" width=\"400\">\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://medium.com/swlh/decision-tree-classification-de64fc4d5aac</font>\n",
    "    </div>\n",
    "</div>\n",
    "<p>&nbsp;</p>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"b356036f\" -->\n",
    "La figure suivante montre la zone d'influence de chaque classe. L'arbre de décision précédent a deux niveaux de décision,\n",
    "ce qui génère les trois zones dans la figure. Une correspond aux sauterelles (cercles) et les deux autres correspondent\n",
    "aux katydides (carrés).\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"d0294d97\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/decision-tree-example4.jpeg\"  width=\"400\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://medium.com/swlh/decision-tree-classification-de64fc4d5aac</font>\n",
    "    </div>\n",
    "</div>\n",
    "<p>&nbsp;</p>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"09233e3d\" -->\n",
    "Toute nouvelle observation d'une sauterelle ou d'une katydide peut être classifiée au moyen\n",
    "de ce diagramme comme le montre le point rouge basé sur les mensurations d'une sauterelle.\n",
    "\n",
    "Le problème avec cette approche est que toutes les frontières sont parallèles aux axes des facteurs\n",
    "$X$. Les décisions du classificateur sont faciles à expliquer, mais il n'est pas très précis, car il ne\n",
    "peut reproduire des frontières complexes qui sont la norme plutôt que l'exception.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"110bacca\" -->\n",
    "## <a id=principe-de-construction-dun-arbre-décisionnel-en-classification>Principe de construction d'un arbre décisionnel en classification</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"a6b78aba\" -->\n",
    "La méthode est très similaire à celle pour les arbres décisionnels en régression. Elle a été vue dans le module\n",
    "portant sur la régression par forêt aléatoire. Nous allons la répéter ici de façon simplifiée, car\n",
    "elle permet de faire le pont avec la version en classification.\n",
    "\n",
    "L'idée des arbres décisionnels en régression est que chaque noeud de l'arbre sert à séparer les\n",
    "données d'entraînement $(x_{1}, \\cdots, x_{n}, y)$ qui y parviennent en deux nouveaux ensembles, ses feuilles. Supposons, par simplicité, qu'il n'y a que deux variables $x_1$ et $x_2$. On sélectionne initialement $x_{1}$ et on impose un seuil donné $\\tau$. Toutes les valeurs d'entraînement $(x_{1}, x_{2}, y)$ pour lesquelles $x_1<\\tau$ seront dirigées vers la feuille de gauche, et les autres vers la feuille de droite. On calcule ensuite la variance des valeurs d'entraînement $\\sigma^2(y)$ dans chaque feuille, puis on calcule la somme des deux. Il faut ensuite\n",
    "déterminer pour quelle valeur de seuil $\\tau$ la somme des deux variances est minimale. On refait ensuite la même procédure avec toutes les valeurs d'entraînement, mais en utilisant, cette fois-ci, la variable $x_2$. Après cette seconde étape, on dispose de deux sommes de variances, une pour $x_1$ et une pour $x_2$. On sélectionne la variable ayant la somme des variances la plus faible. On sépare enfin les données avec cette variable et son seuil $\\tau$ associé.\n",
    "Cette étape vise à uniformiser un peu plus les données dans chaque feuille. On se retrouve à nouveau avec deux ensembles de données, un par feuille. On refait la même procédure avec toutes les feuilles de l'arbre jusqu'à ce que le nombre de données associées à chaque feuille atteigne un nombre minimal prédéterminé. La moyenne des valeurs de $y$ dans chaque feuille est la réponse de l'arbre de régression.\n",
    "\n",
    "Les arbres décisionnels en classification sont conçus de manière similaire. Ils visent plutôt à uniformiser les classes dans chaque feuille. C'est-à-dire, réduire le nombre de classes dans chacune. Ce qui change ici, c'est la fonction à maximiser lors du passage d'un nœud à ses deux feuilles. On maximise maintenant une fonction basée sur la théorie de l'information; le gain d'information. Avant de discuter de celui-ci, nous allons d'abord définir le concept\n",
    "d'entropie sur lequel il est basé.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"175e50d3\" -->\n",
    "### <a id=quest-ce-que-lentropie>Qu'est-ce que l'entropie?</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"4e7d2019\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/confused-girl-cartoon.png\"  width=\"250\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://freesvg.org/confused-cartoon-kid</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"cd8068ef\" -->\n",
    "Sa définition dépend du domaine dans lequel elle est utilisée. Elle tire son origine de la physique statistique.\n",
    "Toutefois, en informatique, l'entropie est une mesure de la quantité d'aléatoire dont dispose un système. En voici un exemple.\n",
    "\n",
    "Désordre minimal:\n",
    "Supposons qu'on a un sac contenant $n$ balles de la même couleur connue. On est donc certain que si on en pige une, elle aura cette couleur. Il n'y a qu'une classe; $p=1$ et l'entropie est nulle puisque\n",
    "\n",
    "$$\\begin{align}\n",
    "E &= -\\sum\\limits_{i=1}^{N_c}p^{(i)} \\log_{2}p^{(i)} \\\\\n",
    "&= -\\sum\\limits_{i=1}^{1} 1 \\log_{2}(1) \\\\\n",
    "&= 0.\n",
    "\\end{align}$$\n",
    "Cette situation requiert peu d'information pour décrire le contenu du sac; soit le nombre $n$ de balles et la couleur.\n",
    "\n",
    "Désordre maximal:\n",
    "Supposons maintenant que le sac contient $n$ balles de $n$ couleurs différentes. On n'a aucune certitude de la couleur\n",
    "de la balle que l'on va choisir. Il y a autant de classes que de balles ($p=1/n)$, l'entropie prend sa valeur maximale\n",
    "\n",
    "$$\\begin{align}\n",
    "E &= -\\sum\\limits_{i=1}^{N_c}p^{(i)} \\log_{2}p^{(i)} \\\\\n",
    "&= -\\sum\\limits_{i=1}^{n} \\frac{1}{n} \\log_{2}(\\frac{1}{n}) \\\\\n",
    "&= \\log_{2}n.\n",
    "\\end{align}$$\n",
    "Cette situation requiert beaucoup d'information pour décrire le contenu du sac; il faut spécifier les $n$ couleurs.\n",
    "\n",
    "Ainsi, plus les données sont uniformes, moins elles contiennent d'information et plus faible est l'entropie.\n",
    "Plus les données sont variées, plus elles contiennent d'information et plus grande est l'entropie.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"45254dcc\" -->\n",
    "### <a id=le-gain-dinformation>Le gain d'information</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"1087a997\" -->\n",
    "Le gain d'information remplace la somme des variances dans la construction de l'arbre de décision. Alors que la somme des variances devait être minimisée, le gain d'information doit être maximisé.\n",
    "\n",
    "L'idée maîtresse ici est de diviser un ensemble de données contenant plusieurs classes en deux ensembles contenant moins de classes par ensemble. La somme des deux entropies finales sera inférieure à l'entropie initiale.\n",
    "\n",
    "Soit un ensemble de $n$ données contenant par exemple trois classes $\\{A, B, C\\}$. L'entropie est une mesure de l'information contenue dans l'ensemble. Elle est définie comme suit\n",
    "\n",
    "$$E = -\\sum\\limits_{i=1}^{n_c}p^{(i)} \\log_{2}p^{(i)}$$\n",
    "\n",
    "où $p^{(i)}$ est la probabilité de chacune des $n_c$ classes constituant l'ensemble. Par exemple, si un noeud contient $n=15$ données réparties comme suit; 3 éléments de la classe A, 2 éléments de la classe B et 10 éléments de la classe C , alors on a $p=[3/15, 2/15, 10/15]$ et\n",
    "\n",
    "$$E_{\\text{noeud}} = -[\\frac{3}{15}\\log_{2}(\\frac{3}{15})+\\frac{2}{15}\\log_{2}(\\frac{2}{15})+\\frac{10}{15}\\log_{2}(\\frac{10}{15})].$$\n",
    "\n",
    "Si l'on répartit les données du nœud dans ses deux feuilles, avec $n = n_1 + n_2$, on peut calculer\n",
    "l'entropie de chacune\n",
    "\n",
    "$$\\begin{align}\n",
    "E_{\\text{feuille 1}} &= -\\sum\\limits_{i=1}^{n_c}p_{1}^{(i)} \\log_{2}p_{1}^{(i)} \\\\\n",
    "E_{\\text{feuille 2}} &= -\\sum\\limits_{i=1}^{n_c}p_{2}^{(i)} \\log_{2}p_{2}^{(i)}\n",
    "\\end{align}$$\n",
    "\n",
    "où $p_{1}^{(i)}$ et $p_{2}^{(i)}$ sont les probabilités des trois classes dans chaque feuille. La\n",
    "partition de l'ensemble original en deux ensembles a une entropie plus faible\n",
    "\n",
    "$$E_{\\text{feuilles}} = (\\dfrac{n_{1}}{n_{1}+n_{2}}) E_{\\text{feuille 1}} + (\\dfrac{n_{2}}{n_{1}+n_{2}}) E_{\\text{feuille 2}}$$\n",
    "\n",
    "Le gain d'information est défini comme suit\n",
    "\n",
    "$$GI = E_{\\text{noeud}} - E_{\\text{feuilles}}$$\n",
    "\n",
    "C'est la fonction que l'on veut maximiser en classification. On refait la même procédure itérativement pour chaque noeud/feuille de l'arbre jusqu'à ce que le nombre de données associées à chaque feuille atteigne un nombre minimal prédéterminé. Le gain d'information est calculé individuellement pour chaque noeud/feuille et non pour tous les noeuds/feuilles en même temps. À la fin, la classe la plus populeuse dans chaque feuille est la réponse $y$ de l'arbre de classification.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"17ea9653\" -->\n",
    "### <a id=pourquoi-est-ce-que-ça-marche>Pourquoi est-ce que ça marche?</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"4561368c\" -->\n",
    "Comme on l'a vu, l'entropie d'un ensemble de $n$ données similaires est plus faible que celle d'un ensemble de $n$ données variées. Si l'on veut uniformiser les classes dans les feuilles, il faut séparer les données de chaque noeud afin de minimiser l'entropie totale dans ses feuilles. En conséquence, le gain d'information, $GI$, doit augmenter. Idéalement, à la fin du processus, chaque feuille ne devrait contenir que des éléments d'une même classe.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"a68c1d79\" -->\n",
    "## <a id=forces-et-faiblesses-des-arbres-décisionnels-en-classification>Forces et faiblesses des arbres décisionnels en classification</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"f8736017\" -->\n",
    "#### Forces:\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"dfb90c29\" -->\n",
    "- Ils permettent l'identification des variables $x_{i}$ importantes; celles-ci sont les plus utilisées pour\n",
    "prendre des décisions à travers un arbre,\n",
    "- ils ont une grande interprétabilité; leurs décisions sont faciles à expliquer, \n",
    "- calculs très rapides dus à leur simplicité. \n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"55ed1522\" -->\n",
    "#### Faiblesses:\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"c4dac116\" -->\n",
    "- De petits changements dans les données d'entraînement peuvent beaucoup influencer la logique décisionnelle,\n",
    "- tendance au sur apprentissage,\n",
    "- les frontières de décisions sont parallèles aux axes des facteurs $X$,\n",
    "- les arbres complexes sont difficiles à interpréter et les décisions qu'ils prennent\n",
    "peuvent apparaître contre-intuitives.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"d1b1d8ad\" -->\n",
    "# <a id=lunion-fait-la-force-la-forêt-aléatoire>L'union fait la force: la forêt aléatoire</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"7628d4ea\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/union-fait-la-force.jpeg\"  width=\"500\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://www.rawpixel.com/image/322555/premium-psd-man-puzzle-jigsaw-colorfull-piece-holding/</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"ff82fc50\" -->\n",
    "Cette section est fortement inspirée de celle dans le module portant sur la méthode de régression par forêts aléatoires.\n",
    "Il y a néanmoins quelques subtilités entre elles. De plus, la familiarité avec un nouveau concept vient avec la\n",
    "répétition et l'usage.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"828a12ae\" -->\n",
    "## <a id=le-principe-du-classificateur-par-forêt-aléatoire>Le principe du classificateur par forêt aléatoire</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"bd5655b5\" -->\n",
    "Attention, utiliser trop de niveaux décisionnels donne une fausse impression de précision aux\n",
    "arbres décisionnels. En fait, c'est l'inverse qui se produit; ils deviennent plus fragiles dans leurs\n",
    "dernières branches, car l'ajout ou le retrait de quelques données à l'ensemble d'entraînement d'un arbre peut\n",
    "modifier sa structure finale et augmenter la variabilité de ses prédictions. La solution à ce dilemme consiste\n",
    "à combiner une multitude d'arbres décisionnels peu profonds et moins précis. Ce que l'on perd en précision pour\n",
    "chacun est largement compensé par ce que l'on gagne à les combiner dans un ensemble, la forêt.\n",
    "\n",
    "La figure suivante montre un exemple de forêt aléatoire combinant $n$ arbres décisionnels.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"7dd23ce3\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/random-forest-example2.jpeg\" width=\"600\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://www.researchgate.net/publication/338730476</font>\n",
    "    </div>\n",
    "</div>\n",
    "<p>&nbsp;</p>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"abc8ccca\" -->\n",
    "Chaque arbre n'utilise qu'un sous-ensemble aléatoire des facteurs $x_{i}$ disponibles; ses performances sont\n",
    "limitées pour cette raison. C'est un classificateur généralement moins performant (*weak classifier*)\n",
    "qu'un arbre les utilisant tous. Il a un biais et une variabilité supérieurs à l'arbre initial.\n",
    "On fait l'hypothèse que les erreurs d'un grand nombre de ces classificateurs vont se compenser mutuellement; le biais et la variabilité des résultats\n",
    "seront réduits par rapport à ceux de l'arbre initial.\n",
    "\n",
    "Un autre aspect heuristique est impliqué dans cette approche. En effet, bien que chaque arbre utilise\n",
    "le même nombre de données d'entraînement, les jeux d'entraînement sont tous\n",
    "différents entre eux. Ils sont générés avec la méthode d'échantillonnage de type *bootstrap*. Cette méthode consiste à créer de « nouveaux échantillons » statistiques, mais uniquement par tirage\n",
    "avec remise, à partir de l'échantillon initial [[Wikipedia](https://fr.wikipedia.org/wiki/Bootstrap_(statistiques))].\n",
    "Cette approche permet de simuler un plus grand nombre de jeux de données tout en respectant la distribution\n",
    "statistique du jeu original. Chaque nouveau jeu de données peut contenir des\n",
    "données dupliquées et manquer certaines des données originales. De plus, les facteurs $x_{i}$ sont\n",
    "différents d'un jeu à l'autre, mais leur nombre demeure le même. Par exemple, si le jeu original d'entraînement contenait\n",
    "1 000 données $(X,y)$ avec 20 facteurs, soit $X=\\{x_{1},\\cdots, x_{20}\\}$, chaque nouveau jeu d'entraînement contiendra\n",
    "1 000 données $(X',y')$ où $X'$ pourrait ne contenir que 10 facteurs $x_{i}$ choisis aléatoirement parmi les 20.\n",
    "\n",
    "\n",
    "\n",
    "Si l'on construisait un arbre décisionnel ($AD$) unique contenant l'ensemble des données d'entraînement $(X_{\\text{train}},y)$, la\n",
    "réponse lors de l'entraînement serait\n",
    "$$\\hat{y} = AD(X_{\\text{train}}|\\Theta)$$\n",
    "avec les paramètres $\\Theta$ définissant l'arbre. En mode d'inférence, la réponse pour une nouvelle donnée X serait\n",
    "$$y = AD(X|\\Theta).$$\n",
    "\n",
    "Dans l'exemple de forêt aléatoire ci-dessus, on rassemble plutôt $n$ arbres décisionnels utilisant chacun un jeu\n",
    "différent de données d'entraînement $(X_{\\text{train}}',y')$. On choisit ensuite la classe majoritaire parmi les $n$\n",
    "prédictions obtenues. La réponse lors de l'entraînement devient\n",
    "\n",
    "$$\\hat{y} = \\text{Vote} \\{AD(X_{\\text{train}}'|\\Theta')\\|$$\n",
    "avec les paramètres $\\Theta'$ définissant chaque arbre. En mode d'inférence, la réponse pour une nouvelle donnée X devient\n",
    "\n",
    "$$y = \\text{Vote} \\{AD(X'|\\Theta')\\|$$\n",
    "avec la sélection des facteurs $X'$ pour chaque arbre.\n",
    "\n",
    "La librairie Scikit-learn contient la classe [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "pour effectuer de la classification par forêt aléatoire. Plusieurs paramètres peuvent être ajustés tel que le nombre d'arbres,\n",
    "le nombre de facteurs $x_{i}$ à utiliser, le nombre de niveaux maximal de chaque arbre, etc.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"f163b72c\" -->\n",
    "## <a id=forces-et-faiblesses-des-forêts-aléatoires-en-classification>Forces et faiblesses des forêts aléatoires en classification</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"95b142e0\" -->\n",
    "#### Forces:\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"e28962c8\" -->\n",
    "- Plus précis que la plupart des classificateurs nonlinéaires,\n",
    "- performent souvent mieux que les réseaux de neurones,\n",
    "- robustesse due au grand nombre d'arbres décisionnels impliqués,\n",
    "- permettent d'estimer les valeurs discrètes manquantes dans un  jeu de données,\n",
    "- permettent d'identifier les facteurs $x_{i}$ les plus importants.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"326ba4dc\" -->\n",
    "#### Faiblesses:\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"f1b5d3e4\" -->\n",
    "- Temps d'inférence généralement plus longs que pour les autres types de classificateurs\n",
    "dus au grand nombre d'arbres décisionnels impliqués,\n",
    "- moins utiles pour les applications en temps réel pour la raison précédente,\n",
    "- résultats plus difficiles à interpréter dus au grand nombre d'arbres décisionnels impliqués.\n",
    "<!-- #endregion -->\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
