{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c6ab45",
   "metadata": {},
   "source": [
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    formats: ipynb,md\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.16.0\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---\n",
    "\n",
    "<!-- #region id=\"8288df97\" -->\n",
    "# Table des matières\n",
    "1. [Qu'est-ce que Scikit-learn?](#quest-ce-que-scikit-learn)\n",
    "1. [Une boîte à outils indispensable](#une-boîte-à-outils-indispensable)\n",
    "1. [Sélectionner le meilleur estimateur; le GPS du développeur](#sélectionner-le-meilleur-estimateur-le-gps-du-développeur)\n",
    "1. [Les exemples de la librairie](#les-exemples-de-la-librairie)\n",
    "1. [La philosophie de Scikit-learn](#la-philosophie-de-scikit-learn)\n",
    "1. [Méthodologie d'entraînement des modèles](#méthodologie-dentraînement-des-modèles)\n",
    "1. [Prétraitement des données](#prétraitement-des-données)\n",
    "1. [Exercice](#exercice)\n",
    "1. [Pour en savoir plus](#pour-en-savoir-plus)\n",
    "1. [Aide-mémoires](#aide-mémoires)\n",
    "\n",
    "# Attention!\n",
    "Ne lancez pas l'exécution automatique du notebook en entier en cliquant sur le bouton **Tout exécuter**. L'exécution serait interrompue, car certaines cellules exigent une entrée de votre part!\n",
    "\n",
    "Il faut simplement exécuter le notebook, une cellule à la fois, et entrer quelques lignes de code lorsque demandées. Il est inutile de sauter ces cellules pour aller aux suivantes car celles-ci ont justement besoin de votre input!\n",
    "\n",
    "Importons d'abord les librairies nécessaires.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ff1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc17232",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"1ea1c306\" -->\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/scikit-learn.png\"  width=\"500\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://radw2020.github.io/2016/11/01/Machine-Learning-con-Java/</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"aec7d7cc\" -->\n",
    "# <a id=quest-ce-que-scikit-learn>Qu'est-ce que Scikit-learn?</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"466d3096\" -->\n",
    "Scikit-learn est la librairie la plus utile et la plus robuste pour l'apprentissage automatique en Python. Elle fournit une sélection d'outils efficaces pour l'apprentissage automatique et la modélisation statistique, notamment pour:\n",
    "\n",
    "- la classification,\n",
    "- la régression,\n",
    "- le regroupement de données (*Data Clustering*),\n",
    "- la réduction de la dimensionnalité.\n",
    "\n",
    "La librairie, qui est en grande partie écrite en Python, est elle-même basée sur celles de\n",
    "\n",
    "- Numpy,\n",
    "- SciPy,\n",
    "- Matplotlib.\n",
    "\n",
    "\n",
    "À l'origine, elle s'appelait scikits.learn et elle a été initialement développée par David Cournapeau en tant que [projet d'été de code de Google](https://summerofcode.withgoogle.com/) en 2007!\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"a213b6c8\" -->\n",
    "# <a id=une-boîte-à-outils-indispensable>Une boîte à outils indispensable</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"e9f41362\" -->\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/toolbox.jpeg\"  width=\"350\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://radw2020.github.io/2016/11/01/Machine-Learning-con-Java/</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"5453cf00\" -->\n",
    "Plutôt que de se concentrer sur le chargement, la manipulation et la synthèse des données, la bibliothèque\n",
    "Scikit-learn se concentre sur la modélisation des données. Parmi les groupes de modèles les plus populaires fournis par Scikit-learn, on retrouve les suivants :\n",
    "\n",
    "**Algorithmes d'apprentissage supervisé**\n",
    "\n",
    "- la régression linéaire,\n",
    "- la machine à vecteurs de support,\n",
    "- l'arbre de décision,\n",
    "- la forêt aléatoire,\n",
    "- le perceptron multicouche (réseau de neurones très simple).\n",
    "\n",
    "\n",
    "**Algorithmes d'apprentissage non supervisé**\n",
    "\n",
    "- le regroupement des données (*Data Clustering*),\n",
    "- l'analyse factorielle,\n",
    "- l'analyse en composantes principales.\n",
    "\n",
    "\n",
    "**Regroupement des données**\n",
    "\n",
    "Cette approche, en apprentissage non supervisé, est utilisée pour regrouper des données non étiquetées. On y retrouve:\n",
    "\n",
    "- les k-moyennes,\n",
    "- le mélange de distributions gaussiennes,\n",
    "- le regroupement spectral,\n",
    "- la méthode DBSCAN.\n",
    "\n",
    "\n",
    "**Validation croisée**\n",
    "\n",
    "Elle est utilisée pour mesurer l'exactitude des modèles supervisés nouvellement entraînés, sur de nouvelles données. Elle est utilisée entre autres pour:\n",
    "\n",
    "- l'entraînement de modèles,\n",
    "- l'optimisation de modèles avec grilles d'hyperparamètres,\n",
    "- la sélection d'un modèle optimal parmi plusieurs,\n",
    "- l'entraînement d'un pipeline de prétraitement ou de traitement des données.\n",
    "\n",
    "\n",
    "**Réduction de la dimensionnalité**\n",
    "\n",
    "Elle est utilisée pour réduire le nombre d'attributs dans les données qui\n",
    "peuvent être ensuite utilisés pour la synthèse, la visualisation et la sélection de caractéristiques. On y retrouve:\n",
    "\n",
    "- l'analyse en composantes principales,\n",
    "- l'analyse par discriminants linéaires,\n",
    "- la méthode IsoMap,\n",
    "- l'échelonnage multidimensionnel métrique,\n",
    "- la méthode t-SNE.\n",
    "\n",
    "**Méthodes d'ensemble**\n",
    "\n",
    "Elles sont utilisées pour combiner les prédictions de plusieurs modèles supervisés. Elles rivalisent souvent avec les réseaux de neurones en matière de performances. On y retrouve:\n",
    "\n",
    "- la méthode AdaBoost,\n",
    "- le *gradient tree boosting*,\n",
    "- la méthode *extra trees*,\n",
    "- la forêt aléatoire.\n",
    "\n",
    "\n",
    "\n",
    "**Extraction de caractéristiques**\n",
    "\n",
    "Elle permet d'extraire des caractéristiques de données de types image et texte pour définir\n",
    "des attributs numériques. Par exemple, on s'en sert pour transformer un texte en\n",
    "tableau de valeurs numériques pouvant être utilisé ensuite dans un algorithme de classification.\n",
    "\n",
    "\n",
    "**Sélection de caractéristiques**\n",
    "\n",
    "Elle est utilisée pour identifier les variables utiles pour créer des modèles supervisés.\n",
    "On isole dans un espace de grande dimension un sous-ensemble de variables pertinentes. On cherche à minimiser la perte d'information venant de la suppression de toutes les autres variables. C'est une méthode de réduction de la dimensionnalité.\n",
    "\n",
    "\n",
    "**Détection d'anomalies**\n",
    "\n",
    "La détection d'anomalies est l'identification d'éléments, d'événements ou d'observations rares qui soulèvent des suspicions en différant de manière significative de la majorité des autres données. On y retrouve:\n",
    "\n",
    "- l'enveloppe elliptique,\n",
    "- la forêt d'isolement,\n",
    "- le facteur d'aberration local,\n",
    "- le SVM à une classe.\n",
    "\n",
    "\n",
    "**Open Source**\n",
    "\n",
    "C'est une bibliothèque libre de droits (*open source*) et également utilisable commercialement sous licence BSD.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"d68a07a1\" -->\n",
    "# <a id=sélectionner-le-meilleur-estimateur-le-gps-du-développeur>Sélectionner le meilleur estimateur; le GPS du développeur</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"d33ad8e1\" -->\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/estimator.png\"  width=\"400\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://pxhere.com/en/photo/1569701</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"f5899239\" -->\n",
    "Après avoir déterminé le type de problème sur lequel on travaille (régression? classification? regroupement des données? etc.), on est souvent pris au dépourvu lorsque vient le moment de sélectionner un modèle pour traiter nos données. Devrions-nous utiliser les plus populaires du moment, les plus faciles à comprendre, les plus performants? Il est recommandé de commencer par un algorithme simple pour voir si l'on comprend bien nos données et le problème à résoudre. Ce premier essai ne sera probablement pas bien performant, mais il va nous aider à résoudre beaucoup de problèmes avant de passer à la vitesse supérieure. C'est pourquoi il est fortement recommandé de consulter les exemples fournis par Scikit-learn; on en discute plus loin dans le module.\n",
    "\n",
    "Une fois cette étape réalisée, et tous les petits et grands problèmes réglés, on peut passer à des algorithmes plus puissants en suivant une logique basée sur l'expérience de milliers de développeurs avant nous.\n",
    "\n",
    "La figure suivante montre le [graphe des modèle](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html); c'est le GPS du développeur. Le lien web mène à la version interactive du graphe. En cliquant sur n'importe quel nom de méthode (en case verte), on accède à la page web décrivant celle-ci et ses applications, avec quelques exemples à la fin. Le lien menant à la fonction Python associée apparaît dans le paragraphe sous le titre. Toutes les fonctions n'apparaissent pas dans le graphe, mais vous disposez de suffisamment d'informations pour guider votre recherche.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"dd0bc579\" -->\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/scikit-cheat.png\"  width=\"900\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"e2382c4b\" -->\n",
    "La figure se lit comme suit. Supposons que l'on désire regrouper des données en trois groupes et qu'on dispose de 2 000 données multidimensionnelles $X=[x_1, x_2, \\cdots, x_n]$ où les $x_i$ sont des caractéristiques mesurées, sans réponses ou étiquettes $y$ associées. On doit prédire une catégorie, dans ce cas-ci le numéro du groupe associé à chaque donnée $X$. À partir du point de départ, on se rend dans la section *clustering*. La carte suggère d'utiliser la méthode *K-Means*. Si elle ne fonctionne pas bien avec nos données, on nous suggère d'essayer plutôt les méthodes de regroupement spectral ou de mélange de gaussiennes. Voilà!\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"3704e2ca\" -->\n",
    "# <a id=les-exemples-de-la-librairie>Les exemples de la librairie</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"72792a33\" -->\n",
    "La familiarité des méthodes vient avec l'usage. C'est pourquoi il est recommandé d'en essayer plusieurs et de les comparer entre elles afin d'en découvrir les forces et les faiblesses. Une bonne approche consiste à explorer la section [Exemples](https://scikit-learn.org/stable/auto_examples/index.html) de la librairie. C'est un incontournable.\n",
    "\n",
    "La figure suivante montre les premiers exemples de la série en classification.\n",
    "Explorez la diversité des exemples pour chaque méthode et soyez attentifs au code Python\n",
    "présent dans ceux-ci; on y apprend beaucoup de choses. Vous pourrez retravailler ceux-ci, expérimenter l'effet de divers paramètres des fonctions, ou expérimenter avec des jeux de données différents.\n",
    "\n",
    "Les exemples devraient être relativement faciles à réutiliser dans vos\n",
    "cahiers Jupyter. La principale difficulté rencontrée sera souvent due au formatage des données que vous\n",
    "devrez traiter. Doivent-elles être prétraitées? Devez-vous transformer vos données dans un\n",
    "DataFrame Pandas? Devez-vous éliminer quelques colonnes? Le prétraitement est souvent une étape à franchir\n",
    "parmi d'autres lors du démarrage d'un projet. Il faut s'y faire. Ce cours contient une\n",
    "série de modules consacrés aux diverses approches en prétraitement. Il faut y revenir de temps en temps\n",
    "jusqu'à ce que le sujet devienne familier.\n",
    "\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"c12dc949\" -->\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/classification.png\"  width=\"900\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://scikit-learn.org/stable/auto_examples/index.html/</font>\n",
    "    </div>\n",
    "</div>\n",
    "<p>&nbsp;</p>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"0acc553f\" -->\n",
    "Comme pour une encyclopédie, ou pour l'internet, il arrive souvent qu'un exemple mène à un autre, puis un\n",
    "autre, et ainsi de suite. Si c'est le cas, c'est que vous plongez graduellement dans l'univers de\n",
    "Scikit-learn. Une suite d'indices vous mènera éventuellement au but. L'exploration et les expérimentations sont le chemin le plus sûr vers les découvertes.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"ce970012\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/discovery.jpeg\"  width=\"600\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://culturacientifica.com/2017/01/07/naukas16-cuando-indiana-jones-se-astronomo//</font>\n",
    "    </div>\n",
    "</div>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"6af89fb5\" -->\n",
    "Un observateur averti remarquera que plusieurs des exemples de Scikit-learn ont servi de base pour\n",
    "créer certains des modules de ce cours; cela montre à quel point leur exploration est utile pour vous et ... pour nous!\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"9dc6287a\" -->\n",
    "# <a id=la-philosophie-de-scikit-learn>La philosophie de Scikit-learn</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"0b09a525\" -->\n",
    "Les concepteurs de la librairie ont oeuvré à maintenir une cohérence dans la façon dont on analyse les données. Il n'y a pas vingt méthodes pour traiter vingt estimateurs différents. Idéalement, il n'y en a qu'une ou deux, et préférablement une seule. La simplicité de la grammaire facilite la compréhension. Il faut être familier avec l'approche orientée objet qui est au coeur de la librairie. C'est assez différent du langage Matlab, par exemple, qui utilise un style de programmation plus classique. En fait, on peut dire la même chose des librairies Matplotlib et Seaborn décrites dans le module sur la visualisation des données.\n",
    "\n",
    "Dans cette partie, nous allons nous pencher sur l'approche systématique de Scikit-learn en apprentissage automatique. Chacun des points a été abordé à de multiples reprises dans les modules précédents. Néanmoins, il est utile de les revisiter en mettant cette fois-ci l'accent sur la méthode plutôt que sur ses applications.\n",
    "\n",
    "Nous allons revoir les points suivants qui comptent parmi les joyaux de la librairie:\n",
    "\n",
    "- la méthodologie d'entraînement des modèles,\n",
    "- le prétraitement des données,\n",
    "- l'optimisation d'un modèle,\n",
    "- les pipelines de traitement.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"44a35ae8\" -->\n",
    "On utilise un  jeu de données sur des échantillons de [vins](https://rowannicholls.github.io/python/data/sklearn_datasets/wine.html). Les données sont les résultats d'une analyse chimique de 178 vins cultivés dans la même région en Italie par trois vignerons différents. Les 13 caractéristiques mesurées pour chaque vin sont:\n",
    "\n",
    "\n",
    "- la concentration en alcool,\n",
    "- la concentration en acide malique,\n",
    "- la concentration en  cendres,\n",
    "- la concentration en  magnésium,\n",
    "- la concentration en  phénols totaux,\n",
    "- la concentration en  flavonoïdes,\n",
    "- la concentration en  phénols flavonoïdes,\n",
    "- la concentration en  proanthocyanidines,\n",
    "- l'intensité de la couleur, \n",
    "- la teinte, \n",
    "- le rapport DO280/DO315 des vins dilués, \n",
    "- la concentration en  proline.\n",
    "\n",
    "\n",
    "Le but de l'analyse est de construire un classificateur permettant d'identifier la provenance (le vigneron) de chaque échantillon de vin. C'est un problème de classification en 13-D et à trois classes.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"c485aa10\" -->\n",
    "# <a id=méthodologie-dentraînement-des-modèles>Méthodologie d'entraînement des modèles</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"84e7316b\" -->\n",
    "Dans cette section, nous allons voir la procédure utilisée pour entraînement et tester les performances de différentes méthodes de classification. On utiliserait une approche similaire pour les problèmes de régression et de regroupement des données. Aucun prétraitement n'est utilisé; c'est le sujet de la section suivante.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63662a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du jeu de données\n",
    "\n",
    "X, y = load_wine(return_X_y=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e004d6",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"bdf2013b\" -->\n",
    "On utilise un échantillonnage stratifié afin de respecter la distribution des types de vin. Le jeu de données étant assez facile à classifier, on rend la tâche plus ardue en n'utilisant que $25~\\%$ des données pour l'entraînement\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8401f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des données en ensembles d'entraînement ($25~\\%$ des données) et de test ($75~\\%$ restants).\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.75, stratify=y, random_state=seed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d8e213",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"60730220\" -->\n",
    "On utilise un classificateur par forêt aléatoire.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e2cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Entraînement du modèle avec les données d'entraînement\n",
    "\n",
    "model = rf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction du modèle avec les données de test\n",
    "\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d361f803",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"a21cab24\" -->\n",
    "Le rapport des performances en classification permet d'obtenir des statistiques sur l'exactitude, la précision, le rappel (*recall*) et le score $F_1$. Ces différentes métriques ont déjà été décrites entre autres dans le module sur les métriques de qualité en classification. Vous pouvez consulter ce module pour en savoir plus.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50696ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cae429",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"99e84e0b\" -->\n",
    "On répéte les étapes mais cette fois-ci on utilise un classificateur de type SVM.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db570dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.LinearSVC(random_state=0)\n",
    "model = clf.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901f52ea",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"b4d6524d\" -->\n",
    "La suite d'opérations est la même dans les deux cas. C'est la philosophie de standardisation des opérations\n",
    "de Scikit-learn; on s'y habitue rapidement et ça simplifie la programmation.\n",
    "\n",
    "On voit que les résultats pour le classificateur SVM sont moins bons que pour ceux de la forêt aléatoire.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"7e2c3140\" -->\n",
    "# <a id=prétraitement-des-données>Prétraitement des données</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"a9214034\" -->\n",
    "Les faibles performances du classificateur SVM sont dues au fait que les données n'étaient pas normalisées.\n",
    "C'est une étape importante dans le prétraitement des données. Le classificateur par forêt aléatoire n'est pas affecté par la normalisation.\n",
    "\n",
    "**Étapes de prétraitement**\n",
    "\n",
    "Il arrive souvent que plusieurs étapes de prétraitement soient nécessaires; imputation, normalisation, élimination des valeurs aberrantes (*Outliers*), etc. Bien que ça ne soit pas le cas avec notre jeu de données, on va néanmoins supposer qu'il contient $1~\\%$ de valeurs aberrantes ainsi que plusieurs valeurs manquantes. Nous allons effectuer les opérations suivantes:\n",
    "\n",
    "\n",
    "- élimination des valeurs aberrantes,\n",
    "- imputation des valeurs manquantes,\n",
    "- équilibrage des données déséquilibrées,\n",
    "- normalisation.\n",
    "\n",
    "\n",
    "Consultez les modules sur le prétraitement pour en savoir plus sur chacune des étapes. Malheureusement, on ne peut pas utiliser ici un pipeline pour les faire toutes ensemble; les estimateurs dans un pipeline doivent avoir les\n",
    "méthodes `fit` et `transform`. Ce n'est pas le cas des fonctions utilisées aux étapes 1 et 3.\n",
    "\n",
    "Il faut d'abord effectuer les étapes de nettoyage des données avant de les séparer en ensembles d'entraînement et de test.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d57c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture des données\n",
    "\n",
    "X, y = load_wine(return_X_y=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe7762b",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"385f9f8d\" -->\n",
    "On utilise la méthode de la forêt d'isolation à cette fin. On élimine $1~\\%$ des valeurs aberrantes au maximum. \n",
    "Cette méthode identifie les valeurs aberrantes en se basant sur la distance entre les données. Puisque les caractéristiques X \n",
    "ont des unités et des ordres de grandeurs différents, il faut les normaliser.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c20b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimateur utilisé.\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(X)\n",
    "\n",
    "# Normalisation de l'ENSEMBLE des données originales. \n",
    "X_s = scaler.transform(X)\n",
    "\n",
    "# Identification des valeurs aberrantes.\n",
    "i_forest = IsolationForest(contamination=0.01, random_state=seed)\n",
    "\n",
    "# Trouve les indices des valeurs aberrantes; la variable aberr vaut -1\n",
    "# pour les valeurs aberrantes et +1 pour les autres.\n",
    "aberr = i_forest.fit(X_s).predict(X_s)\n",
    "\n",
    "# Élimination des valeurs aberrantes à partir des données ORIGINALES.\n",
    "indices = np.where(aberr == -1)\n",
    "X_a = np.delete(X, indices, axis=0)\n",
    "y_a = np.delete(y, indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c8a676",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"6f2511f5\" -->\n",
    "Les valeurs manquantes de la matrice des caractéristiques $X$ sont interpolées à partir des autres. On utilise la\n",
    "méthode MICE décrite dans le module sur l'imputation. On suppose que les valeurs manquantes sont indiquées\n",
    "par des valeurs nulles.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db07042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mice = IterativeImputer(missing_values=0).fit_transform(X_a)\n",
    "y_mice = y_a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d42f74",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"1f1af773\" -->\n",
    "Combien y a-t-il de vins pour chaque producteur?\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ccca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unique, i = np.unique(y_mice, return_counts=True)\n",
    "\n",
    "print(\"Producteur #:\", y_unique)\n",
    "print(\"Nombre de vins:\", i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13fb597",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"e26662e5\" -->\n",
    "On voit qu'il y a un débalancement des classes. On va équilibrer celles-ci avec la méthode SMOTE multiclasse.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce503be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sm, y_sm = SMOTE().fit_resample(X_mice, y_mice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af5bc95",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"73c38420\" -->\n",
    "Testons à nouveau le nombre de vins pour chaque producteur.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a17cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unique, i = np.unique(y_sm, return_counts=True)\n",
    "\n",
    "print(\"Producteur #:\", y_unique)\n",
    "print(\"Nombre de vins:\", i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bffcdc",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"d3b632fb\" -->\n",
    "Les classes sont maintenant balancées. Un échantillonnage stratifié n'est plus nécessaire puisque les classes sont balancées.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17672b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sm, y_sm, test_size=0.75, random_state=seed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07177da",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"65de417c\" -->\n",
    "On peut maintenant normaliser les données pour l'entraînement du modèle. Attention: il faut toujours entraîner l'estimateur sur l'ensemble d'entraînement puis appliquer la transformation sur les deux ensembles.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba77a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimateur utilisé\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(X_train)\n",
    "\n",
    "X_train_n = scaler.transform(X_train)\n",
    "X_test_n = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1403d29e",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"1d9aa6b7\" -->\n",
    "On revient à nos deux modèles précédents, la forêt aléatoire et la méthode SVM. Cette fois-ci avec des données prétraitées.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddc910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classificateur par forêt aléatoire\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "model = rf.fit(X_train_n, y_train)\n",
    "y_pred = model.predict(X_test_n)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e4a3b",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"36f84e79\" -->\n",
    "Les performances sont légèrement supérieures aux précédentes obtenues avec la forêt aléatoire sans prétraîtement des données.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad98df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classificateur SVM\n",
    "\n",
    "clf = svm.LinearSVC(random_state=0)\n",
    "model = clf.fit(X_train_n, y_train)\n",
    "y_pred = model.predict(X_test_n)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57432194",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"5f4243c0\" -->\n",
    "Les nouvelles valeurs des métriques de performances pour la méthode SVM sont bien meilleures qu'auparavant\n",
    "et quasiment équivalentes à celles obtenues avec la forêt aléatoire. Cet exemple montre la pertinence du prétraitement et de la facilité de son implémentation avec Scikit-learn.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"72f03102\" -->\n",
    "# <a id=exercice>Exercice</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"01046208\" -->\n",
    "Dans cet exemple, nous allons utiliser les données prétraitées précédentes. Nous allons réduire la dimensionnalité de l'espace des caractéristiques avec l'analyse en composantes principales, puis choisir entre les deux classificateurs précédents également optimisés.\n",
    "\n",
    "Nous allons optimiser le pipeline en faisant varier les facteurs suivants:\n",
    "\n",
    "- Réduction de la dimensionnalité PCA:\n",
    "    - `n_components` $= [2, 3]$\n",
    "- Classificateur forêt aléatoire:\n",
    "    - `n_estimators` $= [10, 30, 50]$,\n",
    "    - `max_depth`: $= [1, 3, 5]$,\n",
    "    - `min_samples_split`: $= [2, 3, 4]$.\n",
    "- Classificateur SVM:\n",
    "    - `C` $= [0.01, 0.1, 1, 10, 100]$.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"ea97222c\" -->\n",
    "Puisqu'il y a deux classificateurs, la grille des paramètres doit contenir deux dictionnaires de paramètres.\n",
    "Quelle serait la définition du pipeline?\n",
    "\n",
    "Revisitez le module de prétraitement des données portant sur la conception de pipelines. Vous y trouverez des pistes de solution.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = # À remplir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962196fb",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"e01f1013\" -->\n",
    "Entraînez maintenant le pipeline et affichez les hyperparamètres optimaux.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f7f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# À remplir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca098cc",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"a502a7e5\" -->\n",
    "Comment interprétez-vous les résultats?\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"6cd504ed\" -->\n",
    "Affichez les métriques de performances avec la fonction `classification_report` puis les valeurs d'exactitude pour les deux ensembles.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa7a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# À remplir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ce5cb",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"bec601e4\" -->\n",
    "Comment interprétez-vous les résultats?\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"81928674\" -->\n",
    "# <a id=pour-en-savoir-plus>Pour en savoir plus</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"5d3270bd\" -->\n",
    "Les sources d'informations sur Scikit-learn foisonnent. En autant que possible, il faut se référer aux\n",
    "plus récentes d'entre elles.\n",
    "\n",
    "Prenez soin de vérifier la date de publication des livres de référence et leur numéro d'édition. Non seulement les méthodes en apprentissage automatique évoluent rapidement, mais le code en Python dans les vieilles éditions n'est plus compatible avec les versions actuelles des librairies. On finit par ne plus consulter ses livres et leur préférer les sites web des librairies.\n",
    "\n",
    "Les langages évoluent ainsi que la façon de les écrire. Jetez un coup d'oeil à de vieux programmes en Python. On comprend encore ce qu'ils font, mais on aurait de la difficulté à les modifier sans faire d'erreur.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"265ea1d0\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/evolution.png\"  width=\"800\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://www.metmuseum.org/art/collection/search/326711;  https://en.wikipedia.org/wiki/Gothic_Bible; https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html</font>\n",
    "    </div>\n",
    "</div>\n",
    "<p>&nbsp;</p>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"99f01a64\" -->\n",
    "[Hands-On Machine Learning with Scikit-learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems (2019)](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"f6cde784\" -->\n",
    "[Python Machine Learning - Second Edition: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow (2019)](https://www.packtpub.com/product/python-machine-learning-second-edition/9781787125933)\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"0cfdb244\" -->\n",
    "# <a id=aide-mémoires>Aide-mémoires</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"afad53fb\" -->\n",
    "Comme c'était le cas avec les librairies [`matplotlib`](https://matplotlib.org/) et [`Seaborn`](https://seaborn.pydata.org/),\n",
    "il y a tellement de fonctions dans Scikit-learn qu'ont tend à les confondre.\n",
    "\n",
    "Voici trois sources d'information présentant les principales méthodes et leurs bons usages. À conserver près de soi, ou dans la fenêtre de bureau de son ordinateur. Pas de tatouage svp; cela prendrait plus qu'un bras pour les afficher!\n",
    "\n",
    "Le premier lien mène vers un site proposant une collection d'une dizaine d'aide-mémoires. Chacun a été créé par un\n",
    "spécialiste en apprentissage automatique ou par une organisation telle que Scikit-learn.\n",
    "* https://blog.finxter.com/scikit-learn-cheat-sheets/\n",
    "\n",
    "Parmi ceux proposés dans la liste, les deux suivants reviennent souvent dans les réponses aux questions que l'on retrouve sur le web:\n",
    "* https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf\n",
    "* https://intellipaat.com/mediaFiles/2018/12/Sklearn-cheat-sheet.png\n",
    "\n",
    "Bien que mentionnée précédemment, la [carte interactive des estimateurs](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
    "est particulièrement utile lorsque vous connaissez déjà Scikit-learn, mais que vous cherchez à déterminer le meilleur algorithme pour traiter vos données.\n",
    "<!-- #endregion -->\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
