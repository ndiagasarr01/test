{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf9c54c",
   "metadata": {},
   "source": [
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.16.0\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---\n",
    "\n",
    "<!-- #region id=\"89902a6f\" -->\n",
    "# Table des matières\n",
    "1. [Méthodes de régularisation fréquemment utilisées](#Méthodes-de-régularisation-fréquemment-utilisées)\n",
    "  1. [Régularisation en ajoutant des contraintes à la méthode des moindres carrés](#Régularisation-en-ajoutant-des-contraintes-à-la-méthode-des-moindres-carrés)\n",
    "1. [Préparation des données et initialisations](#Préparation-des-données-et-initialisations)\n",
    "1. [Exemples de régression à un polynôme de degré donné](#Exemples-de-régression-à-un-polynôme-de-degré-donné)\n",
    "1. [Exemples de régression à des polynômes de degrés variés](#Exemples-de-régression-à-des-polynômes-de-degrés-variés)\n",
    "1. [Affichage des courbes recalées avec les deux méthodes de régression](#Affichage-des-courbes-recalées-avec-les-deux-méthodes-de-régression)\n",
    "1. [Exercices](#exercices)\n",
    "\n",
    "# Attention!\n",
    "Ne lancez pas l'exécution automatique du notebook en entier en cliquant sur le bouton **Tout exécuter**. L'exécution serait interrompue, car certaines cellules exigent une entrée de votre part!\n",
    "\n",
    "Il faut simplement exécuter le notebook, une cellule à la fois, et entrer quelques lignes de code lorsque demandées. Il est inutile de sauter ces cellules pour aller aux suivantes car celles-ci ont justement besoin de votre input!\n",
    "\n",
    "Importons d'abord les librairies nécessaires.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b8ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.linear_model import (\n",
    "    ElasticNet,\n",
    "    ElasticNetCV,\n",
    "    Lasso,\n",
    "    LassoCV,\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    RidgeCV,\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    ShuffleSplit,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620cbfce",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"8c18324e\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/regularization-illustration.png\"  width=\"350\" />\n",
    "    <div>\n",
    "    <font size=\"1.5\">Image Source: https://commons.wikimedia.org/wiki/File:Regularization.svg</font>\n",
    "    </div>\n",
    "</div>\n",
    "<p>&nbsp;</p>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"2081e838\" -->\n",
    "On a vu dans le module sur le sous-apprentissage et le surapprentissage que les méthodes de régression\n",
    "et de classification sont sensibles au bruit dans les données. Il existe plusieurs méthodes permettant\n",
    "de réduire l'effet du bruit dans les analyses. Elles sont connues sous le nom de régularisation,\n",
    "car elles ont pour tâche de régulariser, c'est-à-dire, de mieux contrôler l'analyse des données.\n",
    "\n",
    "Comme pour le module précédent, nous allons nous concentrer sur les méthodes utilisées en régression\n",
    "et allons utiliser des modèles polynomiaux pour recaler des données expérimentales. Ils sont faciles à utiliser et les\n",
    "résultats de la régularisation sont faciles à interpréter.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"fa794ab8\" -->\n",
    "# <a id=Méthodes-de-régularisation-fréquemment-utilisées>Méthodes de régularisation fréquemment utilisées</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"7e820681\" -->\n",
    "Il existe plusieurs méthodes de régularisation en apprentissage automatique. Les plus connues sont probablement les\n",
    "suivantes:\n",
    "\n",
    "- l'augmentation du nombre de données,\n",
    "- la réduction de la complexité d'un modèle,\n",
    "- l'ajout de contraintes à la méthode des moindres carrés.\n",
    "\n",
    "\n",
    "La première méthode est la plus intuitive et la plus facile à essayer. Il suffit de faire l'acquisition de plus\n",
    "de données lorsque le budget d'un projet le permet. Si cela n'est pas possible, on peut faire de l'augmentation\n",
    "de données à partir de données existantes. C'est une technique largement utilisée en apprentissage profond.\n",
    "\n",
    "La seconde méthode consiste à simplifier le modèle à recaler sur les données. On peut prendre par exemple un polynôme\n",
    "de faible degré qui varie lentement, sans les grandes oscillations caractérisant ceux de degré élevé. La simplicité est\n",
    "une bonne chose, mais un modèle trop simple ne tire pas parti de nos données.\n",
    "\n",
    "La troisième méthode consiste à ajouter des contraintes à la méthode des moindres carrés. Nous allons\n",
    "nous concentrer sur cette approche dans ce module.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"7fa960e5\" -->\n",
    "## <a id=Régularisation-en-ajoutant-des-contraintes-à-la-méthode-des-moindres-carrés>Régularisation en ajoutant des contraintes à la méthode des moindres carrés</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"aa6bd3ba\" -->\n",
    "Supposons que l'on veut estimer les paramètres $a_j$ du modèle 1-D suivant\n",
    "\n",
    "$$y = h(x|\\Theta)$$\n",
    "\n",
    "où $\\Theta$ est l'ensemble des paramètres $a_j$.\n",
    "On veut recaler le modèle à des données expérimentales $(x^{(i)}, y^{(i)})$\n",
    "sans erreurs de mesures. L'indice $i$ représente le numéro d'une donnée tel que $1\\le i \\le n$, où $n$ est le nombre de données. On a vu dans un module précédent qu’on peut estimer ces\n",
    "coefficients avec la méthode des moindres carrés qui minimise la\n",
    "métrique de l'erreur quadratique moyenne (MSE) définie comme suit\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1} ^{n}(y^{(i)}-h(x^{(i)}|\\Theta))^2$$\n",
    "\n",
    "La méthode des moindres carrés est particulièrement utile pour estimer les paramètres $a_j$ des\n",
    "modèles linéaires dans les paramètres comme les suivants par exemple:\n",
    "\n",
    "Modèle multilinéaire $N$-D:\n",
    "$$y = a_0 + a_1 x_1 + \\cdots + a_{N} x_{N}.$$\n",
    "\n",
    "Modèle polynomial $1$-D:\n",
    "$$y = a_0 + a_1 x + a_2 x^2 + \\cdots + a_{N} x^{N}.$$\n",
    "\n",
    "Un problème classique avec la méthode des moindres carrés se produit lorsqu'un modèle contient\n",
    "trop de coefficients $a_j$; l'ajustement de chacun tend à reproduire la distribution\n",
    "des données bruitées plutôt que l'aspect général de la courbe sous-jacente. De plus,\n",
    "les valeurs des coefficients deviennent arbitrairement grandes en valeurs absolues ce qui\n",
    "limite l'interprétation des résultats.\n",
    "\n",
    "La régularisation ajoute une contrainte dans le problème des moindres carrés. On définit une\n",
    "nouvelle fonction à minimiser, la fonction de perte (*Loss*) qui remplace la MSE.\n",
    "En voici quelques exemples:\n",
    "\n",
    "$$\\begin{align}\n",
    "L_{\\text{Standard}} &= \\text{MSE} \\\\\n",
    "L_{\\text{Lasso}} &= \\text{MSE} + \\lambda \\|\\Theta\\|^1 \\\\\n",
    "L_{\\text{Ridge}} &= \\text{MSE} + \\lambda \\|\\Theta\\|^2 \\\\\n",
    "L_{\\text{Elastic Net}} &= \\text{MSE} + \\lambda_1 \\|\\Theta\\|^1 + \\lambda_2 \\|\\Theta\\|^2 \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "avec les normes $\\|\\Theta\\|^1$ et $\\|\\Theta\\|^2$ qui sont calculées à partir des coefficients des modèles\n",
    "$$\\|\\Theta\\|^1 = \\sum_{j=0} ^{N}|a_j |$$\n",
    "$$\\|\\Theta\\|^2 = \\sum_{j=0} ^{N}a_j^2.$$\n",
    "\n",
    "Les paramètres $\\lambda$ sont positifs. On voit que la minimisation de la fonction de\n",
    "perte $L$ implique à la fois la minimisation de la MSE et des valeurs des coefficients $a_j$.\n",
    "\n",
    "Dans ce qui suit, nous n'allons discuter que de la méthode Ridge par souci de simplicité. Notons\n",
    "toutefois les points suivants:\n",
    "\n",
    "\n",
    "- la méthode Ridge est utilisée lorsque toutes les variables $x_j$ et leurs interactions $x_j x_k$ sont considérées importantes,\n",
    "\n",
    "- la méthode Lasso est utilisée lorsque l'on a beaucoup de variables et que l'on se doute que plusieurs d'entre elles.\n",
    "ne sont pas importantes,\n",
    "\n",
    "- \n",
    "dans l'hésitation, on choisit la méthode Elastic Net qui est une combinaison des deux.\n",
    "\n",
    "\n",
    "Les trois méthodes mettent à zéro (Lasso) ou près de zéro (Ridge et Elastic Net) les coefficients $a_j$ des variables\n",
    "$x_j$ non importantes. Cela facilite l'interprétation des modèles en régression.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"fdc341bd\" -->\n",
    "# <a id=Préparation-des-données-et-initialisations>Préparation des données et initialisations</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"4d79105e\" -->\n",
    "Définissons d'abord quelques fonctions qui seront utilisées à plusieurs reprises. Par la suite, nous allons générer le vrai \n",
    "signal idéal que l'on va utiliser pour générer les données. Ce signal sera généré selon la fonction sinusoïdale suivante\n",
    "\n",
    "$$y = 100\\sin(10x).$$\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération du signal idéal\n",
    "def modele(x):\n",
    "    y = 100 * np.sin(10 * x)\n",
    "    return y\n",
    "\n",
    "\n",
    "# Génération d'échantillons du signal idéal et ajout de bruit gaussien.\n",
    "# Les valeurs de x sont réparties aléatoirement sur la plage des valeurs\n",
    "# disponibles.\n",
    "def genere_signal_bruité(N, sigma):\n",
    "    x = np.random.uniform(x_min, x_max, N)\n",
    "    y = modele(x) + np.random.normal(0.0, sigma, N)\n",
    "\n",
    "    df = pd.DataFrame({\"X\": x, \"Y\": y})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bcbac1",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff05921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération du signal idéal pour des valeurs de x réparties uniformément\n",
    "\n",
    "x_min = 0\n",
    "x_max = 1\n",
    "\n",
    "xx = np.linspace(x_min, x_max, 100)[:, np.newaxis]\n",
    "yy = modele(xx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0def0be6",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération du signal expérimental (c.-à-d. bruité) pour des valeurs de x réparties aléatoirement\n",
    "\n",
    "sigma = 60  # Niveau de bruit\n",
    "N = 100  # Nombre de points du signal\n",
    "df = genere_signal_bruité(N, sigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988535a",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"5d0aafc2\" -->\n",
    "La prochaine cellule définit une fonction qui effectue 100 fois les opérations suivantes pour un modèle de régression donné:\n",
    "\n",
    "- séparation des données en ensembles d'entraînement et de test,\n",
    "- entraînement du modèle avec l'ensemble d'entraînement, \n",
    "- prédiction de la réponse y avec les données d'entraînement,\n",
    "- calcul des métriques $R^2$ et MSE,\n",
    "- prédiction de la réponse y avec les données de test,\n",
    "- calcul des métriques $R^2$ et MSE.\n",
    "\n",
    "Les valeurs moyennes des métriques sont ensuite calculées pour les deux ensembles. Le nombre de répétitions\n",
    "permet d'obtenir de meilleures statistiques.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b57205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculeMetriques(models, data, iterations=100):\n",
    "    X = data[\"X\"].to_numpy().reshape(-1, 1)\n",
    "    Y = data[\"Y\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    results = {}\n",
    "    for i in models:\n",
    "        r2_train = []\n",
    "        r2_test = []\n",
    "        mse_train = []\n",
    "        mse_test = []\n",
    "\n",
    "        for j in range(iterations):\n",
    "            # Séparation des données en ensembles d'entraînement et de test\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "            # ------- Ensemble d'entraînement------\n",
    "            # Entraînement du modèle\n",
    "            models[i].fit(X_train, y_train)\n",
    "\n",
    "            # Prédiction des valeurs de y\n",
    "            y_pred = models[i].predict(X_train)\n",
    "\n",
    "            # Calcul des métriques R2 et MSE\n",
    "            r2_train.append(metrics.r2_score(y_train, y_pred))\n",
    "            mse_train.append(metrics.mean_squared_error(y_train, y_pred))\n",
    "\n",
    "            # ------- Ensemble de test------\n",
    "            # Prédiction des valeurs de y\n",
    "            y_pred = models[i].predict(X_test)\n",
    "\n",
    "            # Calcul des métriques R2 et MSE\n",
    "            r2_test.append(metrics.r2_score(y_test, y_pred))\n",
    "            mse_test.append(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "        # Calcul des valeurs moyenne des métriques R2 et MSE pour les\n",
    "        # deux ensembles.\n",
    "        results[i] = [\n",
    "            np.mean(r2_train),\n",
    "            np.mean(r2_test),\n",
    "            np.mean(mse_train),\n",
    "            np.mean(mse_test),\n",
    "        ]\n",
    "        df = pd.DataFrame(\n",
    "            results, index=[\"r2_train\", \"r2_test\", \"mse_train\", \"mse_test\"]\n",
    "        )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c049fed",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"c98c07e8\" -->\n",
    "# <a id=Exemples-de-régression-à-un-polynôme-de-degré-donné>Exemples de régression à un polynôme de degré donné</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"36cda29b\" -->\n",
    "Nous allons comparer les performances des deux méthodes de régression, soit la méthode standard et\n",
    "la méthode Ridge. Alors que la première ajuste les coefficients $a_i$ des polynômes pour minimiser la MSE,\n",
    "la méthode Ridge minimise en même temps les valeurs des mêmes paramètres. On a ainsi\n",
    "\n",
    "$$\\begin{align}\n",
    "L_{\\text{Standard}} &= \\text{MSE} \\\\\n",
    "L_{\\text{Ridge}} &= \\text{MSE} + \\lambda \\sum_{j=0}^{N}a_j^2 \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Lorsque la méthode standard est utilisée, les coefficients $a_j$ tendent à devenir très positifs ou très négatifs lorsqu'on essaie de recaler un polynôme de degré élevé à des données expérimentales. C'est le phénomène du surapprentissage. La régularisation empêche les valeurs des coefficients d'exploser.\n",
    "\n",
    "Dans ce qui suit, pour simplifier l'analyse des données, nous allons concevoir des pipelines implémentant\n",
    "toutes les étapes menant à la régression des données. Les deux pipelines sont très similaires\n",
    "à une exception près; la méthode de recalage (*fit*).\n",
    "\n",
    "Chaque pipeline comprend la création des vecteurs de caractéristiques\n",
    "\n",
    "$$X = \\begin{pmatrix} \\bf{1} & \\bf{x} & \\bf{x^2} & \\cdots & \\bf{x^N}  \\end{pmatrix}$$\n",
    "\n",
    "où $N$ est le degré d'un polynôme puis leur normalisation, et enfin la régression.\n",
    "\n",
    "À titre d'exemple, nous allons effectuer une régression avec des polynômes de degré 12.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d245dafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline pour la régression standard des moindres carrés\n",
    "pipe1 = Pipeline(\n",
    "    [\n",
    "        (\"poly\", PolynomialFeatures(degree=12)),\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"fit\", linear_model.LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Pipeline pour la régression avec la méthode Ridge\n",
    "pipe2 = Pipeline(\n",
    "    [\n",
    "        (\"poly\", PolynomialFeatures(degree=12)),\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"fit\", linear_model.Ridge()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "models1 = {\"Standard\": pipe1, \"Ridge\": pipe2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a2218e",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"c4bf6032\" -->\n",
    "Régression des données avec les deux pipelines et affichage des métriques $R^2$ et MSE pour les ensembles d'entraînement et de test.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f24c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculeMetriques(models1, df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21d33f",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"285d6a2b\" -->\n",
    "Comme attendu, on voit que pour chaque méthode, les métriques de performances $R^2$ et MSE sont meilleures\n",
    "avec l'ensemble d'entraînement qu'avec l'ensemble de test. Si l'on compare maintenant les deux méthodes\n",
    "entre elles, on voit que la méthode Ridge performe nettement moins bien que la méthode standard.\n",
    "La raison en est que l'algorithme de la classe `Ridge` de Scikit-learn utilise une valeur par défaut\n",
    "(et non optimale dans ce cas-ci) pour le paramètre $\\lambda$.\n",
    "\n",
    "Nous allons reprendre l'exemple en spécifiant, cette fois-ci, une plage de valeurs possibles de $\\lambda$ et utiliser\n",
    "l'optimisation sur grille de paramètres avec la fonction GridSearchCV. Cela veut dire que la régression précédente\n",
    "avec la méthode Ridge sera effectuée pour chaque valeur de $\\lambda$ et que le meilleur modèle sera utilisé pour\n",
    "calculer les métriques finales de performances $R^2$ et MSE.\n",
    "\n",
    "> À noter que la classe `Ridge` de Scikit-learn utilise un facteur $\\alpha$ plutôt que $\\lambda$, plus souvent mentionné dans la littérature.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ca298",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_params = [{\"fit__alpha\": np.logspace(-8, 0, num=9)}]\n",
    "models2 = {\n",
    "    \"Standard\": pipe1,\n",
    "    \"Ridge\": GridSearchCV(pipe2, param_grid=ridge_params, refit=True),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d3823d",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"4ab5d4a6\" -->\n",
    "Régression des données avec les deux pipelines et affichage des métriques $R^2$ et MSE pour les ensembles d'entraînement et de test\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf47711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculeMetriques(models2, df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f4d487",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"abfec00f\" -->\n",
    "Les résultats en test montrent que la méthode Ridge performe maintenant mieux que l'entraînement précédent, mais ne permet pas de mieux performer que la méthode standard pour les deux métriques MSE et $R^2$.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"cc4171c3\" -->\n",
    "# <a id=Exemples-de-régression-à-des-polynômes-de-degrés-variés>Exemples de régression à des polynômes de degrés variés</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"a0552174\" -->\n",
    "Nous allons reprendre l'exemple précédent, mais en faisant varier maintenant le degré du polynôme de\n",
    "régression. Ainsi, pour chaque polynôme, la valeur optimale du paramètre $\\lambda$ sera ajustée.\n",
    "\n",
    "Cela nous permettra de comparer les performances des deux méthodes de régression en fonction de la complexité du\n",
    "modèle que l'on veut recaler aux données.\n",
    "\n",
    "La fonction suivante permet de refaire, pour chaque degré de polynôme, les opérations précédentes.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34184535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_modeles(df, degre):\n",
    "    pipe1 = Pipeline(\n",
    "        [\n",
    "            (\"poly\", PolynomialFeatures(degree=degre)),\n",
    "            (\"scale\", StandardScaler()),\n",
    "            (\"fit\", linear_model.LinearRegression()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipe2 = Pipeline(\n",
    "        [\n",
    "            (\"poly\", PolynomialFeatures(degree=degre)),\n",
    "            (\"scale\", StandardScaler()),\n",
    "            (\"fit\", linear_model.Ridge()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    ridge_params = [{\"fit__alpha\": np.logspace(-8, 0, num=9)}]\n",
    "    modeles = {\n",
    "        \"Standard\": pipe1,\n",
    "        \"Ridge\": GridSearchCV(pipe2, param_grid=ridge_params, refit=True),\n",
    "    }\n",
    "\n",
    "    return calculeMetriques(modeles, df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1087e924",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"824fd814\" -->\n",
    "Nous allons maintenant calculer les métriques de performances $R^2$ et MSE pour des polynômes\n",
    "d'ordre variant entre 3 et 12.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e7087",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats1 = {}\n",
    "\n",
    "degres = range(3, 13)\n",
    "\n",
    "for i, degre in enumerate(degres):\n",
    "    print(f\"{i = }\")\n",
    "    stats1[i] = regression_modeles(df, degre)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8deb42",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"dba57a83\" -->\n",
    "La fonction suivante permet d'afficher les métriques de performances $R^2$ et MSE en fonction\n",
    "du degré des polynômes, soit la complexité du modèle de régression.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6795c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affiche_resultats(stats, degres):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "    # Extraction des valeurs de la métrique MSE\n",
    "    mse_train_1 = []\n",
    "    mse_test_1 = []\n",
    "    mse_train_2 = []\n",
    "    mse_test_2 = []\n",
    "\n",
    "    for i, degre in enumerate(degres):\n",
    "        mse_train_1.append(stats[i][\"Standard\"][\"mse_train\"])\n",
    "        mse_test_1.append(stats[i][\"Standard\"][\"mse_test\"])\n",
    "        mse_train_2.append(stats[i][\"Ridge\"][\"mse_train\"])\n",
    "        mse_test_2.append(stats[i][\"Ridge\"][\"mse_test\"])\n",
    "\n",
    "    # Affichage des courbes MSE versus degré\n",
    "    axes[0].plot(degres, mse_train_1, \"k-\", label=\"Standard, Train\")\n",
    "    axes[0].plot(degres, mse_test_1, \"k--\", label=\"Standard, Test\")\n",
    "    axes[0].plot(degres, mse_train_2, \"r-\", label=\"Ridge, Train\")\n",
    "    axes[0].plot(degres, mse_test_2, \"r--\", label=\"Ridge, Test\")\n",
    "\n",
    "    axes[0].set_xlabel(\"Degré\", fontsize=14)\n",
    "    axes[0].set_ylabel(\"$MSE$\", fontsize=14)\n",
    "    axes[0].set_title(\n",
    "        \"Évolution de l'erreur quadratique moyenne en fonction du degré des polynômes\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Extraction des valeurs de la métrique R^2\n",
    "    r2_train_1 = []\n",
    "    r2_test_1 = []\n",
    "    r2_train_2 = []\n",
    "    r2_test_2 = []\n",
    "\n",
    "    for i, degre in enumerate(degres):\n",
    "        r2_train_1.append(stats[i][\"Standard\"][\"r2_train\"])\n",
    "        r2_test_1.append(stats[i][\"Standard\"][\"r2_test\"])\n",
    "        r2_train_2.append(stats[i][\"Ridge\"][\"r2_train\"])\n",
    "        r2_test_2.append(stats[i][\"Ridge\"][\"r2_test\"])\n",
    "\n",
    "    # Affichage des courbes R2 versus degré\n",
    "    axes[1].plot(degres, r2_train_1, \"k-\", label=\"Standard, Train\")\n",
    "    axes[1].plot(degres, r2_test_1, \"k--\", label=\"Standard, Test\")\n",
    "    axes[1].plot(degres, r2_train_2, \"r-\", label=\"Ridge, Train\")\n",
    "    axes[1].plot(degres, r2_test_2, \"r--\", label=\"Ridge, Test\")\n",
    "    axes[1].set_ylim([0, 1])\n",
    "\n",
    "    axes[1].set_xlabel(\"Degré\", fontsize=14)\n",
    "    axes[1].set_ylabel(\"$R^2$\", fontsize=14)\n",
    "    axes[1].set_title(\n",
    "        \"Évolution de la métrique du $R^2$ en fonction du degré des polynômes\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    axes[1].legend()\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242bb51a",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"d00c93af\" -->\n",
    "Affichage des métriques de MSE et du $R^2$ en fonction du degré des polynômes.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74faabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "affiche_resultats(stats1, degres)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03173bed",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"ec1a246a\" -->\n",
    "Considérons d'abord les résultats en entraînement; ils sont assez similaires pour les deux méthodes.\n",
    "\n",
    "Les résultats en test sont plus intéressants. La régularisation n'affecte pas vraiment le sous-apprentissage.\n",
    "Les résultats de $R^2$ et de MSE sont similaires pour les deux méthodes jusqu'au minimum de MSE, là\n",
    "où se termine le sous-apprentissage.\n",
    "\n",
    "Notons que maintenant la régularisation réduit et retarde l'apparition du surapprentissage. Alors que la MSE et le $R^2$ de la méthode standard se dégradent pour des degrés plus grands que $6$, les deux métriques Ridge restent plus ou moins constantes pour les modèles plus complexes!\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"caca36c8\" -->\n",
    "Pourquoi est-ce que la régularisation n'affecte pas vraiment le sous-apprentissage?\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"e837516d\" -->\n",
    "**Réponse: Les coefficients des polynômes de faibles degrés n'explosent jamais lors d'un recalage. La régularisation doit quand même les réduire un peu, en valeur absolue. Toutefois pas assez pour produire un changement dans les figures.**\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"23e8a6ce\" -->\n",
    "# <a id=Affichage-des-courbes-recalées-avec-les-deux-méthodes-de-régression>Affichage des courbes recalées avec les deux méthodes de régression</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"86f83127\" -->\n",
    "Définissons d'abord une fonction d'affichage permettant de comparer les prédictions des modèles standard et Ridge avec les données expérimentales.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c8b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df, pipe1, pipe2, xx, yy):\n",
    "    X = df[\"X\"].to_numpy().reshape(-1, 1)\n",
    "    Y = df[\"Y\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    pipe1.fit(X, Y)\n",
    "    y_pred_1 = pipe1.predict(xx)\n",
    "\n",
    "    pipe2.fit(X, Y)\n",
    "    y_pred_2 = pipe2.predict(xx)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "    ax.scatter(X, Y, color=\"black\", alpha=0.1)\n",
    "    ax.plot(xx, yy, \"k-\", label=\"Vrai\", lw=20, alpha=0.1)\n",
    "    ax.plot(xx, y_pred_1, \"r-\", label=\"Standard\")\n",
    "    ax.plot(xx, y_pred_2, \"g-\", label=\"Ridge\")\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ca102",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"4a6b5472\" -->\n",
    "Comparons les résultats de chaque méthode pour le degré de polynôme optimal, c'est-à-dire, pour lequel la MSE est minimale.\n",
    "Selon la figure précédente, le minimum de MSE de la méthode standard est atteint avec un polynôme de degré 6 alors celui de MSE avec la méthode de Ridge est atteint avec une valeur de 11.\n",
    "Entraînons les modèles correspondants.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e859c604",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_params = [{\"fit__alpha\": np.logspace(-8, 0, num=9)}]\n",
    "\n",
    "pipe1 = Pipeline(\n",
    "    [\n",
    "        (\"poly\", PolynomialFeatures(degree=6)),\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"fit\", linear_model.LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"poly\", PolynomialFeatures(degree=11)),\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"fit\", linear_model.Ridge()),\n",
    "    ]\n",
    ")\n",
    "ridge_params = [{\"fit__alpha\": np.logspace(-8, 0, num=9)}]\n",
    "pipe2 = GridSearchCV(pipe, param_grid=ridge_params, refit=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae305cca",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"b733bd14\" -->\n",
    "Affichage des prédictions des deux modèles lorsque les erreurs de régression sont minimales.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158aac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(df, pipe1, pipe2, xx, yy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e7c4d",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"318c3cbc\" -->\n",
    "La figure montre que les prédictions des deux modèles reproduisent assez bien la distribution des\n",
    "valeurs expérimentales ainsi que la courbe du modèle utilisé soit\n",
    "\n",
    "$$y = 100\\sin(10x)$$\n",
    "\n",
    "Comparons maintenant les résultats pour des polynômes de degré 11, là où la méthode standard est proie au\n",
    "surapprentissage, et où la méthode Ridge résiste très bien. Entraînons les modèles correspondants.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4333cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_params = [{\"fit__alpha\": np.logspace(-8, 0, num=9)}]\n",
    "\n",
    "pipe1 = Pipeline(\n",
    "    [\n",
    "        (\"poly\", PolynomialFeatures(degree=11)),\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"fit\", linear_model.LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"poly\", PolynomialFeatures(degree=11)),\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"fit\", linear_model.Ridge()),\n",
    "    ]\n",
    ")\n",
    "ridge_params = [{\"fit__alpha\": np.logspace(-8, 0, num=9)}]\n",
    "pipe2 = GridSearchCV(pipe, param_grid=ridge_params, refit=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bfaf94",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"9ebff413\" -->\n",
    "Affichage des prédictions des deux modèles dans un régime de surapprentissage.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b056c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(df, pipe1, pipe2, xx, yy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c5c85e",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"d67643f1\" -->\n",
    "La figure montre que la régression Ridge reproduit assez bien le signal original alors que la\n",
    "régression standard présente des oscillations typiques du surapprentissage. La régularisation\n",
    "contrôle bien les oscillations parasites!\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"06679b5a\" -->\n",
    "# <a id=Exercices>Exercices</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"6772dab1\" -->\n",
    "Quel est l'effet de diminuer le nombre de données sur la régularisation?\n",
    "Pour y répondre, faites ceci en réutilisant le code précédent:\n",
    "\n",
    "- mettez le nombre de points du signal à 75,\n",
    "- gardez le niveau de bruit à 60,\n",
    "- réentrainez le pipeline précédent,\n",
    "- affichez les résultats.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd003bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "sigma = 60  # Niveau de bruit\n",
    "N = 75  # Nombre de points du signal\n",
    "\n",
    "# Génération du signal expérimental (c.-à-d. bruité) pour des valeurs\n",
    "# de x réparties aléatoirement.\n",
    "df = genere_signal_bruité(N, sigma)\n",
    "\n",
    "# Entraînement du pipeline précédent\n",
    "pipe2 = GridSearchCV(pipe, param_grid=ridge_params, refit=True)\n",
    "\n",
    "# Affichage des résultats\n",
    "plot_data(df, pipe1, pipe2, xx, yy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d5f70",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"1ca1558f\" -->\n",
    "Que remarquez-vous?\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"744ee49a\" -->\n",
    "**Réponse: la diminution du nombre de points augmente les oscillations de la\n",
    "solution standard, sans régularisation. La solution Ridge est plus robuste au bruit\n",
    "et tend à bien reproduire le signal original, non bruité.**\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"d41909de\" -->\n",
    "Quel est l'effet d'augmenter le nombre de données sur la régularisation?\n",
    "Pour y répondre, faites ceci en réutilisant le code précédent:\n",
    "\n",
    "- mettez le nombre de points du signal à 1 000,\n",
    "- gardez le niveau de bruit à 60,\n",
    "- réentrainez le pipeline précédent,\n",
    "- affichez les résultats.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4200869",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "sigma = 60  # Niveau de bruit\n",
    "N = 1000  # Nombre de points du signal\n",
    "\n",
    "# Génération du signal expérimental (c.-à-d. bruité) pour des valeurs\n",
    "# de x réparties aléatoirement.\n",
    "df = genere_signal_bruité(N, sigma)\n",
    "\n",
    "# Entraînement du pipeline précédent\n",
    "pipe2 = GridSearchCV(pipe, param_grid=ridge_params, refit=True)\n",
    "\n",
    "# Affichage des résultats\n",
    "plot_data(df, pipe1, pipe2, xx, yy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19ee1ec",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"62924478\" -->\n",
    "Que remarquez-vous?\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"5565199d\" -->\n",
    "**Réponse: l'augmentation du nombre de points diminue les oscillations\n",
    "des deux méthodes; la régression Ridge devient inutile. Pourquoi? Parce que l'augmentation\n",
    "du nombre de données est une autre méthode de régularisation telle que mentionnée au début du module.**\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"9b4f060a\" -->\n",
    "Quel est l'effet de diminuer le niveau de bruit sur la régularisation?\n",
    "Pour y répondre, faites ceci en réutilisant le code précédent:\n",
    "\n",
    "- mettez le nombre de points du signal à 100,\n",
    "- mettez le niveau de bruit à 10,\n",
    "- réentrainez le pipeline précédent,\n",
    "- affichez les résultats.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c2c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "sigma = 10  # Niveau de bruit\n",
    "N = 100  # Nombre de points du signal\n",
    "\n",
    "# Génération du signal expérimental (c.-à-d. bruité) pour des valeurs\n",
    "# de x réparties aléatoirement.\n",
    "df = genere_signal_bruité(N, sigma)\n",
    "\n",
    "# Entraînement du pipeline précédent\n",
    "pipe2 = GridSearchCV(pipe, param_grid=ridge_params, refit=True)\n",
    "\n",
    "# Affichage des résultats\n",
    "plot_data(df, pipe1, pipe2, xx, yy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5f9d8",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"848573c4\" -->\n",
    "Que remarquez-vous?\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"787f6b77\" -->\n",
    "**Réponse: la diminution du niveau de bruit réduit les oscillations des deux méthodes de régularisation. Pourquoi? Parce que le signal bruité n'est plus très complexe; il n'a plus besoin de polynômes de degré élevé pour reproduire la distribution des points.**\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"ccd95f55\" -->\n",
    "Quel est l'effet d'augmenter le niveau de bruit sur la régularisation?\n",
    "Pour y répondre, faites ceci en réutilisant le code précédent:\n",
    "\n",
    "- mettez le nombre de points du signal à 100,\n",
    "- mettez le niveau de bruit à 150,\n",
    "- réentrainez le pipeline précédent,\n",
    "- affichez les résultats.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31569f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "sigma = 150  # Niveau de bruit élevé\n",
    "N = 100  # Nombre de points du signal\n",
    "\n",
    "# Génération du signal expérimental (c.-à-d. bruité) pour des valeurs\n",
    "# de x réparties aléatoirement.\n",
    "df = genere_signal_bruité(N, sigma)\n",
    "\n",
    "# Entraînement du pipeline précédent\n",
    "pipe2 = GridSearchCV(pipe, param_grid=ridge_params, refit=True)\n",
    "\n",
    "# Affichage des résultats\n",
    "plot_data(df, pipe1, pipe2, xx, yy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a049b1",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"b0ece3d1\" -->\n",
    "Que remarquez-vous?\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"37299444\" -->\n",
    "**Réponse: l'augmentation du niveau de bruit dégrade considérablement\n",
    "les performances des deux méthodes de régularisation. Le signal bruité devient trop complexe\n",
    "à reproduire. Néanmoins, la méthode Ridge donne les meilleurs résultats, mais de peu. On pourrait améliorer les résultats de la méthode Ridge en déterminant à nouveau l'ordre optimal du polynôme. Il ne serait plus de 11 comme dans la secton précédente.**\n",
    "<!-- #endregion -->\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
