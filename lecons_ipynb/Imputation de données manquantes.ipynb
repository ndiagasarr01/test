{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3b5cbf",
   "metadata": {},
   "source": [
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    formats: md,ipynb\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.16.0\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---\n",
    "\n",
    "<!-- #region id=\"06f1218c\" -->\n",
    "# Table des matières\n",
    "1. [Librairies existantes](#librairies-existantes)\n",
    "1. [Approche pas à pas](#approche-pas-à-pas)\n",
    "  1. [Chargement des données](#chargement-des-données)\n",
    "  1. [Validation de l'équilibre de la distribution des données](#validation-de-léquilibre-de-la-distribution-des-données)\n",
    "  1. [Les données sont elles MCAR ?](#les-données-sont-elles-mcar-)\n",
    "  1. [Baseline de prédiction](#ibaselinei-de-prédiction)\n",
    "  1. [Suppression simple des données manquantes](#suppression-simple-des-données-manquantes)\n",
    "  1. [Substitution par la moyenne](#substitution-par-la-moyenne)\n",
    "  1. [Régression déterministe](#régression-déterministe)\n",
    "  1. [MICE](#mice)\n",
    "  1. [KNN (données normalisées)](#knn-données-normalisées)\n",
    "1. [En résumé](#en-résumé)\n",
    "  1. [Que faut-il comprendre de ce graphique ?](#que-faut-il-comprendre-de-ce-graphique-)\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da5a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import matplotlib.gridspec as gs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import wget\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sns.distributions._has_statsmodels = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d8a3b",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"51b39fe4\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/missing-data-illustration.jpeg\"  width=\"400\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://www.flickr.com/photos/78830297@N05/14556250857</font>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"a758e353\" -->\n",
    "Il arrive souvent que l'on désire analyser un  jeu de données contenant des données manquantes. Le manque\n",
    "de données est frustrant, mais somme toute, assez courant. Si nous avons généré  le jeu de données, il est possible que\n",
    "l'information manquante puisse être récupérée en retournant à nos cahiers de laboratoire par exemple. Toutefois,\n",
    "il arrive que ces données soient irrémédiablement perdues, ou simplement impossibles à obtenir\n",
    "expérimentalement. Le problème est encore plus sérieux lorsque  le jeu de données provient d'une source externe indépendante.\n",
    "Il faut alors faire avec, comme on dit.\n",
    "\n",
    "Imaginez que vous avez un jeu de données contenant 100 observations de trois variables $x_{1}$, $x_{2}$ et $x_{3}$,\n",
    "et qu'il manque plusieurs entrées dans la colonne $x_{3}$. Si vous ne voulez pas vous\n",
    "casser la tête et décidez de laisser tomber cette colonne, vous allez perdre $33~\\%$ de l'ensemble de vos\n",
    "données... Imaginez maintenant que pour 25 observations, il manque une entrée ou deux dans\n",
    "les valeurs des $x_{i}$. Si vous laissez tomber ces lignes, vous allez perdre $25~\\%$ de vos observations...\n",
    "Ce n'est évidemment pas la façon de procéder si l'on est économe de nos données chèrement acquises.\n",
    "\n",
    "Il serait facile de mettre à zéro toutes ces données manquantes, mais cela pourrait introduire une quantité\n",
    "importante de biais et réduire l'efficacité des méthodes statistiques utilisées dans l'analyse du jeu de données.\n",
    "\n",
    "L'imputation des données permet de remplacer les valeurs manquantes dans un jeu de données par des estimations de celles-ci.\n",
    "Elle utilise les distributions des données présentes et les relations entre les différentes variables $x_{i}$ mesurées.\n",
    "La figure suivante en montre un exemple. L'imputation prédit la valeur inconnue dans chaque trou de  le jeu de données (en rouge)\n",
    "en interpolant les valeurs des variables $x_{i}$ mesurées (en vert).\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"664e28d5\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/imputation.jpeg\"  width=\"300\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://www.mathworks.com/matlabcentral/fileexchange/60128-sequential-knn-imputation-method</font>\n",
    "    </div>\n",
    "</div>\n",
    "<p>&nbsp;</p>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region colab_type=\"text\" id=\"d7e6d1aa\" -->\n",
    "> Il est fort probable que vous ne connaissiez pas encore les méthodes de classification utilisées dans ce module.\n",
    "Il y en a beaucoup en apprentissage automatique et on ne peut pas toutes les expliquer dans cette formation. Elles\n",
    "sont utilisées ici, car certaines méthodes d'imputation fonctionnent plutôt bien avec celles-ci. La méthode de\n",
    "validation croisée est également utilisée ici sans explications; elle est présentée dans le module sur la méthodologie.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region colab_type=\"text\" id=\"82f03720\" -->\n",
    "# <a id=librairies-existantes>Librairies existantes</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"e1127adb\" -->\n",
    "Si vous voulez faire simple : la librairie Scikit-learn offre un [`SimpleImputer`](http://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer) et un [`MissingIndicator`](http://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html#sklearn.impute.MissingIndicator) alors que la librairie Pandas offre la méthode [`fillna()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html).\n",
    "\n",
    "\n",
    "La librairie [Impyute](https://pypi.org/project/impyute/) offre les fonctionnalités:\n",
    "- Outils de diagnostic:\n",
    "     - journaux,\n",
    "     - distribution des valeurs nulles,\n",
    "     - comparaison des imputations,\n",
    "     - test [MCAR](https://www.jstor.org/stable/2290157) de Little. Le test MCAR (*missing completely at random*) évalue via un test d'hypothèse. Les valeurs d'un ensemble de données sont dites MCAR si les évènements qui conduisent à la disparition d'un élément de données particulier sont indépendants à la fois des variables observables et des paramètres d'intérêt non observables, et ce, tout en se produisant de manière aléatoire.\n",
    "- Imputation de données transversales:\n",
    "     - imputation aléatoire,\n",
    "     - k-voisins les plus proches,\n",
    "     - imputation moyenne,\n",
    "     - imputation par mode,\n",
    "     - imputation médiane,\n",
    "     - imputation multivariée par équations chaînées ([MICE](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/)),\n",
    "     - espérance/maximisation.\n",
    "- Imputation de données chronologiques:\n",
    "     - dernière observation reportée,\n",
    "     - fenêtre mobile,\n",
    "     - moyenne mobile intégrée autorégressive (WIP).\n",
    "\n",
    "Finalement, la librairie [Fancyimpute](https://pypi.org/project/fancyimpute/) offre les fonctionnalités suivantes:\n",
    "* `SimpleFill`: remplace les entrées manquantes par la moyenne ou la médiane de chaque colonne.\n",
    "* `KNN`: imputations du voisin le plus proche qui pondère les échantillons en utilisant la différence quadratique moyenne sur les entités pour lesquelles deux lignes contiennent des données observées.\n",
    "* `SoftImpute`: complétion de la matrice par seuillage souple itératif des décompositions SVD. Inspiré de la librairie [softImpute](https://web.stanford.edu/~hastie/swData/softImpute/vignette.html) pour R, basé sur [Spectral Regularization Algorithms for Learning Large Incomplete Matrices](http://web.stanford.edu/~hastie/Papers/mazumder10a.pdf) de Mazumder et. al..\n",
    "* `IterativeSVD`: achèvement de la matrice par décomposition itérative SVD de bas rang. Devrait être similaire à SVDimpute de [Missing value estimation methods for DNA microarrays](http://www.ncbi.nlm.nih.gov/pubmed/11395428) de Troyanskaya et. al..\n",
    "* `IterativeImputer` (ex-MICE): une stratégie pour imputer les valeurs manquantes en modélisant chaque entité avec des valeurs manquantes en fonction des autres entités de manière alternée.\n",
    "* `MatrixFactorization`: factorisation directe de la matrice incomplète en « U » et « V » de bas rang, avec une pénalité de faible densité L1 sur les éléments de« U »et une pénalité de L2 sur les éléments de « V ». Résolu par descente progressive.\n",
    "* `NuclearNormMinimization`: implémentation simple de [Exact Matrix Completion via Convex Optimization](http://statweb.stanford.edu/~candes/papers/MatrixCompletion.pdf) d'Emmanuel Candes et Benjamin Recht utilisant [cvxpy](http://www.cvxpy.org). Algorithme trop lent pour les grandes matrices.\n",
    "* `BiScaler`: estimation itérative de la moyenne des rangées/colonnes et des écarts types pour obtenir une double normalisation. Pas garanti de converger, mais fonctionne bien dans la pratique. Tiré de [Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares](http://arxiv.org/abs/1410.2596).\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region _cell_guid=\"b57ada49-2e30-473d-9e6e-fa57afd87bc5\" _uuid=\"c66c9a187266122b4fdc5b50ad650988f9e23603\" colab_type=\"text\" id=\"9ce650b0\" -->\n",
    "# <a id=approche-pas-à-pas>Approche pas à pas</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"2d635f36\" -->\n",
    "Dans ce qui suit, nous allons appliquer différentes méthodes d'imputation au\n",
    " jeu de données Titanic. Celle-ci est tirée d'un projet de classification proposé en 2014 sur la plateforme Web\n",
    "[Kaggle](https://www.kaggle.com/c/titanic/) qui organise\n",
    "des compétitions en science des données. Le but est de prédire la survie de passagers lors du\n",
    "naufrage du Titanic en 1912, au large de Terre-Neuve, au Canada.\n",
    "\n",
    " Le jeu de données contient beaucoup de données manquantes. La raison principale étant que lors du voyage inaugural du Titanic,\n",
    "la compagnie maritime a décidé de remplir ses cales d'émigrants pour les États-Unis afin de maximiser ses profits\n",
    "et ne pas faire le voyage à moitié vide. Les gens occupant la dernière classe ont embarqué\n",
    "peu de temps avant leur départ, de la France ou de l'Angleterre, et leurs informations personnelles\n",
    "n'ont pas toutes été colligées assidument, d'où les données manquantes.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"6d298b6e\" -->\n",
    "## <a id=chargement-des-données>Chargement des données</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"387b37dd\" -->\n",
    "Les BD d'entraînement et de test contiennent les variables suivantes:\n",
    "\n",
    " - survival: survie (0 = Non, 1 = Oui),\n",
    " - pclass: classe de billets (1 = 1er, 2 = 2e, 3 = 3e),\n",
    " - sex: sexe,\n",
    " - Age: âge en années,\n",
    " - sibsp: nombre de frères et sœurs / conjoints à bord du Titanic,\n",
    " - parch: nombre de parents/enfants à bord du Titanic,\n",
    " - ticket: numéro de billet,\n",
    " - name: nom du passager,\n",
    " - fare: prix du billet,\n",
    " - embarked: port d'embarquement (C = Cherbourg, Q = Queenstown, S = Southampton).\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdec5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/pax/shared/GIF-U014/titanic_train.csv\")\n",
    "test = pd.read_csv(\"/pax/shared/GIF-U014/titanic_test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d04c31b",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region colab_type=\"text\" id=\"1b84e5fa\" -->\n",
    "Certaines variables numériques sont converties en catégories et d'autres sont éliminées, car elles sont inutiles.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281b3a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des types de colonnes\n",
    "\n",
    "\n",
    "def enforce_types_titanic(df):\n",
    "    Pclass_dtype = pd.api.types.CategoricalDtype(categories=[1, 2, 3], ordered=True)\n",
    "    df.Survived = df.Survived.astype(\"category\")\n",
    "    df.Pclass = df.Pclass.astype(Pclass_dtype)\n",
    "    df.Sex = df.Sex.astype(\"category\")\n",
    "    df.Embarked = df.Embarked.astype(\"category\")\n",
    "    df = df.drop(\"Name\", axis=1)\n",
    "    df = df.drop(\"Ticket\", axis=1)\n",
    "    df = df.drop(\"index\", axis=1)\n",
    "    df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "train = enforce_types_titanic(train)\n",
    "test = enforce_types_titanic(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24643f5",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d71a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comptage des valeurs nulles dans le jeu de données\n",
    "\n",
    "\n",
    "def na_summary(df):\n",
    "    return df.isnull().sum()\n",
    "\n",
    "\n",
    "na_summary(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38f0eed",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451fd56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_summary(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34f502e",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"e0403c53\" -->\n",
    "Il manque des valeurs dans l'ensemble d'entraînement, mais pas dans l'ensemble de test.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region _cell_guid=\"602b7065-1c54-4a77-8e47-4836e68a0a05\" _uuid=\"008a9e79ab99b1553bd002d0b745a85fa11f08a2\" colab_type=\"text\" id=\"62d264f3\" -->\n",
    "## <a id=validation-de-léquilibre-de-la-distribution-des-données>Validation de l'équilibre de la distribution des données</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"2b75acc6\" -->\n",
    "Si l'on veut entraînement puis tester un classificateur permettant de prédire la survie d'un passager, il\n",
    "faut que les deux ensembles de données soient distribués similairement selon toutes les variables. Nous allons\n",
    "comparer les distributions de chaque variable dans les deux ensembles de données.\n",
    "\n",
    "Est-ce vraiment nécessaire? Pour vous en donner une idée, imaginez que vous travaillez dans une\n",
    "agence de sondages et que vous voulez prédire le résultat d'une élection. Si vous n'interviewez\n",
    "que des gens de plus de 65 ans (ensemble entraînement) alors que l'on sait que le jour de l'élection,\n",
    "des gens de tout âge voteront (ensemble de test) alors vos prédictions initiales risquent d'être\n",
    "totalement biaisées! Voilà pourquoi il faut que les deux jeux de données soient bien équilibrés.\n",
    "\n",
    "> À noter que valider la similarité des distributions des caractéristiques entre les deux jeux de données ne doit pas devenir du zèle. Il est normal et désirable d'avoir une \"certaine\" divergence entre les deux. Nous cherchons surtout à ne pas avoir tous les passagers survivants dans l'ensemble d'entraînement et tous ceux qui ont péri dans l'ensemble de test.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88b75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la taille de la grille en fonction du nombre de caractéristiques.\n",
    "def grid_size(nb_features):\n",
    "    a = len(nb_features)\n",
    "    if a % 2 != 0:\n",
    "        a += 1\n",
    "    n = np.floor(np.sqrt(a)).astype(np.int64)\n",
    "    while a % n != 0:\n",
    "        n -= 1\n",
    "    m = (a / n).astype(np.int64)\n",
    "    return m, n\n",
    "\n",
    "\n",
    "# Affichage des deux distributions pour chaque colonne des dataframes 1 et 2\n",
    "def dist_comparison(df1, df2):\n",
    "\n",
    "    assert len(df1.columns) == len(df2.columns)\n",
    "\n",
    "    m, n = grid_size(df1.columns)\n",
    "    coords = list(itertools.product(list(range(m)), list(range(n))))\n",
    "\n",
    "    # Choix des graphiques pour chaque type de colonne.\n",
    "    numerics = df1.select_dtypes(include=[np.number]).columns\n",
    "    cats = df1.select_dtypes(include=[\"category\"]).columns\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    axes = gs.GridSpec(m, n)\n",
    "    axes.update(wspace=0.25, hspace=0.25)\n",
    "    # Graphiques pour données numériques : on fait un KDE de la distribution.\n",
    "    for i in range(len(numerics)):\n",
    "        x, y = coords[i]\n",
    "        ax = plt.subplot(axes[x, y])\n",
    "        col = numerics[i]\n",
    "\n",
    "        sns.kdeplot(df1[col].dropna(), ax=ax, label=\"df1\").set(xlabel=col)\n",
    "        sns.kdeplot(df2[col].dropna(), ax=ax, label=\"df2\")\n",
    "\n",
    "    # Graphique pour les données catégoriques : diagramme en bâtons.\n",
    "    for i in range(0, len(cats)):\n",
    "        x, y = coords[len(numerics) + i]\n",
    "        ax = plt.subplot(axes[x, y])\n",
    "        col = cats[i]\n",
    "\n",
    "        df1_temp = df1[col].value_counts()\n",
    "        df2_temp = df2[col].value_counts()\n",
    "        df1_temp = pd.DataFrame(\n",
    "            {\n",
    "                col: df1_temp.index,\n",
    "                \"value\": df1_temp / len(df1),\n",
    "                \"Set\": np.repeat(\"df1\", len(df1_temp)),\n",
    "            }\n",
    "        )\n",
    "        df2_temp = pd.DataFrame(\n",
    "            {\n",
    "                col: df2_temp.index,\n",
    "                \"value\": df2_temp / len(df2),\n",
    "                \"Set\": np.repeat(\"df2\", len(df2_temp)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        sns.barplot(\n",
    "            x=col, y=\"value\", hue=\"Set\", data=pd.concat([df1_temp, df2_temp]), ax=ax\n",
    "        ).set(ylabel=\"Percentage\")\n",
    "\n",
    "\n",
    "# Affichage de la comparaison entre les données de train et de test (sans la colonne des étiquettes).\n",
    "dist_comparison(train, test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68d54e",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region _cell_guid=\"eb108455-8d35-42b5-a6e6-0d644d30959e\" _uuid=\"de2217f045c02397cf908b43a38ee9dc9b86fc6b\" colab_type=\"text\" id=\"95932e6b\" -->\n",
    "Les distributions des variables sont bien balancées (en pourcentages) entre les deux jeux de données.\n",
    "\n",
    "Comme approche de base (souvent appelée une *baseline*), nous complétons arbitrairement les données manquantes dans l'ensemble d'entraînement\n",
    "\n",
    "> À noter que l'on ne touche pas à la variable `Age`. On va utiliser plusieurs méthodes d'imputation pour en estimer les valeurs manquantes.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0efcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarked est le port d'embarquement, on complète les données manquantes par le premier port.\n",
    "train.Embarked = train.Embarked.fillna(\"C\")\n",
    "\n",
    "# Il manque une donnée de prix payé pour le billet; choisissons une valeur arbitraire.\n",
    "train.Fare = train.Fare.fillna(8.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc5b7e9",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5801e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_summary(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a2e02f",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_summary(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f15a3",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region _cell_guid=\"d36d00c2-6fd3-4b5f-9140-3dff423e0b87\" _uuid=\"f4046a90e27ad0913eca82b47cc76c2d07850ccd\" colab_type=\"text\" id=\"0f25671c\" -->\n",
    "## <a id=les-données-sont-elles-mcar->Les données sont elles MCAR ?</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"bbe2261b\" -->\n",
    "L'opération suivante n'est effectuée qu'avec le jeu de données d'entraînement, car il n'y a pas de\n",
    "valeurs manquantes dans le jeu de données de test.\n",
    "\n",
    "La variable `Age` est celle pour laquelle on manque le plus de données. Dans ce qui\n",
    "suit, on sépare les données d'entraînement en deux sous-ensembles; un pour les passagers dont l'âge connu et un\n",
    "autre pour les passagers dont l'âge est inconnu. On compare ensuite les distributions des autres variables\n",
    "(hormis les variables `Age` et `Survived`). Si elles sont identiques (MCAR), cela veut dire que les données\n",
    "manquent complètement au hasard; elles sont distribuées uniformément en fonction du sexe, de la classe de passagers, etc.\n",
    "Pour vous donner une idée de la pertinence de la question, imaginez par exemple si toutes les valeurs manquantes provenaient\n",
    "d'hommes de plus de 20 ans ayant voyagé en troisième classe et provenant de Cherbourg. Les valeurs\n",
    "manquantes correspondraient toutes à un cas particulier, plus difficile à traiter.\n",
    "\n",
    "Pourquoi faire ce test? Réponse: si les données manquent complètement au hasard, cela veut dire qu'on peut\n",
    "prédire les âges manquants simplement en interpolant à partir des autres variables.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96c5773",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_present = train.dropna().drop(\"Age\", 1)\n",
    "age_missing = train[train.isnull().any(axis=1)].drop(\"Age\", 1)\n",
    "\n",
    "cat_dtype = pd.api.types.CategoricalDtype(categories=list(range(8)), ordered=True)\n",
    "age_present.Parch = age_present.Parch.astype(cat_dtype)\n",
    "age_missing.Parch = age_missing.Parch.astype(cat_dtype)\n",
    "\n",
    "age_present.SibSp = age_present.SibSp.astype(cat_dtype)\n",
    "age_missing.SibSp = age_missing.SibSp.astype(cat_dtype)\n",
    "\n",
    "dist_comparison(age_present.drop(\"Survived\", 1), age_missing.drop(\"Survived\", 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b915e",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region _cell_guid=\"ec01755d-01b8-45e8-b57a-7f83ff302927\" _uuid=\"0fee94f930b9a76b642e4182f2e0a073da427c6c\" colab_type=\"text\" id=\"149b8b44\" -->\n",
    "Il semble que nous ne puissions pas vérifier l’hypothèse MCAR. L'explication semble être que nous sommes\n",
    "moins susceptibles de connaître l'âge des personnes décédées. Comme en témoigne la proportion beaucoup plus\n",
    "grande de passagers de la classe inférieure, le pic plus net dans les tarifs plus bas et une légère asymétrie\n",
    "à l’égard des hommes.\n",
    "\n",
    "De manière plus significative encore, il semble que les personnes qui se sont embarquées à Queenstown (Q)\n",
    "ont un taux beaucoup plus élevé d’âge manquant.\n",
    "\n",
    "> À noter qu'il serait préférable d'utiliser une mesure plus objective et robuste de MCAR, comme le test de Little.\n",
    "Dans ce qui suit, nous allons toutefois faire l'hypothèse de MCAR afin de simplifier l'analyse.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region _cell_guid=\"8be28abb-f498-450a-9d25-4771c432ecec\" _uuid=\"1eb821dba55457db166b63b8bf943bb1f5a30164\" colab_type=\"text\" id=\"446a0186\" -->\n",
    "## <a id=ibaselinei-de-prédiction>*Baseline* de prédiction</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region _cell_guid=\"64668e8b-0ae7-4c4e-8062-afdaf708c9d4\" _uuid=\"5079fff2dea16b57d9c220e40b694a4dde489b38\" colab_type=\"text\" id=\"149300e6\" -->\n",
    "Sans l'utilisation des données `Age`, notre prédicteur sera un classificateur par forêt aléatoire\n",
    "avec les paramètres affichés. Toutes les estimations d'erreur de test sont obtenues par une\n",
    "validation croisée 10 fois. Le classificateur élimine le problème des valeurs manquantes en\n",
    "éliminant simplement la variable les contenant. Il jette le bébé avec l'eau du bain.\n",
    "\n",
    "> À noter que la validation croisée est une méthode qui permet, lors de l'entraînement d'un modèle, de partitionner les données d'entraînement en $N$ sections. Le modèle est entrainé sur $N-1$ sections et testé sur la dernière. L'opération est effectuée $N$ fois et on prend la moyenne des $N$ résultats. C'est particulièrement utile lorsque l'ensemble d'entraînement contient peu de données. On s'en sert également pour faire de la sélection de modèles en régression et en classification. La validation croisée est universellement utilisée en apprentissage automatique; on en discute plus en détail dans un des modules sur la méthodologie.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64faa1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données afin d'enlever les données catégoriques.\n",
    "def prep_des_donnees(df):\n",
    "    new_df = df.copy()\n",
    "    # Classe de cabine en entier\n",
    "    new_df.Pclass = new_df.Pclass.astype(\"int\")\n",
    "    # Sexe binaire\n",
    "    new_df.Sex.cat.categories = [0, 1]\n",
    "    new_df.Sex = new_df.Sex.astype(\"int\")\n",
    "    # Port d'embarquement en entier (on pourrait aussi utiliser une variable dummy)\n",
    "    new_df.Embarked.cat.categories = [0, 1, 2]\n",
    "    new_df.Embarked = new_df.Embarked.astype(\"int\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# Même pipeline pour le train et le test.\n",
    "train_cl = prep_des_donnees(train)\n",
    "test_cl = prep_des_donnees(test)\n",
    "\n",
    "# Sélection des colonnes sans l'âge.\n",
    "Xcol = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "Ycol = \"Survived\"\n",
    "X = train_cl[Xcol]\n",
    "Y = train_cl[Ycol]\n",
    "\n",
    "# Mémorisation des jeux de données avant qu'on ne fasse d'autres transformations.\n",
    "Xbase = X\n",
    "Ybase = Y\n",
    "\n",
    "# Classification par forêt aléatoire (Random Forest en anglais, ou RF.)\n",
    "rf = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=10)\n",
    "\n",
    "baseline_err = cross_val_score(rf, X, Y, cv=10, n_jobs=2).mean()\n",
    "print(\n",
    "    \"[BASELINE] Estimation RF sur Train (n = {}, 10-fold CV): {}\".format(\n",
    "        len(X), baseline_err\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1895c8",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region _cell_guid=\"9d2a87fb-4317-4fd2-a6db-f5bd62fb3362\" _uuid=\"bcad8d2354eb04f7139567624d27ac2d9e73ad5a\" colab_type=\"text\" id=\"1825ef3d\" -->\n",
    "## <a id=suppression-simple-des-données-manquantes>Suppression simple des données manquantes</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"af97c2e1\" -->\n",
    "On ajoute maintenant la variable `Age`, mais en laissant tomber toutes les données pour les passagers dont\n",
    "l'âge n'est pas connu. On perd, chez ceux-ci, des données pertinentes dans les colonnes autres que celle de l'âge.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbae080",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdel = train_cl.dropna()[Xcol + [\"Age\"]]\n",
    "Ydel = train_cl.dropna()[Ycol]\n",
    "\n",
    "deletion_err = cross_val_score(rf, Xdel, Ydel, cv=10, n_jobs=2).mean()\n",
    "print(\n",
    "    \"[DELETION] Estimation RF sur Train (n = {}, 10-fold CV): {}\".format(\n",
    "        len(Xdel), deletion_err\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebbbae2",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region _cell_guid=\"a6ac5c2b-364b-476d-8786-dd9f0fd7a9cc\" _uuid=\"4dc1d8e53ddc961c92a334367201c89cdc21af3b\" colab_type=\"text\" id=\"92f49470\" -->\n",
    "## <a id=substitution-par-la-moyenne>Substitution par la moyenne</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"0a0fa709\" -->\n",
    "Afin de ne pas perdre inutilement des données, on ne laisse plus tomber les passagers d'âges inconnus. On\n",
    "remplace les valeurs manquantes d'âge par la moyenne des valeurs connues.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa93b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cl = prep_des_donnees(train)\n",
    "train_cl.Age = train_cl.Age.fillna(train_cl.Age.mean(skipna=True))\n",
    "\n",
    "Xmean = train_cl[Xcol + [\"Age\"]]\n",
    "Ymean = train_cl[Ycol]\n",
    "\n",
    "mean_err = cross_val_score(rf, Xmean, Ymean, cv=10, n_jobs=2).mean()\n",
    "print(\n",
    "    \"[MEAN] Estimation RF sur Train (n = {}, 10-fold CV): {}\".format(\n",
    "        len(Xmean), mean_err\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788411f9",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region _cell_guid=\"a52e28e5-1333-4c96-bda7-10b36c39c137\" _uuid=\"ec26ebd70984436e825d406d2616830f0f12e9cc\" colab_type=\"text\" id=\"05416ba4\" -->\n",
    "## <a id=régression-déterministe>Régression déterministe</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"933f02f7\" -->\n",
    "Plutôt que de remplacer les âges manquants par l'âge moyen, on modélise la variable `Age` comme une fonction des\n",
    "autres variables. On entraine, à cette fin, un régresseur n'utilisant que les données des passagers dont l'âge\n",
    "est connu. On s'en sert ensuite pour prédire l'âge des passagers dont l'âge est inconnu.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1343100",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cl = prep_des_donnees(train)\n",
    "train_reg = train_cl.dropna()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "Xrcol = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "Yrcol = \"Age\"\n",
    "\n",
    "X_reg = train_reg[Xrcol]\n",
    "Y_reg = train_reg[Yrcol]\n",
    "\n",
    "age_lm = LinearRegression()\n",
    "age_lm.fit(X_reg, Y_reg)\n",
    "abs_residuals = np.absolute(Y_reg - age_lm.predict(X_reg))\n",
    "\n",
    "# Identifie les indices des passagers dont l'âge est inconnu\n",
    "nan_inds = train_cl.Age.isnull().to_numpy().nonzero()[0]\n",
    "train_cl2 = train_cl.copy()\n",
    "\n",
    "for i in nan_inds:\n",
    "    train_cl[\"Age\"].at[i] = age_lm.predict(train_cl.loc[i, Xrcol].values.reshape(1, -1))\n",
    "\n",
    "Xreg = train_cl[Xcol + [\"Age\"]]\n",
    "Yreg = train_cl[Ycol]\n",
    "\n",
    "reg_err = cross_val_score(rf, Xreg, Yreg, cv=10, n_jobs=2).mean()\n",
    "print(\n",
    "    \"[DETERMINISTIC REGRESSION] Estimation RF sur Train (n = {}, 10-fold CV): {}\".format(\n",
    "        len(Xreg), reg_err\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b5c3ea",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region _cell_guid=\"30dc53c5-cb32-4d44-8d23-d2c806a9cd23\" _uuid=\"408dbaa3bdcefb881222e942070e050e36bd40f3\" colab_type=\"text\" id=\"45977539\" -->\n",
    "## <a id=mice>MICE</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"66ac5731\" -->\n",
    "La méthode MICE (*multiple imputation by chained equations*) est une méthode d'imputation itérative.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca967b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cl = prep_des_donnees(train)\n",
    "\n",
    "X = train_cl.loc[:, Xcol + [\"Age\"]]\n",
    "Y = train_cl.loc[:, Ycol]\n",
    "\n",
    "Xmice = IterativeImputer().fit_transform(X)\n",
    "Ymice = Y\n",
    "\n",
    "mice_err = cross_val_score(rf, Xmice, Y, cv=10, n_jobs=2).mean()\n",
    "print(\n",
    "    \"[MICE] Estimation RF sur Train (n = {}, 10-fold CV): {}\".format(\n",
    "        len(Xmice), mice_err\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9c07ee",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region _cell_guid=\"d37c7551-67bd-4e7f-94d9-25a950021876\" _uuid=\"5cb98a3c43ea86f8348b529623eb2cc39f3970c3\" colab_type=\"text\" id=\"6f16653b\" -->\n",
    "## <a id=knn-données-normalisées>KNN (données normalisées)</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"a15ea731\" -->\n",
    "Nous allons maintenant utiliser le classificateur des N plus proches voisins afin de trouver, dans l'espace des variables\n",
    "$$[\\text{Pclass}, \\text{Sex}, \\text{SibSp}, \\text{Parch}, \\text{Fare}, \\text{Embarked}]$$\n",
    "\n",
    "quels sont les N passagers, d'âges connus, les plus près\n",
    "d'un passager d'âge inconnu. On assigne à celui-ci la moyenne des âges de ses $N$ plus proches voisins.\n",
    "\n",
    "Il peut sembler étrange, dans ce qui suit, de normaliser ces variables puisque plusieurs facteurs (colonnes) correspondent à\n",
    "des variables binaires ou ordinales. L'idée est de rapporter tous ces facteurs à la même échelle, entre 0 et 1,\n",
    "afin de calculer correctement les « distances » entre les passagers et trouver leurs plus proches voisins.\n",
    "Sans la normalisation (ou standardisation), les distances ne dépendraient principalement que des différences de\n",
    "prix du billet (`Fare`) puisque celui-ci a les plus grandes valeurs.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54b640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cl = prep_des_donnees(train)\n",
    "\n",
    "Xcol = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "X = train_cl.loc[:, Xcol + [\"Age\"]]\n",
    "Y = train_cl.loc[:, Ycol]\n",
    "\n",
    "Xnorm = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "kvals = np.linspace(1, 100, 20, dtype=\"int64\")\n",
    "\n",
    "knn_errs = []\n",
    "for k in kvals:\n",
    "    knn_err = []\n",
    "    Xknn = KNNImputer(n_neighbors=k).fit_transform(Xnorm)\n",
    "    knn_err = cross_val_score(rf, Xknn, Y, cv=10, n_jobs=2).mean()\n",
    "\n",
    "    knn_errs.append(knn_err)\n",
    "    print(\n",
    "        \"[KNN] Estimation RF sur Train (n = {}, k = {}, 10-fold CV): {}\".format(\n",
    "            len(Xknn), k, np.mean(knn_err)\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d8f7e",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"13c0c463\" -->\n",
    "Affiche le meilleur résultat, c'est-à-dire, pour le nombre $N_{\\text{optimal}}$ de voisins.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5022476",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "_ = plt.plot(kvals, knn_errs)\n",
    "_ = plt.xlabel(\"K\")\n",
    "_ = plt.ylabel(\"10-fold CV Error Rate\")\n",
    "\n",
    "knn_err = max(knn_errs)\n",
    "k_opt = kvals[knn_errs.index(knn_err)]\n",
    "\n",
    "knn = KNNImputer(n_neighbors=k_opt)\n",
    "knn.fit(Xnorm)\n",
    "Xknn = knn.transform(Xnorm)\n",
    "\n",
    "print(\n",
    "    \"[BEST KNN] Estimation RF sur Train (n = {}, k = {}, 10-fold CV): {}\".format(\n",
    "        len(Xknn), k_opt, np.mean(knn_err)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b20946",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region _cell_guid=\"b6b05aaf-e846-4225-95c0-e66f807e61ea\" _uuid=\"c02e770675a39fd1b4c4e3f13639b852f5a24d7d\" colab_type=\"text\" id=\"4772778a\" -->\n",
    "# <a id=en-résumé>En résumé</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"ac194a37\" -->\n",
    "Comparons maintenant les résultats des multiples méthodes d'imputation testées (prends quelque temps à exécuter) .\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f8572",
   "metadata": {},
   "outputs": [],
   "source": [
    "errs = {\n",
    "    \"BEST KNN (k = {})\".format(k_opt): knn_err,\n",
    "    \"DETERMINISTIC REGRESSION\": reg_err,\n",
    "    \"MICE\": mice_err,\n",
    "    \"MEAN\": mean_err,\n",
    "    \"DELETION\": deletion_err,\n",
    "    \"BASELINE\": baseline_err,\n",
    "}\n",
    "\n",
    "err_df = pd.DataFrame.from_dict(errs, orient=\"index\")\n",
    "err_df.index.name = \"Imputation Method\"\n",
    "err_df.reset_index(inplace=True)\n",
    "err_df.columns = [\"Imputation\", \" Estimation sur Test  (10-fold CV)\"]\n",
    "\n",
    "ax = sns.barplot(\n",
    "    x=err_df.columns[1],\n",
    "    y=err_df.columns[0],\n",
    "    order=list.sort(list(errs.values())),\n",
    "    data=err_df,\n",
    ")\n",
    "ax.set_xlabel(err_df.columns[1])\n",
    "ax.set_ylabel(\"\")\n",
    "_ = plt.xlim(0.7, 0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364c2f91",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"ad529b45\" -->\n",
    "## <a id=que-faut-il-comprendre-de-ce-graphique->Que faut-il comprendre de ce graphique ?</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"57bd887d\" -->\n",
    "Que si on complète les données manquantes avec divers modèles on améliore le pouvoir de\n",
    "prédiction ... en ajoutant un biais dans les données :-).\n",
    "\n",
    "Notez à quel point les deux méthodes du bas, bien qu'intuitives, ne sont pas efficaces. La *baseline*\n",
    "laisse tomber la variable problématique `Age` et la seconde enlève les données des passagers avec\n",
    "valeurs manquantes. En creusant un peu plus, on est passés d'un score de $74~\\%$ à $78~\\%$ juste en interpolant\n",
    "au travers des données existantes.\n",
    "\n",
    "Un lecteur attentif aura remarqué qu'on n'a pas utilisé les données de test après avoir enlevé les données catégoriques. Le but de ce module était de montrer comment on sélectionne la meilleure méthode d'imputation des données manquantes. Cette opération se fait avec les données d'entraînement.\n",
    "<!-- #endregion -->\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
