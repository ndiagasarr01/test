{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "755cef25",
   "metadata": {},
   "source": [
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.16.0\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---\n",
    "\n",
    "<!-- #region id=\"0487f37c\" -->\n",
    "# Table des matières\n",
    "1. [Rappel: différences entre la régression et la classification](#rappel-différences-entre-la-régression-et-la-classification)\n",
    "1. [Modèle de la régression logistique](#modèle-de-la-régression-logistique)\n",
    "1. [Différences et similarités entre la régression logistique et la régression linéaire](#différences-et-similarités-entre-la-régression-logistique-et-la-régression-linéaire)\n",
    "1. [Applications](#applications)\n",
    "1. [Origine du modèle de régression logistique](#origine-du-modèle-de-régression-logistique)\n",
    "1. [Estimation des paramètres de la régression logistique](#estimation-des-paramètres-de-la-régression-logistique)\n",
    "  1. [Quelle est l'intuition derrière la fonction de perte?](#quelle-est-lintuition-derrière-la-fonction-de-perte)\n",
    "    1. [Que vaut la perte $S_i$ lors d'une classification parfaite?](#que-vaut-la-perte-s_i-lors-dune-classification-parfaite)\n",
    "    1. [Que vaut la perte $S_i$ lors d'une mauvaise classification?](#que-vaut-la-perte-s_i-lors-dune-mauvaise-classification)\n",
    "    1. [En résumé](#en-résumé)\n",
    "1. [Exemple de classification 2-D](#exemple-de-classification-2-d)\n",
    "1. [Identification des facteurs importants](#identification-des-facteurs-importants)\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb32ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5051bc6",
   "metadata": {},
   "source": [
    "<!-- #region id=\"e93269f3\" -->\n",
    "Voilà une belle source de confusion; la régression logistique est en fait une méthode de ... classification!\n",
    "L'origine historique de cette confusion de nom est expliquée plus loin. On peut dire que c'est une méthode d'analyse\n",
    "des données à la frontière entre la régression et la classification.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"ad590c8f\" -->\n",
    "# <a id=rappel-différences-entre-la-régression-et-la-classification>Rappel: différences entre la régression et la classification</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"34aa2e7d\" -->\n",
    "La régression sert à prédire une réponse $y$, qui est une variable numérique continue, en fonction de plusieurs\n",
    "facteurs $x_{i}$\n",
    "\n",
    "$$y=f(x_{1}, \\cdots, x_{N}, \\Theta)$$\n",
    "\n",
    "où $\\Theta$ représente l'ensemble des paramètres de la fonction $f$.\n",
    "\n",
    "La classification, quant à elle, sert à prédire une classe $C$, qui est une variable catégorielle, en fonction de\n",
    "plusieurs facteurs $x_{i}$\n",
    "\n",
    "$$C=g(x_{1}, \\cdots, x_{N}, \\Theta)$$\n",
    "\n",
    "où $\\Theta$ représente l'ensemble des paramètres de la fonction $g$.\n",
    "\n",
    "La régression logistique chevauche les deux; elle prédit la probabilité de la classe $C$.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"61fbb3b4\" -->\n",
    "# <a id=modèle-de-la-régression-logistique>Modèle de la régression logistique</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"1b0a77ea\" -->\n",
    "Ainsi, la régression logistique sert à prédire la probabilité $p \\in [0,1]$ de succès (1, \"Vrai\", \"True\") qu'un\n",
    "évènement $E$ se produise, en fonction de plusieurs facteurs $x_{i}$. La formulation standard du modèle est\n",
    "\n",
    "$$p(E|X,\\Theta)= {\\dfrac {1}{1+e^{-(a_{0}+a_{1}x_{1}+\\cdots + a_{N} x_{N})}}}$$\n",
    "\n",
    "où les $a_{i}$ sont les paramètres de la fonction.\n",
    "\n",
    "Dans le cas plus général, on peut remplacer l'expression précédente avec\n",
    "\n",
    "$$p(E\\vert X,\\Theta) = {\\dfrac {1}{1+e^{-f(x_{1},\\cdots, x_{N},\\Theta)}}}$$\n",
    "\n",
    "où $\\Theta$ représente l'ensemble des paramètres de la fonction $f$. Celle-ci est généralement\n",
    "un polynôme de degré 1 ou 2.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"01e8d982\" -->\n",
    "Lorsque $p(E)>\\tau$, où $\\tau$ est un seuil de probabilité prédéterminé, on considère que l'évènement E s'est\n",
    "produit et l'étiquette de la classe résultante est (1, \"Vrai\", \"True\", A, etc.). Dans le cas contraire, l'étiquette est\n",
    "(0, \"Faux\", \"False\", B, etc.) Il est à noter que le seuil de probabilité $\\tau \\in [0,1]$; il peut être différent de $50~\\%$.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"ba75bd6f\" -->\n",
    "# <a id=différences-et-similarités-entre-la-régression-logistique-et-la-régression-linéaire>Différences et similarités entre la régression logistique et la régression linéaire</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"afe8ce74\" -->\n",
    "La figure suivante compare les deux méthodes. Dans le panneau de\n",
    "droite, on voit un exemple classique de régression linéaire où les paramètres d'une droite ont été ajustés\n",
    "afin de la recaler sur les données (*fit a linear model to the data*). Le panneau\n",
    "de gauche montre une application totalement différente. Dans ce cas-ci, la droite sert de frontière entre\n",
    "les deux classes de points. C'est donc un problème de classification. C'est le genre de frontière que l'on\n",
    "observe lorsque l'on utilise la régression logistique avec une fonction $f(x_{1},\\cdots, x_{N},\\Theta)$ linéaire.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"9a5f3e2d\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/logistic-vs-linear-regression.png\"  width=\"500\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://www.researchgate.net/publication/335786324_Regression_Analysis_With_Differential_Privacy_Preserving/figures?lo=1/</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"694c7fdd\" -->\n",
    "# <a id=applications>Applications</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"38f3d072\" -->\n",
    "La régression logistique est très utilisée en sciences sociales, en médecine, en finances.\n",
    "On s'en sert afin de prédire la probabilité $p(E)$ qu'un évènement $E$ se produise alors que\n",
    "celui-ci résulte le l'effet combiné d'une multitude de facteurs $x_{i}$. Ces facteurs peuvent être des\n",
    "variables continues, catégorielles ou ordinales comme pour les modèles de régression\n",
    "multilinéaires présentés dans les sections précédentes.\n",
    "\n",
    "Voici quelques situations dans lesquelles la régression logistique est utilisée. Chacune dépend de multiples facteurs\n",
    "sociétaux, financiers, technologiques, physiologiques, psychologiques, etc. Il faut pouvoir les mesurer\n",
    "afin de pouvoir ensuite modéliser la probabilité de l'évènement étudié.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"b89098cc\" -->\n",
    "## Hôtellerie:\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"b398c6bc\" -->\n",
    "- Quelle est la probabilité qu'un touriste canadien loue une chambre dans un hôtel en Égypte à\n",
    "partir de ses réponses à un questionnaire en ligne?\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"864f2351\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/cairo-egypt.jpeg\"  width=\"300\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://www.publicdomainpictures.net/en/view-image.php?image=354011&picture=cairo-travel-poster/</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"ee44f8e2\" -->\n",
    "## Jeux vidéo:\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"8f883892\" -->\n",
    "- Quelle est la probabilité qu'un joueur achète différents types de pouvoirs?\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"394cc587\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/video-game-phone.jpeg\"  width=\"300\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://www.pexels.com/photo/crop-faceless-man-playing-video-game-on-smartphone-4293307/</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"ff9b39d0\" -->\n",
    "## Sécurité:\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"eec99309\" -->\n",
    "- Quelle est la probabilité d'échec d'un lancement de fusée?\n",
    "- Quelle est la probabilité que le taux de criminalité dans une ville augmente?\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"396cebb8\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/burglar.jpeg\"  width=\"300\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://pxhere.com/en/photo/1589039</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"79ecb9f5\" -->\n",
    "## Médecine:\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"73ad3990\" -->\n",
    "- Au triage à l'urgence, il faut estimer la probabilité qu'une personne blessée survive à une opération.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"f4fa7f68\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/medecine-personel.png\"  width=\"300\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://www.peoplematters.in/article/talent-management/heres-a-look-at-people-management-issues-in-healthcare-18553</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"a66bb1d4\" -->\n",
    "## Sciences sociales:\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"72b136f9\" -->\n",
    "- Quelle est la probabilité de retourner aux études et de compléter son programme?\n",
    "- Quelle était la probabilité de survie d'un passager lors du naufrage du Titanic?\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"0273c0ca\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/titanic-ship.jpeg\"  width=\"300\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://publicdomainpictures.net/en/view-image.php?image=63492&picture=wallpaper-clip-art</font>\n",
    "    </div>\n",
    "</div>\n",
    "<p>&nbsp;</p>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"4c19f122\" -->\n",
    "# <a id=origine-du-modèle-de-régression-logistique>Origine du modèle de régression logistique</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"3f33f9b9\" -->\n",
    "On sait tous par expérience qu'on améliore nos chances de passer un examen si on étudie à l'avance. Elles\n",
    "sont faibles si on étudie peu ou pas, et s'améliorent avec le nombre d'heures consacrées à l'étude. On sait également\n",
    "que le succès n'est pas assuré même avec un nombre d'heures important. Comment peut-on modéliser ce phénomène basé sur\n",
    "les observations de générations d'étudiants avant nous?\n",
    "\n",
    "Faisons l'hypothèse que la seule variable réellement importante $x$ est le nombre d'heures étudiées et que la réponse à prédire $y$ est la probabilité de réussite. La figure suivante\n",
    "montre les résultats d'un examen donné avec l'évènement $E=\\{0, 1\\}$ où $E=1$ implique un examen réussi. Les deux observations\n",
    "précédentes sont reproduites; plus on étudie, meilleures sont les chances de succès, mais cela n'empêche pas\n",
    "certains étudiants d'échouer malgré plusieurs heures consacrées à l'étude. De plus, pour un même nombre d'heures étudiées,\n",
    "certains passent, d'autres non.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"59fedc69\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/logistic-regression-origin.jpeg\"  width=\"500\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://en.wikipedia.org/wiki/Logistic_regression#Applications</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"fc7f4d87\" -->\n",
    "La distribution inhabituelle des points $(x,y)$ indique qu'on ne peut pas modéliser $y$ comme une simple relation\n",
    "univoque $y=f(x)$, car plusieurs valeurs de $y$ correspondent à une même valeur de $x$. La courbe représente\n",
    "la probabilité de passage de l'examen; c'est ce que nous devons modéliser. Elle a une forme en \"S\"\n",
    "et porte le nom de sigmoïde pour cette raison.\n",
    "\n",
    "> À noter que la lettre grecque sigma correspond à la lettre latine $s$.\n",
    "\n",
    "La fonction logistique est un exemple de fonction sigmoïde couramment utilisée. Elle est définie comme suit.\n",
    "\n",
    "$$y = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "La fonction logistique est bornée entre 0 et +1 lorsque x varie entre $-\\infty$ et $+\\infty$. On l'utilise\n",
    "généralement en remplaçant l'argument $x$ par une fonction arbitraire $f(x,\\Theta)$ qui augmente ou\n",
    "diminue de façon monotone.\n",
    "\n",
    "$$y = \\frac{1}{1+e^{-f(x,\\Theta)}} $$\n",
    "où $\\Theta$ représente l'ensemble des coefficients de la fonction $f$.\n",
    "\n",
    "Cette fonction est utilisée à la fois dans les problèmes de régression et de classification. Dans les deux cas,\n",
    "les coefficients $\\Theta$ doivent être ajustés afin de recaler des données expérimentales à un modèle. La\n",
    "similarité entre les deux approches contribue à la confusion associée au nom de la méthode de classification.\n",
    "\n",
    "On retrouve souvent la fonction logistique en biologie. On s'en sert pour modéliser les phénomènes de croissance des populations bactérienne et virale en fonction du temps. C'est un exemple de régression où l'on cherche à recaler un modèle théorique de croissance à des données $(N,t)$ expérimentales où $N$ est le nombre d'organismes observés après un temps $t$.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"0ccb4f2f\" -->\n",
    "# <a id=estimation-des-paramètres-de-la-régression-logistique>Estimation des paramètres de la régression logistique</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"d5fd7fe6\" -->\n",
    "En régression logistique, la fonction à minimiser est l'entropie croisée (*cross-entropy loss*). Celle-ci provient d'un modèle linéaire généralisé avec une distribution de Bernouilli.\n",
    "\n",
    "$$S(\\Theta)= S(a_{0}, a_{1}, \\cdots, a_{N})=-\\frac{1}{n}\\sum\\limits_{i=1}^{n}[y_{i}\\log{\\hat y_{i}} +(1-y_{i})\\log{(1-\\hat y_{i})}]$$\n",
    "\n",
    "où les $y_{i}=\\{0, 1\\}$ sont les observations et les $\\hat y_{i}$ sont les prédictions du modèle général\n",
    "utilisé en régression logistique multivariable\n",
    "\n",
    "$$\\hat y_{i} = p(1\\vert X_{i},\\Theta) = {\\dfrac  {1}{1+e^{-f(x_{1},\\cdots, x_{N},\\Theta)}}}.$$\n",
    "\n",
    "Contrairement à la méthode des moindres carrés, on ne peut obtenir d'expressions exactes pour les paramètres\n",
    "$\\Theta$ en minimisant l'entropie croisée avec $\\nabla S(\\Theta) = 0$. Il faut utiliser des méthodes d'optimisation numérique. La classe\n",
    "[`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "de la librairie Scikit-Lean calcule les paramètres $\\Theta$  pour nous.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"d144e94e\" -->\n",
    "## <a id=quelle-est-lintuition-derrière-la-fonction-de-perte>Quelle est l'intuition derrière la fonction de perte?</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"59beff2f\" -->\n",
    "Voyons comment elle fonctionne. Pour une donnée à classifier $(X_i,y_i)$ on a\n",
    "$$S_i = -[y_{i}\\log{\\hat y_{i}} +(1-y_{i})\\log{(1-\\hat y_{i})}]$$\n",
    "\n",
    "que l'on peut réécrire comme suit\n",
    "\n",
    "$$S_i = -y_{i}\\log{p(1|X)} -(1-y_{i})\\log{p(0|X)}.$$\n",
    "\n",
    "\n",
    "Si la classe réelle vaut 1 ($y_i=1$), on a\n",
    "$$S_i = -\\log{p(1|X)}.$$\n",
    "\n",
    "\n",
    "Si la classe réelle vaut 0, ($y_i=0$), on a\n",
    "$$S_i = -\\log{p(0|X)}.$$\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"3b8516b0\" -->\n",
    "### <a id=que-vaut-la-perte-s_i-lors-dune-classification-parfaite>Que vaut la perte $S_i$ lors d'une classification parfaite?</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"609506e1\" -->\n",
    "Dans ce cas on a $p(0|X)=1$ ou $p(1|X)=1$, soit\n",
    "\n",
    "$$S_i=-\\log{1}=0.$$\n",
    "\n",
    "La perte associée est nulle, quelle que soit la classe.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"48772d62\" -->\n",
    "### <a id=que-vaut-la-perte-s_i-lors-dune-mauvaise-classification>Que vaut la perte $S_i$ lors d'une mauvaise classification?</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"53a6b502\" -->\n",
    "Faisons l'hypothèse que la classification est tellement mauvaise que la probabilité associée est de $1~\\%$.Dans ce cas on a la prochaine équation, quelle que soit la classe\n",
    "\n",
    "$$S_i=-\\log{0,01}=2$$\n",
    "\n",
    "où on a utilisé un logarithme en base 10 pour simplifier l'analyse. La perte est d'autant plus grande que la probabilité est faible.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"376bd30f\" -->\n",
    "### <a id=en-résumé>En résumé</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"9efd8c3e\" -->\n",
    "La fonction de perte logistique assigne une valeur nulle aux bonnes classifications et une valeur positive\n",
    "aux mauvaises classifications. En minimisant la perte totale, $S=\\sum\\limits_{i=1}^{n}S_i$, on force le classificateur à faire de bonnes classifications. Il fallait y penser!\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"47b0aebf\" -->\n",
    "# <a id=exemple-de-classification-2-d>Exemple de classification 2-D</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"d6b3bcc3\" -->\n",
    "Dans ce suit, nous allons montrer un exemple simple où la réponse $y$\n",
    "dépend de deux variables $x_{1}$ et $x_{2}$. On a\n",
    "\n",
    "$$p(1|X,\\Theta) = {\\dfrac  {1}{1+e^{-(a_{0}+a_{1}x_{1}+a_{2}x_{2})}}}$$\n",
    "\n",
    "La frontière entre les classes 0 et 1 est située aux endroits où la probabilité $p(1|X,\\Theta)=50\\%$, ce qui se produit lorsque l'argument de l'exponentielle est nul. On obtient alors l'équation de la courbe délimitant la frontière entre les deux classes:\n",
    "\n",
    "$$x_{2} = -\\dfrac{(a_{0}+a_{1}x_{1})}{a_{2}}$$\n",
    "\n",
    "Les coefficients $a_{1}$ sont calculés lors de l'entraînement du classificateur. Cet exemple s'inspire du\n",
    "[blogue](https://thedatafrog.com/fr/articles/logistic-regression-neural-network/) suivant.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"48c852c3\" -->\n",
    "Pour l'exemple, on utilise deux distributions gaussiennes similaires se superposant légèrement. On assigne\n",
    "les étiquettes 0 et 1 aux données composant chacune.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c085841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération et affichage des données\n",
    "\n",
    "# Distribution gaussienne en 2D\n",
    "normal = np.random.multivariate_normal\n",
    "\n",
    "# Génère 500 valeurs de x1 et de x2\n",
    "n = 500\n",
    "x1 = normal([-1.0, -1.0], [[1, 0.0], [0.0, 1]], n)\n",
    "x2 = normal([1.0, 1.0], [[1, 0.0], [0.0, 1]], n)\n",
    "\n",
    "# Génère les étiquettes de x1 et x2\n",
    "y1 = np.zeros((n,))\n",
    "y2 = np.ones((n,))\n",
    "\n",
    "# Génère l'ensemble d'entraînement (X, y)\n",
    "X_train = np.concatenate((x1, x2))\n",
    "y_train = np.concatenate((y1, y2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac61af0",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"595e9c0f\" -->\n",
    "La figure suivante montre l'ensemble de données d'entraînement composé des deux nuages de points appartenant\n",
    "aux classes 0 et 1.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186f5361",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x1[:, 0], x1[:, 1], alpha=0.5, label=\"classe 0\")\n",
    "ax.scatter(x2[:, 0], x2[:, 1], alpha=0.5, label=\"classe 1\")\n",
    "ax.set_xlabel(\"x1\", fontsize=20)\n",
    "ax.set_ylabel(\"x2\", fontsize=20, rotation=0)\n",
    "ax.legend(loc=\"upper left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd9bcb",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération d'une grille couvrant la même plage de valeurs en $x_{1}$ et $x_{2}$\n",
    "\n",
    "xmin, xmax, npoints = (-5, 5, 51)\n",
    "linx1 = np.linspace(xmin, xmax, npoints)\n",
    "\n",
    "gridx1, gridx2 = np.meshgrid(linx1, linx1)\n",
    "grid = np.c_[gridx1.flatten(), gridx2.flatten()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f874dd9e",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"a07e6a67\" -->\n",
    "Dans la prochaine cellue, nous initialisons le classificateur à l'aide de Scikit-learn. Nous utilisons la fonction d'optimisation numérique LBFGS pour estimer les paramètres du modèle.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9509d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver=\"lbfgs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c4027",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da41f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du classificateur\n",
    "\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f41ff4",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"493606d9\" -->\n",
    "Par la suite, nous effectuons les prédictions à l'aide du classificateur. Nous obtenons la probabilité locale qu'un point $(x_{1}, x_{2})$ appartienne à la classe 1.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba0ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = clf.predict_proba(grid)\n",
    "z2 = prob[:, 1].reshape(npoints, npoints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9aac4b",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"0061f3f0\" -->\n",
    "La figure suivante montre la distribution de probabilité d'appartenance à la classe 1 ($P(1|X)$) après\n",
    "l'entraînement du classificateur.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0e954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.plot_wireframe(gridx1, gridx2, z2, alpha=0.25, color=\"k\")\n",
    "\n",
    "ax.scatter3D(x1[:, 0], x1[:, 1], y1)\n",
    "ax.scatter3D(x2[:, 0], x2[:, 1], y2)\n",
    "ax.set_xlabel(\"x2\", fontsize=20)\n",
    "ax.set_ylabel(\"x1\", fontsize=20)\n",
    "ax.zaxis.set_rotate_label(False)\n",
    "ax.set_zlabel(\"$P(1|X)$\", rotation=90, fontsize=18)\n",
    "\n",
    "ax.invert_xaxis()\n",
    "ax.invert_yaxis()\n",
    "ax.view_init(20, 35)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a3c5d9",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"2c5d630e\" -->\n",
    "Comme dans le cas de la méthode des moindres carrés, on cherche à réduire la distance verticale entre chaque mesure $y_{i}$\n",
    "et la surface modélisée. La régression logistique procède légèrement différemment. Si $y_{i}=0$, la distance est prise sous la courbe.\n",
    "Si $y_{i}=1$, la distance est prise au-dessus de la courbe. La minimisation\n",
    "de la fonction $S(\\Theta)$ rapproche la surface des observations. C'est ce que l'on observe ci-dessus.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"aa5797b6\" -->\n",
    "Par la suite, nous affichons la zone d'influence de chaque classe. C'est-à-dire, la localisation de la zone où la probabilitée est de $50~\\%$.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fdb015",
   "metadata": {},
   "outputs": [],
   "source": [
    "linx2 = -(clf.intercept_ + clf.coef_[0][0] * linx1) / clf.coef_[0][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d588b2e4",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"5f41f13c\" -->\n",
    "La figure suivante montre les résultats précédents en couleurs en y superposant la frontière,\n",
    "la ligne blanche pointillée, entre les deux classes.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f928bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "im = ax.pcolor(\n",
    "    gridx1, gridx2, prob[:, 1].reshape(npoints, npoints), shading=\"auto\", cmap=\"viridis\"\n",
    ")\n",
    "cb = plt.colorbar(im)\n",
    "ax.scatter(x1[:, 0], x1[:, 1], alpha=0.5)\n",
    "ax.scatter(x2[:, 0], x2[:, 1], alpha=0.5)\n",
    "ax.set_xlabel(\"x1\", fontsize=20)\n",
    "ax.set_ylabel(\"x2\", fontsize=20, rotation=0)\n",
    "ax.plot(linx1, linx2, \"w\", linestyle=\"dashed\")\n",
    "ax.set_ylim([-5, 5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82ae037",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"47476328\" -->\n",
    "Si l'on utilisait maintenant un ensemble de test, chaque couple de valeurs\n",
    "$(x_{1},x_{2})$ serait classifié selon la zone où il est situé. Ainsi, la\n",
    "régression logistique sépare le domaine des variables $x_{1}$ et $x_{2}$ en deux zones lorsqu'il y a deux classes.\n",
    "La frontière est linéaire, car la fonction $f(x_{1},x_{2},\\Theta)$ est linéaire. Si l'on ajoutait le terme\n",
    "d'interaction $x_{1}x_{2}$ et des puissances $x_{i}^2$, la frontière deviendrait plus complexe.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"ca14d30b\" -->\n",
    "# <a id=identification-des-facteurs-importants>Identification des facteurs importants</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"2434035b\" -->\n",
    "La régression logistique permet non seulement de prédire la probabilité d'un évènement $E$, mais aussi d'identifier les\n",
    "principaux facteurs dans le modèle qui contrôlent ce choix. C'est possible si les facteurs ont été préalablement normalisés lors de l'étape\n",
    "de prétraitement. Ce sujet est abordé plus en détail dans le module sur la méthodologie. Dans ce qui suit, nous\n",
    "allons supposer que cette étape essentielle a été effectuée.\n",
    "\n",
    "Selon la formulation standard de la régression logistique, on a\n",
    "\n",
    "$$p(E|X, \\Theta)= {\\dfrac {1}{1+e^{-(a_{0}+a_{1}x_{1}+\\cdots + a_{N} x_{N})}}}$$\n",
    "\n",
    "Plus grande sera la valeur absolue d'un paramètre $a_{i}$, plus important sera le facteur $x_{i}$ associé! Si l'on ordonne\n",
    "les valeurs des $|a_{i}|$ en ordre décroissant, on obtiendra les facteurs $x_{i}$ en ordre décroissant d'importance.\n",
    "C'est ainsi que l'on détermine les facteurs $x_{i}$ d'intérêt dans un problème donné. L'effet de\n",
    "chaque facteur $x_{i}$ est déterminé ensuite par le signe du paramètre associé $a_{i}$.\n",
    "\n",
    "<!-- #endregion -->\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
