{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b0a772",
   "metadata": {},
   "source": [
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    formats: md,ipynb\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.16.0\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---\n",
    "\n",
    "<!-- #region id=\"496978f4\" -->\n",
    "# Table des matières\n",
    "1. [Lecture des données](#lecture-des-données)\n",
    "1. [Premier pipeline](#première-pipeline)\n",
    "    1. [Définition du pipeline](#définition-du-pipeline)\n",
    "    1. [Entraînement du pipeline avec les données d'entraînement](#entraînement-du-pipeline-avec-les-données-dentraînement)\n",
    "      1. [Important](#important)\n",
    "    1. [Évaluation de ses performances en classification sur l'ensemble de test](#évaluation-de-ses-performances-en-classification-sur-lensemble-de-test)\n",
    "1. [Deuxième pipeline](#deuxième-pipeline)\n",
    "    1. [Définition du pipeline](#définition-du-pipeline-1)\n",
    "    1. [Définition de la recherche sur grille](#définition-de-la-recherche-sur-grille)\n",
    "    1. [Entraînement du pipeline avec chaque combinaison de paramètres](#entraînement-du-pipeline-avec-chaque-combinaison-de-paramètres)\n",
    "    1. [Affichage des paramètres du pipeline optimal](#affichage-des-paramètres-du-pipeline-optimal)\n",
    "1. [Troisième pipeline](#troisième-pipeline)\n",
    "    1. [Définition du pipeline](#définition-du-pipeline-2)\n",
    "    1. [Définition de la recherche sur grille](#définition-de-la-recherche-sur-grille)\n",
    "    1. [Entraînement du pipeline avec chaque combinaison de paramètres](#entraînement-du-pipeline-avec-chaque-combinaison-de-paramètres-1)\n",
    "    1. [Affichage des paramètres du pipeline optimal](#affichage-des-paramètres-du-pipeline-optimal-1)\n",
    "1. [Diviser pour conquérir](#diviser-pour-conquérir)\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9793742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, decomposition, discriminant_analysis, manifold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Élimination des avertissements (warnings). On en observe régulièrement lorsqu'on utilise\n",
    "# une méthode de recherche d'hyperparamètres. Plusieurs des entraînements associés\n",
    "# ne convergent pas et un message d'avertissement est affiché pour chacun.\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "seed = 42\n",
    "\n",
    "rng_seed = seed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b18fc",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"6ba1bb53\" -->\n",
    "On a vu, dans les modules précédents sur le prétraitement des données, les différentes étapes de\n",
    "nettoyage, d'imputation de données manquantes, de normalisation et de redimensionnement.\n",
    "Dans chaque cas, on étudiait l'étape séparément,\n",
    "hors d'un contexte global, afin d'apprendre à l'utiliser dans des situations précises.\n",
    "Lorsqu'on finit par maîtriser chaque étape, il devient intéressant de pouvoir\n",
    "en combiner plusieurs, sinon toutes, afin d'élaborer un processus de prétraitement plus spécialisé.\n",
    "\n",
    "Imaginons un projet où l'on doit effectuer chacune des opérations de prétraitement apparaissant dans\n",
    "la figure suivante avant même de passer à la phase de classification (de régression, de regroupement de données, etc.).\n",
    "Il nous faudrait optimiser chacune des étapes en tenant compte des autres afin de maximiser les performances\n",
    "de notre application finale.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"561d3d77\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/data-processing.png\"  width=\"500\" />\n",
    "    <div>\n",
    "    <font size=\"1.5\">Image Source: http://pzs.dstu.dp.ua/DataMining/preprocessing/bibl/Data%20Preprocessing%20in%20Data%20Mining.pdf/</font>\n",
    "    </div>\n",
    "</div>\n",
    "<p>&nbsp;</p>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"bfee40f1\" -->\n",
    "C'est exactement ce que permet de faire l'optimisation d'un pipeline d'analyse des données! La majorité des projets\n",
    "devraient éventuellement passer par cette étape.\n",
    "Elle est efficace sur le plan de la gestion et du développement d'un projet complexe. Idéalement,\n",
    "on devrait ne modifier que quelques sections d'un pipeline lorsqu'on veut tester une nouvelle idée\n",
    "ou implémenter un nouvel algorithme d'analyse. Pas besoin de recommencer un nouveau programme à\n",
    "chaque fois; la roue une fois inventée n'a besoin que d'être poussée dans une nouvelle direction.\n",
    "\n",
    "Le concept de pipeline d'analyse des données est un des joyaux de la librairie Scikit-learn, rien de moins. Nous\n",
    "allons apprendre comment en construire plusieurs et comment les optimiser pour une tâche donnée. Un pipeline\n",
    "simplifie considérablement la tâche des programmeurs. Apprenez à les utiliser!\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"70fa1d81\" -->\n",
    "# <a id=lecture-des-données>Lecture des données</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"ab120b30\" -->\n",
    "Nous allons utiliser à nouveau un sous-ensemble du jeu de données\n",
    "[**MNIST**](https://en.wikipedia.org/wiki/MNIST_database)\n",
    "qui comprend des images de chiffres 0 à 9 de taille $8 \\text{ par } 8$. Les images du jeu de données original sont de taille $28 \\text{ par } 28$. Nous allons n'utiliser que les images des chiffres 0 à 5.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3374aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du jeu de données et séparation de celles-ci en ensembles d'entraînement et de test\n",
    "\n",
    "digits = datasets.load_digits(n_class=6)\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "n_samples, n_features = X.shape\n",
    "n_neighbors = 30\n",
    "n_components = 2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=rng_seed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a22e9b",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf910887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de quelques images de chiffres pour chaque classe (0 à 5)\n",
    "\n",
    "n_img_per_row = 20\n",
    "img = np.zeros((10 * n_img_per_row, 10 * n_img_per_row))\n",
    "for i in range(n_img_per_row):\n",
    "    ix = 10 * i + 1\n",
    "    for j in range(n_img_per_row):\n",
    "        iy = 10 * j + 1\n",
    "        img[ix : ix + 8, iy : iy + 8] = X[i * n_img_per_row + j].reshape((8, 8))\n",
    "\n",
    "plt.imshow(img, cmap=plt.cm.binary)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Une sélection des 6 premiers chiffres du jeu de données MNIST\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcf6057",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"16eab83e\" -->\n",
    "# <a id=première-pipeline>Premier pipeline</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"9ab94536\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/pipeline-photo.jpeg\"  width=\"400\" />\n",
    "    <div>\n",
    "    <font size=\"1.5\">Image Source: https://www.bdcmagazine.com/2020/08/the-importance-of-pipeline-repair-services//</font>\n",
    "    </div>\n",
    "</div>\n",
    "<p>&nbsp;</p>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"ecd550ce\" -->\n",
    "Comme premier essai, construisons un pipeline de base combinant deux étapes de prétraitement, suivie d'une étape\n",
    "de classification. On utilise ici un classificateur de type\n",
    "[SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) non vu dans la formation.\n",
    "Le type de classificateur n'est pas vraiment important dans cette série d'exemples. D'ailleurs, il\n",
    "recommandé d'en essayer plusieurs lors d'un projet et de choisir le meilleur parmi eux. Dans le cas du classificateur SVM,\n",
    "il a deux hyperparamètres à ajuster: $\\alpha$ et $C$.\n",
    "\n",
    "Nous allons effectuer les étapes suivantes:\n",
    "\n",
    "- normalisation standard,\n",
    "- redimensionnement avec la méthode PCA,\n",
    "- classification avec la méthode SVM. \n",
    "\n",
    "Chaque opération utilisera ses paramètres par défaut ou ceux spécifiés dans l'appel de la fonction.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"3332b350\" -->\n",
    "## <a id=définition-du-pipeline-1>Définition du pipeline</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"e78eabf0\" -->\n",
    "Les noms (« `\"transformation\"` », « `\"reduce_dim\"` », « `\"classify\"` ») utilisés dans la définition du pipeline sont arbitraires. Vous\n",
    "pouvez les changer, mais faites en sorte qu'ils signifient quelque chose de plus utile que « testA », « Bozo », ou autres noms avec peu de signification.\n",
    "\n",
    "Après chaque nom apparaît le nom de l'opérateur associé, par exemple `'StandardScaler'` qui effectue la\n",
    "normalisation standard. Puis, apparaissent les hyperparamètres sélectionnés différents de ceux par\n",
    "défaut de chaque opérateur.\n",
    "\n",
    "L'opérateur de réduction de dimensionnalité indique qu'on ne garde que les deux premières composantes\n",
    "après la transformation en composantes principales.\n",
    "\n",
    "L'opérateur de classification SVC indique qu'on utilise un classificateur de type SVM avec quatre\n",
    "hyperparamètres d'initialisations.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"transformation\", StandardScaler()),\n",
    "        (\"reduce_dim\", decomposition.PCA(n_components=2)),\n",
    "        (\"classify\", SVC(kernel=\"rbf\", C=1, gamma=0.2, max_iter=1000)),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79764cf",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"2d0df9a4\" -->\n",
    "## <a id=entraînement-du-pipeline-avec-les-données-dentraînement>Entraînement du pipeline avec les données d'entraînement</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"fd5c857b\" -->\n",
    "Cette opération calcule les paramètres suivants:\n",
    "\n",
    "\n",
    "- coefficients de la transformation de normalisation standard,\n",
    "- coefficients de la matrice de conversion en composantes principales\n",
    "ne tenant compte que des deux premières composantes,\n",
    "- coefficients du classificateur SVM.\n",
    "\n",
    "\n",
    "Une fois entrainés, les différents opérateurs pourront être appliqués à de nouvelles données, celles de test.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0296b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5495022",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"eb6f93cb\" -->\n",
    "### <a id=important>Important</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"78652762\" -->\n",
    "Si l'on désire traiter un nouvel ensemble de données `X_new`, il ne reste plus qu'à faire ceci:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798bb4fc",
   "metadata": {},
   "source": [
    "\n",
    "Si l'on désire calculer le score associé, on fait ceci:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626a97a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pipeline.score(X_new, y_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622587f6",
   "metadata": {},
   "source": [
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"24c7737d\" -->\n",
    "## <a id=évaluation-de-ses-performances-en-classification-sur-lensemble-de-test>Évaluation de ses performances en classification sur l'ensemble de test</a>\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nScore en entraînement: %0.1f %%\" % (100 * pipeline.score(X_train, y_train)))\n",
    "print(\"\\nScore en test: %0.1f %%\" % (100 * pipeline.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6667a291",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"ecfff660\" -->\n",
    "# <a id=deuxième-pipeline>Deuxième pipeline</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"7d426a7b\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/pipeline-photo2.jpeg\"  width=\"400\" />\n",
    "    <div>\n",
    "    <font size=\"1.5\">Image Source: https://pxhere.com/en/photo/1081470/</font>\n",
    "    </div>\n",
    "</div>\n",
    "<p>&nbsp;</p>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"c13af654\" -->\n",
    "Supposons maintenant qu'on ne soit pas certain que la normalisation standard soit la meilleure méthode de\n",
    "transformation des données. On aimerait en tester au moins deux:\n",
    "\n",
    "\n",
    "- normalisation standard,\n",
    "- transformation par les quantiles (`QuantileTransformer`).\n",
    "\n",
    "\n",
    "On voudrait garder inchangées les étapes précédentes de redimensionnement et de classification.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"16c2984e\" -->\n",
    "## <a id=définition-du-pipeline-2>Définition du pipeline</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"89d7d6e0\" -->\n",
    "Puisqu'il y a deux méthodes de transformation de données, la grille des paramètres suivante contient deux dictionnaires de paramètres. On peut ainsi considérer qu'il y a deux mini pipelines; un pour chaque méthode de transformation. On procède en utilisant le mot clé `'passthrough'` qui permet de spécifier chacun des choix à essayer pour l'étape de transformation des données.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca156f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"transformation\", \"passthrough\"),\n",
    "        (\"reduce_dim\", decomposition.PCA(n_components=2)),\n",
    "        (\"classify\", SVC(kernel=\"rbf\", C=1, gamma=0.2, max_iter=1000)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Grille des paramètres à modifier. le mot clé passthrough renvoie ici\n",
    "param_grid = [\n",
    "    {\"transformation\": [StandardScaler()]},\n",
    "    {\n",
    "        \"transformation\": [\n",
    "            QuantileTransformer(output_distribution=\"normal\", n_quantiles=50)\n",
    "        ]\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e9b67d",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"90c10d06\" -->\n",
    "Il faut maintenant tester les deux mini pipelines afin de trouver le plus performant des deux. Pour\n",
    "cela, on utilisera la fonction\n",
    "[`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.predict)\n",
    "qui permet d'effectuer une recherche sur une grille d'hyperparamètres qui sont\n",
    "dans ce cas-ci les deux méthodes de transformation.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"208e9e6a\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/light-bulb-idea.jpeg\"  width=\"200\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://basementdesigner.com/basement-finishing-102/light-bulb-idea//</font>\n",
    "    </div>\n",
    "</div>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"57284d80\" -->\n",
    "`GridSearchCV` procède par validation croisée sur l'ensemble d'entraînement. Les principes de la validation\n",
    "croisée sont expliqués dans la partie 5. En gros, elle permet de choisir le plus performant parmi\n",
    "plusieurs modèles, dans ce cas-ci, entre les deux mini pipelines.\n",
    "\n",
    "Enfin, `GridSearchCV` réentraîne le pipeline optimal en utilisant cette fois-ci toutes les données\n",
    "d'entraînement, sans validation croisée.\n",
    "\n",
    "\n",
    "Si l'on désire traiter un nouvel ensemble de données `X_new`, il ne reste plus qu'à faire ceci:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800bf501",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2401a4",
   "metadata": {},
   "source": [
    "\n",
    "Si l'on désire calculer le score associé, on fait ceci:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a5dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = grid_search.score(X_new, y_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad936ce",
   "metadata": {},
   "source": [
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"4772afd4\" -->\n",
    "## <a id=définition-de-la-recherche-sur-grille>Définition de la recherche sur grille</a>\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    pipeline, param_grid=param_grid, n_jobs=2, verbose=1, refit=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a78097",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"27c28bed\" -->\n",
    "## <a id=entraînement-du-pipeline-avec-chaque-combinaison-de-paramètres-1>Entraînement du pipeline avec chaque combinaison de paramètres</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"010e75b1\" -->\n",
    "L'affichage qui suit indique que 10 *fits* (entraînement) ont été effectués. Il y avait 2 modèles à\n",
    "tester et 5 repliements pour chacun lors de la validation croisée, soit un total de 10 entraînements.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdeb4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df24dc",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"d29f11b1\" -->\n",
    "## <a id=affichage-des-paramètres-du-pipeline-optimal-1>Affichage des paramètres du pipeline optimal</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"f1e42289\" -->\n",
    "Définissons une fonction permettant d'afficher les paramètres.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00b2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def afficheMeilleurChoixParametres(grid_search):\n",
    "    print(\"\\nMeilleur choix de paramètres:\")\n",
    "    steps = dict(grid_search.best_estimator_.steps)\n",
    "\n",
    "    for param_name in sorted(steps.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, steps[param_name]))\n",
    "\n",
    "    print(\"\\nScore optimal en entraînement: %0.1f %%\" % (100 * grid_search.best_score_))\n",
    "    print(\n",
    "        \"\\nScore en test avec le pipeline optimal: %0.1f %%\\n\"\n",
    "        % (100 * grid_search.score(X_test, y_test))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e67d720",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7677877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "afficheMeilleurChoixParametres(grid_search)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3aedbc",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"532001c3\" -->\n",
    "# <a id=troisième-pipeline>Troisième pipeline</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"3942feef\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/pipeline-photo3.jpeg\"  width=\"400\" />\n",
    "    <div>\n",
    "    <font size=\"1.5\">Image Source: https://www.wallpaperflare.com/search?wallpaper=pipeline/</font>\n",
    "    </div>\n",
    "</div>\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"b08a5b62\" -->\n",
    "Se pourrait-il que le choix précédent de la normalisation standard, par rapport à l'autre méthode de transformation, ait\n",
    "été influencé par les valeurs des paramètres des deux autres opérations? C'est très pertinent\n",
    "comme question. On va le vérifier.\n",
    "\n",
    "On va optimiser à nouveau le pipeline en faisant varier les facteurs suivants:\n",
    "\n",
    "\n",
    "- Transformation:\n",
    "   - normalisation standard,\n",
    "   - transformation par les quantiles (`QuantileTransformer`). \n",
    "- Réduction de la dimensionnalité:\n",
    "    - `n_components` $= [2, 4, 8, 16, 32]$\n",
    "- Classificateur SVM:\n",
    "    - `C` $= [0.1, 1, 10, 100, 1000]$,\n",
    "    - `gamma` $= [1, 0.1, 0.01, 0.001, 0.0001]$.\n",
    "\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"d520bf6f\" -->\n",
    "## <a id=définition-du-pipeline>Définition du pipeline</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"999d21ec\" -->\n",
    "Puisqu'il y a à nouveau deux méthodes de transformation de données, la grille des paramètres suivante contient deux dictionnaires de paramètres. Il y a encore deux mini pipelines.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555695f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"transformation\", \"passthrough\"),\n",
    "        (\"reduce_dim\", decomposition.PCA()),\n",
    "        (\"classify\", SVC(kernel=\"rbf\", max_iter=1000)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"transformation\": [StandardScaler()],\n",
    "        \"reduce_dim__n_components\": [2, 4, 8, 16, 32],\n",
    "        \"classify__C\": [0.1, 1, 10, 100, 1000],\n",
    "        \"classify__gamma\": [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    },\n",
    "    {\n",
    "        \"transformation\": [\n",
    "            QuantileTransformer(output_distribution=\"normal\", n_quantiles=50)\n",
    "        ],\n",
    "        \"reduce_dim__n_components\": [2, 4, 8, 16, 32],\n",
    "        \"classify__C\": [0.1, 1, 10, 100, 1000],\n",
    "        \"classify__gamma\": [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b5ba6b",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"4d68f200\" -->\n",
    "## <a id=définition-de-la-recherche-sur-grille>Définition de la recherche sur grille</a>\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a956f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    pipeline, param_grid=param_grid, n_jobs=2, verbose=1, refit=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f69fa",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"c7ac6f3c\" -->\n",
    "## <a id=entraînement-du-pipeline-avec-chaque-combinaison-de-paramètres>Entraînement du pipeline avec chaque combinaison de paramètres</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"4614cd6d\" -->\n",
    "L'affichage qui suit indique que 1 250 entraînements ont été effectués. Il y avait 2 modèles à tester, $5 \\times 5 \\times 5=125$\n",
    "combinaisons d'hyperparamètres pour chacun, et 5 repliements pour chacun lors de la validation croisée,\n",
    "soit un total de $2\\times 125\\times 5= 1\\ 250$ entraînements.\n",
    "\n",
    "> Prends un peu de temps à être exécuter étant donné qu'il y a 1 250 entraînements à faire.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c771e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbaf0a5",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"c28d08fc\" -->\n",
    "## <a id=affichage-des-paramètres-du-pipeline-optimal>Affichage des paramètres du pipeline optimal</a>\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d1ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "afficheMeilleurChoixParametres(grid_search)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd37d2cf",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"80511ee6\" -->\n",
    "Le score en test est passé de $83,8~\\%$ à $98,9~\\%$!\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"4592e89f\" -->\n",
    "# <a id=diviser-pour-conquérir>Diviser pour conquérir</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"f37bb5a2\" -->\n",
    "Bien qu'il soit pratique de nettoyer ses données et de les classifier dans un même pipeline, ce\n",
    "n'est généralement pas la meilleure stratégie.\n",
    "\n",
    "Vous devriez plutôt utiliser un pipeline pour trouver la bonne méthode de prétraitement puis l'implémenter.\n",
    "Ça vous permettrait de vous concentrer ensuite sur la partie la plus intéressante de votre projet c.-à-d. la\n",
    "classification, la régression, etc. Créez un second pipeline pour traiter cette nouvelle partie\n",
    "d'un projet. Inutile de refaire le prétraitement des données à chaque fois.\n",
    "\n",
    "<!-- #endregion -->\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
