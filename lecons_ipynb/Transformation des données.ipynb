{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cf8cdc4",
   "metadata": {},
   "source": [
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    formats: md,ipynb\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.16.0\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---\n",
    "\n",
    "<!-- #region id=\"b40a2694\" -->\n",
    "# Table des matières\n",
    "1. [Importance de la normalisation des données](#importance-de-la-normalisation-des-données)\n",
    "1. [Rééchelonnage ou normalisation](#rééchelonnage-ou-normalisation)\n",
    "    1. [Lecture de la base de données](#lecture-de-la-base-de-données)\n",
    "    1. [Méthodes de transformation](#méthodes-de-transformation)\n",
    "      1. [Données brutes](#données-brutes)\n",
    "      1. [Standard](#standard)\n",
    "      1. [Min-max](#min-max)\n",
    "      1. [Max-abs](#max-abs)\n",
    "      1. [Robuste (basé sur les quantiles 1, 2 et 3)](#robuste-basé-sur-les-quantiles-1-2-et-3)\n",
    "      1. [Transformation par puissance](#transformation-par-puissance)\n",
    "      1. [Transformation quantile à quantile](#transformation-quantile-à-quantile)\n",
    "      1. [Normalisation vectorielle](#normalisation-vectorielle)\n",
    "    1. [Comparaison entre les différents rééchantillonneurs](#comparaison-entre-les-différents-rééchantillonneurs)\n",
    "1. [Le traitement des variables catégorielles](#le-traitement-des-variables-catégorielles)\n",
    "  1. [Regroupement de catégories en une seule valeur](#regroupement-de-catégories-en-une-seule-valeur)\n",
    "  1. [Regroupement de variables numériques en catégories](#regroupement-de-variables-numériques-en-catégories)\n",
    "    1. [Catégories de taille égales](#catégories-de-taille-égales)\n",
    "    1. [Catégories par intervalles identiques](#catégories-par-intervalles-identiques)\n",
    "  1. [Encoder des variables catégorielles](#encoder-des-variables-catégorielles)\n",
    "    1. [Encoder une variable booléenne en la transformant en entier](#encoder-une-variable-booléenne-en-la-transformant-en-entier)\n",
    "    1. [Encoder manuellement en établissant les correspondances avec un dictionnaire](#encoder-manuellement-en-établissant-les-correspondances-avec-un-dictionnaire)\n",
    "    1. [Encoder automatiquement](#encoder-automatiquement)\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510cdeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    minmax_scale,\n",
    ")\n",
    "\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "seed = 42\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d713b",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"88ac5d01\" -->\n",
    "Il arrive souvent que les valeurs des variables $x_{i}$ dans les bases de données diffèrent de\n",
    "plusieurs ordres de grandeur. Beaucoup d'algorithmes d'apprentissage sont sensibles\n",
    "à ce phénomène. On transforme alors les données afin de les ramener dans des\n",
    "intervalles de valeurs pour lesquels certaines caractéristiques de la distribution (moyenne, médiane,\n",
    "minimum, maximum, etc.) deviennent similaires. Ces opérations sont réalisées par normalisation et\n",
    "par transformation non linéaire des données. Nous allons en voir plusieurs exemples dans ce module.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"e357d7ae\" -->\n",
    "# <a id=importance-de-la-normalisation-des-données>Importance de la normalisation des données</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region colab_type=\"text\" id=\"f0a07a3e\" -->\n",
    "La normalisation des données est une des opérations les plus importantes en préparation des données.\n",
    "Pour en donner une idée, supposons qu'on utilise le modèle de régression suivant pour prédire une réponse\n",
    "$y$ en fonction de deux variables $x_{1}$ et $x_{2}$.\n",
    "\n",
    "$$y \\approx a_{0}+a_{1}x_{1}+a_{2}x_{2}$$\n",
    "\n",
    "Si la variable $x_{1} \\in [0, 1]$ et la variable $x_{2} \\in [0, 1000]$, le dernier terme de\n",
    "l'équation précédente aura le plus d'impact sur la solution de la régression. Cela ne veut pas dire\n",
    "qu'il est le plus important. Par exemple si $x_{1}$ correspond à une distance mesurée en mètre et $x_{2}$ correspond\n",
    "à une autre distance mesurée en millimètres, alors tous les deux correspondent au même intervalle de longueur. Ils ont une\n",
    "importance égale si l'on tient compte des échelles respectives.\n",
    "\n",
    "La normalisation enlève les échelles et met les deux variables normalisées $\\tilde{x}_{1}$ et $\\tilde{x}_{2}$\n",
    "sur un même pied d'égalité. Si l'on effectue la régression avec le modèle utilisant les variables normalisées,\n",
    "on obtient\n",
    "\n",
    "$$y \\approx a_{0}+a_{1}\\tilde{x}_{1}+a_{2}\\tilde{x}_{2}.$$\n",
    "\n",
    "Dans ce cas-ci, les valeurs estimées des coefficients permettront de bien comparer entre elles les variables.\n",
    "Par exemple, si $a_{1} \\gg a_{2}$ alors la variable $x_{1}$ a un plus grand effet sur la réponse $y$ que la\n",
    "variable $x_{2}$. La normalisation permet une comparaison non biaisée des variables entre elles en plus de permettre\n",
    "aux coefficients de prendre toute leur importance. Il y a d'autres raisons justifiant l'étape de normalisation,\n",
    "mais celles-ci sont les plus importantes à retenir.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"f83e6594\" -->\n",
    "# <a id=rééchelonnage-ou-normalisation>Rééchelonnage ou normalisation</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"d9ace578\" -->\n",
    "La séquence suivante est inspirée des [exemples de code](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py) de la librairie Scikit-learn.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"b3a4ee50\" -->\n",
    "Le code ci-dessous sert à afficher les graphiques joliment.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5dd74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = getattr(cm, \"jet_r\", cm.jet_r)\n",
    "\n",
    "\n",
    "def create_axes(title, figsize=(16, 6)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    # Définition des axes pour la première figure.\n",
    "    left, width = 0.1, 0.22\n",
    "    bottom, height = 0.1, 0.7\n",
    "    bottom_h = height + 0.15\n",
    "    left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.1]\n",
    "    rect_histy = [left_h, bottom, 0.05, height]\n",
    "\n",
    "    ax_scatter = plt.axes(rect_scatter)\n",
    "    ax_histx = plt.axes(rect_histx)\n",
    "    ax_histy = plt.axes(rect_histy)\n",
    "\n",
    "    # Définition des axes pour la zone agrandie.\n",
    "    left = width + left + 0.2\n",
    "    left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.1]\n",
    "    rect_histy = [left_h, bottom, 0.05, height]\n",
    "\n",
    "    ax_scatter_zoom = plt.axes(rect_scatter)\n",
    "    ax_histx_zoom = plt.axes(rect_histx)\n",
    "    ax_histy_zoom = plt.axes(rect_histy)\n",
    "\n",
    "    # Définition des axes pour la barre de couleurs.\n",
    "    left, width = width + left + 0.13, 0.01\n",
    "\n",
    "    rect_colorbar = [left, bottom, width, height]\n",
    "    ax_colorbar = plt.axes(rect_colorbar)\n",
    "\n",
    "    return (\n",
    "        (ax_scatter, ax_histy, ax_histx),\n",
    "        (ax_scatter_zoom, ax_histy_zoom, ax_histx_zoom),\n",
    "        ax_colorbar,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_distribution(axes, X, y, hist_nbins=50, title=\"\", x0_label=\"\", x1_label=\"\"):\n",
    "    ax, hist_X1, hist_X0 = axes\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x0_label)\n",
    "    ax.set_ylabel(x1_label)\n",
    "\n",
    "    # Diagramme de points\n",
    "    colors = cmap(y)\n",
    "    ax.scatter(X[:, 0], X[:, 1], alpha=0.5, marker=\"o\", s=5, lw=0, c=colors)\n",
    "\n",
    "    # Histogramme pour l'axe X1 (feature 5)\n",
    "    hist_X1.set_ylim(ax.get_ylim())\n",
    "    hist_X1.hist(\n",
    "        X[:, 1],\n",
    "        bins=hist_nbins,\n",
    "        orientation=\"horizontal\",\n",
    "        color=\"grey\",\n",
    "        ec=\"grey\",\n",
    "    )\n",
    "    hist_X1.axis(\"off\")\n",
    "\n",
    "    # Histogramme pour l'axe X0 (feature 0)\n",
    "    hist_X0.set_xlim(ax.get_xlim())\n",
    "    hist_X0.hist(\n",
    "        X[:, 0],\n",
    "        bins=hist_nbins,\n",
    "        orientation=\"vertical\",\n",
    "        color=\"grey\",\n",
    "        ec=\"grey\",\n",
    "    )\n",
    "    hist_X0.axis(\"off\")\n",
    "\n",
    "\n",
    "def make_plot(title, X):\n",
    "    ax_zoom_out, ax_zoom_in, ax_colorbar = create_axes(title)\n",
    "    axarr = (ax_zoom_out, ax_zoom_in)\n",
    "    plot_distribution(\n",
    "        axarr[0],\n",
    "        X,\n",
    "        y,\n",
    "        hist_nbins=200,\n",
    "        x0_label=\"Revenu médian\",\n",
    "        x1_label=\"Nombre d'occupants\",\n",
    "        title=\"Données complètes\",\n",
    "    )\n",
    "\n",
    "    # Élimination des outliers\n",
    "    zoom_in_percentile_range = (0, 99)\n",
    "    cutoffs_X0 = np.percentile(X[:, 0], zoom_in_percentile_range)\n",
    "    cutoffs_X1 = np.percentile(X[:, 1], zoom_in_percentile_range)\n",
    "\n",
    "    non_outliers_mask = np.all(X > [cutoffs_X0[0], cutoffs_X1[0]], axis=1) & np.all(\n",
    "        X < [cutoffs_X0[1], cutoffs_X1[1]], axis=1\n",
    "    )\n",
    "    plot_distribution(\n",
    "        axarr[1],\n",
    "        X[non_outliers_mask],\n",
    "        y[non_outliers_mask],\n",
    "        hist_nbins=200,\n",
    "        x0_label=\"Revenu médian\",\n",
    "        x1_label=\"Nombre d'occupants\",\n",
    "        title=\"Sans outliers\",\n",
    "    )\n",
    "\n",
    "    norm = mpl.colors.Normalize(y_full.min(), y_full.max())\n",
    "    mpl.colorbar.ColorbarBase(\n",
    "        ax_colorbar,\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        orientation=\"vertical\",\n",
    "        label=\"Valeur de la maison *100 K$\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e87e3",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"80bdf6bb\" -->\n",
    "## <a id=lecture-de-la-base-de-données>Lecture de la base de données</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"c5c9cc31\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/house-image.jpeg\"  width=\"400\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://pxhere.com/cs/photo/672220</font>\n",
    "    </div>\n",
    "</div>\n",
    "<p>&nbsp;</p>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region colab_type=\"text\" id=\"a214cffc\" -->\n",
    "Celle-ci contient huit caractéristiques mesurées pour environ 21 000 zones d'habitation (*blocks*) en Californie.\n",
    "Elle a été conçue afin de prédire, entre autres, la valeur médiane des maisons en centaines de milliers\n",
    "de dollars US (en 1990).\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736689ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_california_housing()\n",
    "X_full, y_full = dataset.data, dataset.target\n",
    "\n",
    "# Description de la base de données\n",
    "print(dataset.DESCR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f4ef50",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region colab_type=\"text\" id=\"a214cffc\" -->\n",
    "## <a id=méthodes-de-transformation>Méthodes de transformation</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"cc80ade5\" -->\n",
    "### <a id=données-brutes>Données brutes</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"614603ab\" -->\n",
    "Dans les exemples qui suivent, on va comparer les distributions de deux caractéristiques.\n",
    "La caractéristique « $0$ », le revenu médian dans une zone, a une distribution\n",
    "à queue lourde (*heavy-tail distribution*). La caractéristique « $5$ », le nombre d'occupants dans\n",
    "la même zone, contient quelques données aberrantes (*outliers*) très prononcées.\n",
    "\n",
    "Toutes les figures qui suivent utilisent la convention suivante:\n",
    "\n",
    "\n",
    "- chaque panneau montre les valeurs de $(x_{0}, x_{5})$ avec un code de couleur correspondant au\n",
    "prix médian des maisons dans chaque zone,\n",
    "- le panneau de gauche montre les données transformées contenant des valeurs aberrantes,\n",
    "- le panneau de droite montre les données transformées et nettoyées des valeurs aberrantes.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadcc99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_full[:, [0, 5]]\n",
    "\n",
    "# On rééchelonne la donnée de sortie, le prix médian des maisons, entre 0 et 1 pour mieux visualiser la\n",
    "# légende (étalement de la couleur).\n",
    "y = minmax_scale(y_full)\n",
    "\n",
    "data = (\"Unscaled data\", X)\n",
    "make_plot(*data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd375c6",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"32e1e43d\" -->\n",
    "On remarque que la présence de valeurs aberrantes dans le panneau de gauche écrase les distributions\n",
    "de données et les histogrammes.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region colab_type=\"text\" id=\"216eaf3f\" -->\n",
    "### <a id=standard>Standard</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"3ece8b88\" -->\n",
    "La normalisation standard d'une variable $x_i$ se calcule comme suit\n",
    "\n",
    "$$x_i = \\dfrac{x_i - \\bar{x}}{\\sigma(x)}$$\n",
    "\n",
    "Elle consiste à enlever la moyenne de la variable et diviser le résultat par son écart-type. Elle permet de transformer\n",
    "une distribution normale/gaussienne $N(\\mu, \\sigma)$ en une autre $N(0,1)$ de moyenne nulle et de variance unité. Autrement, elle ne change pas la forme de la distribution des données. C'est la transformation la plus utilisée.\n",
    "\n",
    "Elle ne change rien à l'affichage puisque les fonctions d'affichage ajustent par défaut les bornes des figures afin de contenir l'ensemble des données. L'aspect d'une distribution de points ne change donc pas avant et après la transformation\n",
    "mis à part les bornes des axes.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3cb97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\"Data after standard scaling\", StandardScaler().fit_transform(X))\n",
    "make_plot(*data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd3d49d",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region colab_type=\"text\" id=\"59583029\" -->\n",
    "### <a id=min-max>Min-max</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"da872a92\" -->\n",
    "Cette transformation est la plus utilisée après celle de la normalisation standard. Elle force les valeurs de la\n",
    "variable $x_i$ à être distribuées entre 0 et 1. La variable transformée n'a pas nécessairement une valeur moyenne de 0 comme dans le cas précédent. Elle est aussi plus sensible aux valeurs aberrantes, car celles-ci déterminent\n",
    "les valeurs minimale et maximale de la variable qui sont utilisées dans la transformation.\n",
    "\n",
    "$$x_i = \\dfrac{x_i - \\min_i{x_i}}{\\max_i{x_i}-\\min_i{x_i}}$$\n",
    "\n",
    "\n",
    "C'est une autre transformation linéaire. Les résultats sont similaires aux précédents, mis à part les bornes des axes.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\"Data after min-max scaling\", MinMaxScaler().fit_transform(X))\n",
    "make_plot(*data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5474e36",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region colab_type=\"text\" id=\"b961d2c4\" -->\n",
    "### <a id=max-abs>Max-abs</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"587a4d09\" -->\n",
    "Cette transformation impose aux données une borne valant $-1$ si $\\max_i{x_i}<0$ ou $+1$ si $\\max_i{x_i}>0$.\n",
    "\n",
    "$$x_i = \\dfrac{x_i}{\\max_i{|x_i|}}$$\n",
    "\n",
    "C'est une autre transformation linéaire. Les résultats sont similaires aux précédents, mis à part les bornes des axes.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62722237",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\"Data after max-abs scaling\", MaxAbsScaler().fit_transform(X))\n",
    "make_plot(*data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72d3983",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region colab_type=\"text\" id=\"ea9b5565\" -->\n",
    "### <a id=robuste-basé-sur-les-quantiles-1-2-et-3>Robuste (basé sur les quantiles 1, 2 et 3)</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"32df4e47\" -->\n",
    "Cette transformation ressemble à celle la normalisation standard, où l'on a remplacé la moyenne et l'écart-type de la distribution des valeurs de la variable $x_i$ par des estimateurs plus robustes; la médiane $Q_2(x)$ et l'intervalle interquartile $Q_3(x) - Q_1(x)$.\n",
    "\n",
    "$$x_i = \\dfrac{x_i - Q_2(x)}{Q_3(x) - Q_1(x)}$$\n",
    "\n",
    "La moyenne et l'écart-type sont des estimateurs statistiques sensibles aux valeurs aberrantes. Dans ces situations, la médiane et l’intervalle interquartile donnent souvent de meilleurs résultats. La transformation résultante demeure quand même linéaire; mêmes résultats que précédemment, aux bornes près!\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3670820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    \"Data after robust scaling\",\n",
    "    RobustScaler(quantile_range=(25, 75)).fit_transform(X),\n",
    ")\n",
    "make_plot(*data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7907e80",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region colab_type=\"text\" id=\"60b532fc\" -->\n",
    "### <a id=transformation-par-puissance>Transformation par puissance</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"be749ca0\" -->\n",
    "Il existe plusieurs familles de transformations non linéaires qui permettent de transformer\n",
    "une distribution arbitraire de données en une autre approximativement gaussienne. C'est très utile\n",
    "pour traiter les problèmes liés à l'hétéroscédasticité (variance non constante) ou ceux pour lesquels\n",
    "une distribution normale des données est souhaitée. En effet, plusieurs méthodes en apprentissage automatique ont été\n",
    "développées en faisant l'hypothèse de normalité dans les données à traiter. Lorsque ce n'est pas le cas,\n",
    "elles ne performent pas très bien.\n",
    "\n",
    "Une transformation non linéaire fausse les corrélations linéaires entre les variables mesurées à la même échelle.\n",
    "Toutefois, elle rend les variables mesurées à différentes échelles plus directement comparables.\n",
    "\n",
    "L'exemple suivant utilise deux familles de transformations: Yeo-Johnson et Box-Cox. Ce sont des cas particuliers de la [transformation par puissance](https://en.wikipedia.org/wiki/Power_transform).\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    \"Données après transformation par puissance (Yeo-Johnson)\",\n",
    "    PowerTransformer(method=\"yeo-johnson\").fit_transform(X),\n",
    ")\n",
    "make_plot(*data)\n",
    "\n",
    "data = (\n",
    "    \"Données après transformation par puissance (Box-Cox)\",\n",
    "    PowerTransformer(method=\"box-cox\").fit_transform(X),\n",
    ")\n",
    "make_plot(*data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d64e5",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"d5834e5f\" -->\n",
    "Comparez ces résultats avec les précédents. Les distributions des variables transformées sont devenues presque gaussiennes.\n",
    "L'asymétrie des distributions précédentes a disparu.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region colab_type=\"text\" id=\"845ba7bf\" -->\n",
    "### <a id=transformation-quantile-à-quantile>Transformation quantile à quantile</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"fcbe9d1a\" -->\n",
    "La fonction de densité cumulée d'une variable est utilisée pour projeter les valeurs originales\n",
    "(à l'intérieur d'une plage de valeurs) vers de nouvelles valeurs ayant une distribution précise\n",
    "(gaussienne, uniforme, etc.). Les valeurs originales hors de la plage seront projetées aux limites\n",
    "de la distribution en sortie. Voilà une autre transformation non linéaire.\n",
    "\n",
    "Voyez cet [exemple interactif](https://geostatisticslessons.com/lessons/normalscore)\n",
    "où l'on transforme une variable $z$ ayant une distribution quelconque en une autre $y$ ayant\n",
    "une distribution gaussienne de moyenne $0$ et de variance $1$.\n",
    "\n",
    "L'exemple suivant transforme les distributions originales en distributions gaussiennes et uniformes.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca20ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    \"Données après transformation par quantile\" \"(distribution finale gaussienne)\",\n",
    "    QuantileTransformer(output_distribution=\"normal\").fit_transform(X),\n",
    ")\n",
    "make_plot(*data)\n",
    "\n",
    "data = (\n",
    "    \"Données après transformation par quantile\" \"(distribution finale uniforme)\",\n",
    "    QuantileTransformer(output_distribution=\"uniform\").fit_transform(X),\n",
    ")\n",
    "make_plot(*data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7708ed7",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"43c92943\" -->\n",
    "Notez à quel point les distributions finales sont presque parfaitement gaussiennes et uniformes! Cette\n",
    "approche fonctionne bien avec un grand nombre de données. Ce type de transformation est non\n",
    "linéaire et peut contenir des discontinuités. C'est un avantage sur la méthode précédente qui est\n",
    "continue, par définition.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region colab_type=\"text\" id=\"62f5c75e\" -->\n",
    "### <a id=normalisation-vectorielle>Normalisation vectorielle</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"3d451ab1\" -->\n",
    "Cette transformation est totalement différente des précédentes. Alors que ces dernières étaient appliquées une variable\n",
    "$x_i$ à la fois, celle-ci utilise la norme du vecteur $X$ en entier pour normaliser toutes les variables $x_i$ en même temps.\n",
    "\n",
    "$$x_i = \\dfrac{x_i}{ ||X|| }$$\n",
    "\n",
    "\n",
    "La mise à l'échelle des entrées, au moyen de la norme du vecteur d'entrées, est une opération courante pour la classification de texte et le regroupement des données. La figure suivante montre l'effet de la normalisation d'échelle avec des données non reliées à l'analyse de texte; c'est l'exemple des maisons californiennes.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74475a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    \"Données après transformation par normalisation L2\",\n",
    "    Normalizer().fit_transform(X),\n",
    ")\n",
    "make_plot(*data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115214b4",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"6c28e1d4\" -->\n",
    "Comment interpréter ces résultats si différents des précédents? On remarque que la transformation.\n",
    "\n",
    "$$x_i = \\frac{x_i}{ ||x_i|| }$$\n",
    "\n",
    "positionne chaque vecteur $x_i$ sur un cercle de rayon 1, dans ce cas-ci un quart de cercle puisque\n",
    "les composantes du vecteur sont positives.\n",
    "\n",
    "Quel avantage y a-t-il à faire cela? Notez que la couleur change du rouge au bleu de façon assez uniforme. Ainsi,\n",
    "le prix des maisons dans une zone augmente lorsque le revenu médian des résidents augmente et que leur\n",
    "nombre diminue. L'extrémité bleue doit correspondre au quartier de Beverly Hills, à Los Angeles, et l'extrémité\n",
    "rouge doit correspondre à un quartier défavorisé dans la même mégalopole.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region colab_type=\"text\" id=\"c7a0158a\" -->\n",
    "## <a id=comparaison-entre-les-différents-rééchantillonneurs>Comparaison entre les différents rééchantillonneurs</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"5ff86c57\" -->\n",
    "Comment comparer entre elles les différentes méthodes de transformation? Examinons comment chacune\n",
    "affecte une même caractéristique $x_i$, dans ce cas-ci le revenu médian des résidents dans chaque zone.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ef535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "dfX = dfX.drop(dataset.feature_names[1:], axis=1)\n",
    "\n",
    "col = dfX.MedInc.values.reshape(-1, 1)\n",
    "\n",
    "scalers = [\n",
    "    (\"Standard\", StandardScaler()),\n",
    "    (\"Min-max\", MinMaxScaler()),\n",
    "    (\"Max-abs\", MaxAbsScaler()),\n",
    "    (\"Robuste\", RobustScaler(quantile_range=(25, 75))),\n",
    "    (\"Quantile (gaussian)\", QuantileTransformer(output_distribution=\"normal\")),\n",
    "    (\"Quantile (uniforme)\", QuantileTransformer(output_distribution=\"uniform\")),\n",
    "]\n",
    "\n",
    "for scaler in scalers:\n",
    "    dfX[scaler[0]] = scaler[1].fit_transform(col)\n",
    "\n",
    "dfX.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65859504",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"92730133\" -->\n",
    "Pour mieux comprendre les différences entre les rééchantillonneurs, la figure suivante montre la distribution de la\n",
    "caractéristique `Revenu médian` avant et après chaque type de transformation.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e158d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt = 1\n",
    "f = plt.figure(figsize=(30, 12))\n",
    "for scaler in scalers:\n",
    "    name = scaler[0]\n",
    "    ax = f.add_subplot(2, len(scalers), cpt)\n",
    "\n",
    "    sns.histplot(dfX.MedInc, ax=ax, kde=True, color=\"b\")\n",
    "    ax.axvline(dfX.MedInc.mean(), color=\"k\", linestyle=\"dashed\", linewidth=1)\n",
    "    sns.histplot(dfX[name], ax=ax, kde=True, color=\"r\")\n",
    "    ax.axvline(dfX[name].mean(), color=\"k\", linestyle=\"dashed\", linewidth=1)\n",
    "    ax.set_title(name)\n",
    "\n",
    "    ax = f.add_subplot(2, len(scalers), cpt + len(scalers))\n",
    "    g = sns.regplot(x=\"MedInc\", y=name, data=dfX, ax=ax)\n",
    "    cpt += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b66402",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"6d2c801b\" -->\n",
    "Dans la ligne du haut, les distributions des valeurs originales apparaissent en bleu, et celles des valeurs\n",
    "transformées, en rouge. La ligne du bas montre la relation entre le `Revenu médian` original et sa valeur transformée. Les quatre\n",
    "premières transformations sont linéaires et les deux suivantes sont non linéaires.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"6946a633\" -->\n",
    "# <a id=le-traitement-des-variables-catégorielles>Le traitement des variables catégorielles</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"80f64164\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/plastic-balls.jpeg\"  width=\"500\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://pixabay.com/sv/photos/bollhav-m%c3%a5ngf%c3%a4rgad-bollar-leksak-1029865/</font>\n",
    "    </div>\n",
    "</div>\n",
    "<p>&nbsp;</p>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"18f38646\" -->\n",
    "Il est rare qu'un projet d'analyse des données\n",
    "n'utilise que des variables numériques continues (température, salaire, etc.). En pratique,\n",
    "il faut aussi tenir compte de variables catégorielles (sexe, couleur, etc.) et ordinales\n",
    "(niveau d'appréciation, échelle d'intensité des piments, etc.). On s'intéresse ici aux variables catégorielles.\n",
    "\n",
    "Comment effectuer une régression si une des variables d'entrée prend les valeurs\n",
    "$\\{'\\text{Rouge}', '\\text{Vert}', '\\text{Bleu}', '\\text{Jaune}'\\}$? Peut-on la convertir en une variable entière prenant les\n",
    "valeurs $\\{0, 1, 2, 3\\}$ afin de l'intégrer dans les calculs? Mais alors, puisque $3>2$ est-ce que cela implique que\n",
    "la couleur jaune est plus importante que la bleue? Bien sûr que non. Le traitement des variables catégorielles\n",
    "gère ce genre de situation.\n",
    "\n",
    "Nous allons voir dans ce qui suit quelques exemples de traitement de variables catégorielles\n",
    "souvent utilisés en pratique. \n",
    "\n",
    "> À noter que plusieurs d'entre eux ont été inspirés de l'aide mémoire en prétraitement des données d'\n",
    "[Elisabeth Reitmayr](https://github.com/lis365b/data_analysis/blob/master/data_prep_python_cheatsheet.md).\n",
    "Jetez un coup d'oeil pour y apprendre plein de trucs utiles.\n",
    "\n",
    "Dans les exemples qui suivent, on suppose que des données ont été enregistrées dans un tableau de données \n",
    "`df` de la librairie Pandas. Cette librairie, ainsi que d'autres en Python, sont décrites dans un autre\n",
    "module de la formation. En bref, un tableau de données est un moyen de représenter et de travailler avec des données tabulaires de divers types. Il peut être considéré comme un tableau qui organise les données en lignes et en colonnes, ce qui en fait une structure de données bidimensionnelle.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"15630ac6\" -->\n",
    "## <a id=regroupement-de-catégories-en-une-seule-valeur>Regroupement de catégories en une seule valeur</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"4b55f5c3\" -->\n",
    "Par exemple, on a une variable catégorielle qui contient des noms de pays. On veut créer une nouvelle variable « `'Filtre'` » qui\n",
    "ne garde que les trois pays les plus populeux et place les autres dans une classe « `'autre'` ».\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c803ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de remplacement\n",
    "def repl(x):\n",
    "    if x == \"USA\":\n",
    "        return \"USA\"\n",
    "    elif x == \"Chine\":\n",
    "        return \"Chine\"\n",
    "    elif x == \"Inde\":\n",
    "        return \"Inde\"\n",
    "    else:\n",
    "        return \"Autre\"\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [\"Canada\", \"USA\", \"France\", \"Chine\", \"Burundi\", \"UK\", \"Brésil\", \"Inde\"],\n",
    "    columns=[\"Pays\"],\n",
    ")\n",
    "\n",
    "df[\"Filtre\"] = df[\"Pays\"].apply(repl)\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3bd817",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"4a229698\" -->\n",
    "## <a id=regroupement-de-variables-numériques-en-catégories>Regroupement de variables numériques en catégories</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"8bc6a9b9\" -->\n",
    "### <a id=catégories-de-taille-égales>Catégories de taille égales</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"c8d5a5e7\" -->\n",
    "Par exemple, on veut convertir les notes d'un examen en trois classes $\\{\\text{'Faible'}, \\text{'Moyenne' et } \\text{'Forte'}\\}$ réparties également entre la note la plus faible et la plus forte.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c16e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([60, 50, 90, 90, 100, 40, 30, 60, 40, 70], columns=[\"Note\"])\n",
    "\n",
    "df[\"Évaluation\"] = pd.qcut(df[\"Note\"], q=3, labels=[\"Faible\", \"Moyenne\", \"Forte\"])\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9873b495",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"59deffc4\" -->\n",
    "### <a id=catégories-par-intervalles-identiques>Catégories par intervalles identiques</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"342e76d7\" -->\n",
    "Par exemple, on veut convertir des valeurs de température en cinq classes $\\{\\text{'Très froid'}, \\text{'Froid'}, \\text{'Tiède'}, \\text{'Chaud'}, \\text{'Très chaud'}\\}$ réparties dans les intervalles $[0,20[, [20,40[,\\cdots$\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b9b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([60, 50, 90, 15, 10, 45, 30, 60, 40, 75], columns=[\"Température\"])\n",
    "\n",
    "bins = [0, 20, 40, 60, 80, 100]\n",
    "df[\"Impression\"] = pd.cut(\n",
    "    df[\"Température\"],\n",
    "    bins,\n",
    "    labels=[\"Très froid\", \"Froid\", \"Tiède\", \"Chaud\", \"Très chaud\"],\n",
    ")\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d191a32",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"0075b56c\" -->\n",
    "## <a id=encoder-des-variables-catégorielles>Encoder des variables catégorielles</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"d1709424\" -->\n",
    "### <a id=encoder-une-variable-booléenne-en-la-transformant-en-entier>Encoder une variable booléenne en la transformant en entier</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"1b000784\" -->\n",
    "Par exemple, on veut convertir la variable `Sexe` contenant initialement les valeurs $\\{\\text{'Femme'}, \\text{'Homme'}\\}$ en entiers $\\{0, 1\\}$ correspondants.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\"Homme\", \"Homme\", \"Femme\", \"Femme\", \"Femme\", \"Homme\", \"Femme\", \"Femme\"],\n",
    "    columns=[\"Sexe\"],\n",
    ")\n",
    "\n",
    "df[\"Genre\"] = (df[\"Sexe\"] == \"Homme\").astype(int)\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17830fb9",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"e41f1911\" -->\n",
    "### <a id=encoder-manuellement-en-établissant-les-correspondances-avec-un-dictionnaire>Encoder manuellement en établissant les correspondances avec un dictionnaire</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"1618bf94\" -->\n",
    "Autre façon d'effectuer la même conversion.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b8226",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\"Femme\": 0, \"Homme\": 1}\n",
    "df[\"Genre\"] = df[\"Sexe\"].map(dic)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2890b7",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"8dc6997e\" -->\n",
    "### <a id=encoder-automatiquement>Encoder automatiquement</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"6f5fb49a\" -->\n",
    "Par exemple, supposons qu'une variable « `'COL'` » contienne cinq catégories $\\{\\text{'A'}, \\text{'B'}, \\text{'C'}, \\text{'D'}, \\text{'E'}\\}$. On veut la\n",
    "transformer en cinq nouvelles variables, dont chacune prendra les valeurs $\\{0, 1\\}$ selon que $\\text{COL}=\\text{'A'}$ ou non,\n",
    "et ainsi de suite pour les autres valeurs de la variable « `'COL'` ». Initialisons d'abord la variable:\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a31cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\"A\", \"A\", \"B\", \"C\", \"B\", \"E\", \"D\"], columns=[\"COL\"])\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a42c6",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"9ee9d0bf\" -->\n",
    "Transformons la variable « `'COL'` » en cinq variables factices (*dummy variables*). On peut utiliser la fonction\n",
    "[`get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) de Pandas\n",
    "ou encore la classe [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) de Scikit-learn. Utilisons la première.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda05d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.get_dummies(df[\"COL\"])\n",
    "df2.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7f0ff",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"4a3351f5\" -->\n",
    "Après ces étapes, il y a maintenant autant de colonnes dans le tableau de données `df2` que de\n",
    "catégories dans la colonne « `'COL'` ». Ces colonnes sont colinéaires; n'importe laquelle d'entre elles\n",
    "peut être déduite des quatre autres.\n",
    "\n",
    "Si nécessaire, on peut éliminer la colinéarité en laissant tomber la première colonne pour ne conserver que les quatre autres en faisant ceci:\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170f6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.get_dummies(df[\"COL\"], drop_first=True)\n",
    "df2.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c357c331",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"ecd7f568\" -->\n",
    "Cette opération peut être effectuée avec toutes les variables catégorielles d'un tableau de données. Regardons un autre exemple ou le tableau de données `df` contient maintenant une variable entière et deux catégorielles.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Sexe\": [\"H\", \"F\", \"F\", \"H\"],\n",
    "        \"Ville\": [\"Québec\", \"Québec\", \"Montréal\", \"Laval\"],\n",
    "        \"Age\": [20, 50, 30, 10],\n",
    "    }\n",
    ")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac916f",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"111ff07c\" -->\n",
    "Convertissons les deux variables catégorielles en variables factices. Notez que le nom de chaque\n",
    "variable catégorielle est maintenant utilisé comme préfixe afin de ne pas confondre les variables factices entre elles!\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060dc695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.get_dummies(df)\n",
    "df_2.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd3b16d",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"73ff4be6\" -->\n",
    "Si nécessaire, on peut à nouveau éliminer la colinéarité en laissant tomber la première variable factice de chaque variable catégorielle:\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b637b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.get_dummies(df, drop_first=True)\n",
    "df_2.head()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
