{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238208c5",
   "metadata": {},
   "source": [
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    formats: md,ipynb\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.16.0\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---\n",
    "\n",
    "<!-- #region id=\"4dd4fba5\" -->\n",
    "# Table des matières\n",
    "1. [Le traitement des données dupliquées](#le-traitement-des-données-dupliquées)\n",
    "1. [Le traitement des données aberrantes](#le-traitement-des-données-aberrantes)\n",
    "  1. [Génération des données](#génération-des-données)\n",
    "  1. [Élimination des données aberrantes](#élimination-des-données-aberrantes)\n",
    "    1. [Covariance robuste](#covariance-robuste)\n",
    "    1. [SVM monoclasse](#svm-monoclasse)\n",
    "    1. [Forêt d'isolation](#forêt-disolation)\n",
    "    1. [Facteur d'aberration local](#facteur-daberration-local)\n",
    "    1. [Test de Tukey pour les valeurs extrêmes](#test-de-tukey-pour-les-valeurs-extrêmes)\n",
    "    1. [Estimation de densité par noyau (KDE)](#estimation-de-densité-par-noyau-kde)\n",
    "1. [Conclusion](#conclusion)\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce0a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "matplotlib.rcParams[\"contour.negative_linestyle\"] = \"solid\"\n",
    "\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8cfcd8",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region colab_type=\"text\" id=\"1585634b\" -->\n",
    "Dans ce module, nous allons nous concentrer sur les deux premières étapes du prétraitement des données. La\n",
    "première concerne les données dupliquées. La seconde porte sur la détection et l'élimination\n",
    "des valeurs aberrantes dans les jeux de données.\n",
    "\n",
    "La présence de données dupliquées n'affecte pas vraiment l'entraînement d'un modèle. Toutefois, cela a des conséquences sur ses performances et l'interprétation des résultats. La solution est simple : éliminer ces valeurs.\n",
    "\n",
    "Le problème des valeurs aberrantes est plus sérieux. Il y a plusieurs raisons pour lesquelles on peut observer de\n",
    "telles valeurs dans des résultats expérimentaux. En voici quelques-unes:\n",
    "\n",
    "\n",
    "- erreurs instrumentales,\n",
    "- bruit naturel dans les données,\n",
    "- mauvaise entrée dans un fichier Excel,\n",
    "- valeur insérée intentionnellement afin de tester un algorithme,\n",
    "- nouveau phénomène!\n",
    "\n",
    "\n",
    "Le dernier point est le plus important. Il se peut que la donnée soit correcte, mais totalement\n",
    "inattendue. Avec un peu de chance, une investigation plus poussée peut lever le voile sur un nouveau phénomène scientifique! Il\n",
    "faut garder cette éventualité à l'esprit avant d'éliminer toute valeur aberrante. C'est un bel\n",
    "exemple de l'expression « Jeter le bébé avec l'eau du bain ». Le bébé est le point inattendu; l'eau\n",
    "du bain est le reste des données aberrantes.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"a2718f78\" -->\n",
    "# <a id=le-traitement-des-données-dupliquées>Le traitement des données dupliquées</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"aa42a1e5\" -->\n",
    "Cette première section se concentre sur quelques opérations de base en nettoyage de données. Dans l'exemple suivant,\n",
    "les données d'un  jeu de données ont été enregistrées dans un [`DataFrame`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) (`df`) de la librairie Pandas. L'élimination des données dupliquées (ou doublons) est une étape nécessaire qu'il faut effectuer bien avant celle de la détection des valeurs aberrantes.\n",
    "\n",
    "Lors de la collecte des données, il arrive souvent que l'on doive fusionner plusieurs ensembles de données entre eux.\n",
    "Cela risque d'entraîner la duplication de plusieurs valeurs. \n",
    "\n",
    "Ces données dupliquées peuvent fausser l'importance de certaines caractéristiques/variables dans l'ensemble de données. \n",
    "Si une caractéristique est systématiquement associée à des instances dupliquées, le modèle d'apprentissage automatique peut lui attribuer une importance supérieure à celle qu'elle mérite, ce qui entraîne une sélection ou une interprétation incorrecte de la caractéristique.\n",
    "\n",
    "De plus, les données dupliquées peuvent se retrouver dans les ensembles d'entraînement **et** de test. \n",
    "Cela augmente alors artificiellement les performances des modèles en test.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"3c8e28ca\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/alice.jpeg\"  width=\"400\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://boingboing.net/2005/10/20/disney-launches-alic.html</font>\n",
    "    </div>\n",
    "</div>\n",
    "<p>&nbsp;</p>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"c29f097c\" -->\n",
    "Les opérations suivantes sont souvent effectuées lors de cette étape de débroussaillage.\n",
    "\n",
    "**Compter les valeurs uniques dans la colonne `Nom`:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6735fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Nom').id_user.nunique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5e7839",
   "metadata": {},
   "source": [
    "\n",
    "**Montrer les valeurs dupliquées dans la colonne `Nom`:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b4c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(i for _, i in df.groupby('Nom') if len(i) > 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca58c4d",
   "metadata": {},
   "source": [
    "\n",
    "**Montrer les valeurs dupliquées dans plus d'une colonne:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(i for _, i in df.groupby(['Nom1', 'Nom2', 'Nom3']) if len(i) > 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02af4d4d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Retirer les données dupliquées dans la colonne `Nom`:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0121d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='Nom', keep=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fda881",
   "metadata": {},
   "source": [
    "\n",
    "Les doublons sont parfois faciles à repérer visuellement dans une colonne après quelques étapes de prétraitement. Le simple affichage des données d'une colonne en ordre croissant peut révéler des anomalies. Par exemple, si la colonne contient des valeurs entre 0 et 1 000 et que vous notez sept valeurs consécutives de 102,37 alors ça vaudrait la peine d'investiguer. La plupart du temps, des exemples \"différents\" deviennent des doublons après prétraitement.\n",
    "\n",
    "> À noter que ces quelques commandes très utiles ont été extraites de l'aide-mémoire en prétraitement des données d'[Elisabeth Reitmayr](https://github.com/lis365b/data_analysis/blob/master/data_prep_python_cheatsheet.md).\n",
    "Jetez un coup d'oeil pour y apprendre plein de trucs utiles.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"171a9c94\" -->\n",
    "# <a id=le-traitement-des-données-aberrantes>Le traitement des données aberrantes</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"413f6ae5\" -->\n",
    "<p>&nbsp;</p>\n",
    "<div align=\"center\">\n",
    "    <img src= \"../images/fish-outlier.png\"  width=\"500\" />\n",
    "    <div>\n",
    "    <font size=\"0.5\">Image Source: https://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561</font>\n",
    "    </div>\n",
    "</div>\n",
    "<p>&nbsp;</p>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region colab_type=\"text\" id=\"366f6882\" -->\n",
    "La séquence suivante est inspirée des [exemples de code](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_anomaly_comparison.html)\n",
    "de la librairie Scikit-learn.\n",
    "\n",
    "Nous allons commencer par générer plusieurs ensembles de données en y incluant des données aberrantes, c'est-à-dire,\n",
    "hors de la distribution des données d'intérêt. Puis, nous allons utiliser diverses techniques\n",
    "permettant (avec plus ou moins de succès) de déterminer quelles données font partie de la distribution\n",
    "initiale, et lesquelles n'en font pas partie.\n",
    "\n",
    "N'hésitez pas à jouer avec les paramètres des différents algorithmes pour voir l'implication de chacun!\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"508fb9ab\" -->\n",
    "## <a id=génération-des-données>Génération des données</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"b25f8974\" -->\n",
    "Nous allons générer cinq ensembles de données 2-D avec des distributions spatiales de complexités croissantes\n",
    "et y inclure des données aberrantes. Nous allons ensuite examiner diverses techniques permettant\n",
    "de déterminer quelles données font partie de la distribution initiale,\n",
    "et lesquelles n'en font pas partie.\n",
    "\n",
    "N'hésitez pas à jouer avec les paramètres des différents algorithmes pour voir l'implication de chacun!\n",
    "\n",
    "> À noter que dans la littérature anglophone, les bonnes données sont des *inliers* et les valeurs aberrantes\n",
    "sont des *outliers*. Pensez à une file de gens alignés. Ceux qui sont dans la ligne (*inliers*) sont\n",
    "correctement positionnés. Ceux qui en sortent (*outliers*) sont des gens qui attirent l'attention\n",
    "(*outstanding people*). Voilà l'origine de ces termes.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf1981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres des exemples\n",
    "n_samples = 300\n",
    "\n",
    "# Quantité de données aberrantes (en %)\n",
    "outliers_fraction = 0.25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea494d8",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa48866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des jeux de données avec données aberrantes selon les paramètres précédents\n",
    "\n",
    "n_outliers = int(outliers_fraction * n_samples)\n",
    "n_inliers = n_samples - n_outliers\n",
    "\n",
    "blobs_params = dict(random_state=0, n_samples=n_inliers, n_features=2)\n",
    "# Cinq datasets:\n",
    "datasets = [\n",
    "    # Un seul nuage de point (blob en anglais)\n",
    "    make_blobs(centers=[[0, 0], [0, 0]], cluster_std=0.5, **blobs_params)[0],\n",
    "    # Deux nuages de points séparés\n",
    "    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[0.5, 0.5], **blobs_params)[0],\n",
    "    # Deux nuages de points séparés dont un plus étalé\n",
    "    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[1.5, 0.3], **blobs_params)[0],\n",
    "    # Deux croissants de lune\n",
    "    4.0\n",
    "    * (\n",
    "        make_moons(n_samples=n_samples, noise=0.05, random_state=0)[0]\n",
    "        - np.array([0.5, 0.25])\n",
    "    ),\n",
    "    # Distribution aléatoire uniforme\n",
    "    14.0 * (np.random.RandomState(seed).rand(n_samples, 2) - 0.5),\n",
    "]\n",
    "\n",
    "# Grillage (mesh en anglais) pour afficher les contours de décision lorsque\n",
    "# disponibles.\n",
    "xx, yy = np.meshgrid(np.linspace(-7, 7, 150), np.linspace(-7, 7, 150))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ecbd6d",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"05cc04df\" -->\n",
    "L'affichage montre les bonnes données, les valeurs aberrantes et les contours de décision générés par certains algorithmes.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f974ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_datasets(name=\"\", algorithm=None, addOutliers=True):\n",
    "    # Taille des figures\n",
    "    plt.figure(figsize=(len(datasets) * 2 + 3, 3))\n",
    "    plt.subplots_adjust(\n",
    "        left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n",
    "    )\n",
    "\n",
    "    plot_num = 1\n",
    "    # Garantit la reproductibilité\n",
    "    rng = np.random.RandomState(42)\n",
    "\n",
    "    for i_dataset, X in enumerate(datasets):\n",
    "\n",
    "        y_pred = np.repeat(0, len(X))\n",
    "        # Ajout des valeurs aberrantes\n",
    "        if addOutliers:\n",
    "            X = np.concatenate(\n",
    "                [X, rng.uniform(low=-6, high=6, size=(n_outliers, 2))], axis=0\n",
    "            )\n",
    "\n",
    "            # On met la vérité dans la prédiction si on ne fait pas\n",
    "            # de prédiction.\n",
    "            y_pred = np.concatenate([y_pred, np.repeat(1, n_outliers)], axis=0)\n",
    "\n",
    "        t0 = 0\n",
    "        t1 = 0\n",
    "        ax = plt.subplot(1, len(datasets), plot_num)\n",
    "\n",
    "        # Si on veut voir la prédiction dans l'affichage :\n",
    "        if algorithm is not None:\n",
    "            t0 = time.time()\n",
    "            algorithm.fit(X)\n",
    "            t1 = time.time()\n",
    "            if i_dataset == 0:\n",
    "                plt.title(name, size=18)\n",
    "\n",
    "            # Traite les données et prédit si chaque point est une valeur\n",
    "            # aberrante ou non\n",
    "            if name == \"Local Outlier Factor\":\n",
    "                # LOF n'implémente pas la méthode predict.\n",
    "                y_pred = algorithm.fit_predict(X)\n",
    "            elif name == \"KDE\":\n",
    "                # KDE non plus\n",
    "                # Score samples\n",
    "                pred = np.exp(algorithm.fit(X).score_samples(X))\n",
    "                n = sum(pred < 0.05)\n",
    "                outlier_ind = np.asarray(pred).argsort()[:n]\n",
    "                y_pred = np.array(\n",
    "                    [-1 if i in outlier_ind else 1 for i in range(len(X))]\n",
    "                )\n",
    "            elif name == \"Tukey\":\n",
    "                # Tukey non plus\n",
    "                outlier_ind = algorithm.outliers(X)\n",
    "                y_pred = np.array(\n",
    "                    [-1 if i in outlier_ind else 1 for i in range(len(X))]\n",
    "                )\n",
    "            else:\n",
    "                y_pred = algorithm.fit(X).predict(X)\n",
    "                # Affichage des points et des contours de décision\n",
    "                Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "                Z = Z.reshape(xx.shape)\n",
    "                plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors=\"black\")\n",
    "\n",
    "            plt.text(\n",
    "                0.99,\n",
    "                0.01,\n",
    "                (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n",
    "                transform=plt.gca().transAxes,\n",
    "                size=15,\n",
    "                horizontalalignment=\"right\",\n",
    "            )\n",
    "\n",
    "            # Ajustement pour identifier en orange les valeurs aberrantes\n",
    "            y_pred = 1 - y_pred\n",
    "\n",
    "        colors = np.array([\"#377eb8\", \"#ff7f00\"])\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=10, color=colors[(y_pred + 1) // 2])\n",
    "        plt.xlim(-7, 7)\n",
    "        plt.ylim(-7, 7)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plot_num += 1\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c3dad",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e22df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_datasets(addOutliers=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce3782c",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"4f3e9a01\" -->\n",
    "Le dernier panneau de la figure montre une absence de distribution cohérente des points. Cette distribution aléatoire\n",
    "est utilisée afin d'observer le comportement des algorithmes de détection de valeurs aberrantes\n",
    "en l'absence de distribution cohérente sous-jacente. Les algorithmes font l'hypothèse que les vraies données ont une\n",
    "organisation corrélée spatialement et que les données aberrantes n'en ont pas.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32961250",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_datasets(addOutliers=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e1314",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"5e9761fb\" -->\n",
    "Dans la figure ci-dessus et les suivantes, les points identifiés comme valeurs aberrantes (vraies ou fausses)\n",
    "sont toujours affichés en orange. Les temps de calcul des différentes méthodes sont également\n",
    "affichés dans le coin inférieur droit de chaque panneau.\n",
    "\n",
    "Les distributions de données dans les panneaux 1, 2 et 4 devraient être les plus faciles à nettoyer des valeurs aberrantes.\n",
    "\n",
    "Le panneau 3 est plus difficile puisque les points externes du nuage de points le plus étendu se confondent\n",
    "avec des valeurs aberrantes.\n",
    "\n",
    "Le dernier panneau permettra de voir les artefacts introduits par chaque méthode de nettoyage puisqu'il n'y\n",
    "a pas de différence significative entre les deux distributions de points.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"996b9ca8\" -->\n",
    "## <a id=élimination-des-données-aberrantes>Élimination des données aberrantes</a>\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"6613d7c2\" -->\n",
    "À partir d'ici nous allons essayer différentes méthodes de détection de valeurs aberrantes.\n",
    "Pour chacune des fonctions utilisées, n'hésitez pas à vous référer à la documentation\n",
    "de [Scikit-learn](https://scikit-learn.org/stable/).\n",
    "\n",
    "Généralement, une simple recherche sur Google de la fonction (par exemple ci-dessous, cherchez\n",
    "« *EllipticEnvelope* » sur [Google](https://www.google.com/search?q=EllipticEnvelope)) vous mènera directement\n",
    "à la bonne page de documentation.\n",
    "\n",
    "La partie importante de cette documentation, qui n'est pas discutée dans ce module, est l'ensemble des\n",
    "paramètres arbitrairement fixés à leurs valeurs par défaut que vous voudriez peut-être essayer de corriger\n",
    "vu que vous connaissez déjà quelles sont les \"bonnes\" données et les \"mauvaises\" données.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region colab_type=\"text\" id=\"fe901fa8\" -->\n",
    "### <a id=covariance-robuste>Covariance robuste</a>\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cbbfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "algo = (\"Robust covariance\", EllipticEnvelope(contamination=outliers_fraction))\n",
    "\n",
    "plot_datasets(*algo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad5206",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"686917f7\" -->\n",
    "Cette fonction calcule l'ellipse de covariance entre les données. Le pourcentage de contamination de\n",
    "points aberrants dans l'ellipse peut être ajusté.\n",
    "\n",
    "L'hypothèse d'une ellipse de covariance basée sur les vraies données n'est pas très bonne ici puisque\n",
    "les données, sauf dans le premier panneau, sont tout sauf distribuées comme un nuage de point ellipsoïdal.\n",
    "\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region colab_type=\"text\" id=\"896b210a\" -->\n",
    "### <a id=svm-monoclasse>SVM monoclasse</a>\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f348f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "algo = (\"One-Class SVM\", svm.OneClassSVM(nu=outliers_fraction, kernel=\"rbf\", gamma=0.4))\n",
    "\n",
    "plot_datasets(*algo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab56e7",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"2c9e41ec\" -->\n",
    "Cette méthode est plus sélective que la précédente, car elle isole les vrais ensembles de données. Elle en ajoute\n",
    "d'autres malheureusement.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region colab_type=\"text\" id=\"6e7c9ed5\" -->\n",
    "### <a id=forêt-disolation>Forêt d'isolation</a>\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbed47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "algo = (\n",
    "    \"Isolation Forest\",\n",
    "    IsolationForest(contamination=outliers_fraction, random_state=seed),\n",
    ")\n",
    "\n",
    "plot_datasets(*algo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ffe6d2",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"733fdd09\" -->\n",
    "Cette méthode est encore meilleure et plus discriminante que la précédente.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region colab_type=\"text\" id=\"7ece0b7f\" -->\n",
    "### <a id=facteur-daberration-local>Facteur d'aberration local</a>\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922c09c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "algo = (\n",
    "    \"Local Outlier Factor\",\n",
    "    LocalOutlierFactor(n_neighbors=35, contamination=outliers_fraction),\n",
    ")\n",
    "\n",
    "plot_datasets(*algo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b83b60",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"0a6fb97c\" -->\n",
    "Cette méthode fonctionne plutôt bien; la plupart des valeurs aberrantes dans les quatre premiers panneaux sont identifiées.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region colab_type=\"text\" id=\"b610b963\" -->\n",
    "### <a id=test-de-tukey-pour-les-valeurs-extrêmes>Test de Tukey pour les valeurs extrêmes</a>\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf7c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecart = 1.5\n",
    "\n",
    "\n",
    "# Définissons la fonction qui utilise les déviations interquartiles avec les\n",
    "# quartiles 1 et 3 comme plancher et plafond.\n",
    "class Tukey:\n",
    "    def fit(self, X):\n",
    "        None\n",
    "\n",
    "    def outliers(self, x):\n",
    "        q1 = np.percentile(x, 25)\n",
    "        q3 = np.percentile(x, 75)\n",
    "        iqr = q3 - q1\n",
    "        floor = q1 - ecart * iqr\n",
    "        ceiling = q3 + ecart * iqr\n",
    "        outlier_indices = np.where((x < floor) | (x > ceiling))[0]\n",
    "        return outlier_indices\n",
    "\n",
    "\n",
    "algo = (\"Tukey\", Tukey())\n",
    "plot_datasets(*algo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc81c0",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"9fc34cf0\" -->\n",
    "Cette méthode statistique fait l'hypothèse que les vraies données sont distribuées selon une gaussienne et\n",
    "identifie comme aberrantes celles ayant une trop faible probabilité d'appartenir à la gaussienne. Le modèle\n",
    "marche bien dans le premier panneau. Dans les panneaux, les valeurs des quartiles sont surestimées comme l'est\n",
    "l'étendue de la gaussienne résultante. Aucun point n'est alors exclu de la gaussienne; il n'y a pas de\n",
    "valeurs aberrantes détectées.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region colab_type=\"text\" id=\"3840c65b\" -->\n",
    "### <a id=estimation-de-densité-par-noyau-kde>Estimation de densité par noyau (KDE)</a>\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec747b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Variez le type de kernel ci-dessous pour comparer les résultats\n",
    "# kernel : [‘gaussian’|’tophat’|’epanechnikov’|’exponential’|’linear’|’cosine’]\n",
    "\n",
    "algo = (\"KDE\", KernelDensity(bandwidth=0.2, kernel=\"gaussian\"))\n",
    "\n",
    "plot_datasets(*algo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a4645c",
   "metadata": {},
   "source": [
    "\n",
    "<!-- #region id=\"4de460d6\" -->\n",
    "Cette méthode marche bien pour les distributions gaussiennes d'une taille donnée. C'est le cas des blobs concentrés\n",
    "dans les trois premiers panneaux. Le second blob dans le panneau 3 est gaussien, mais trop étendu pour être sélectionné.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"4b2fd1d4\" -->\n",
    "# <a id=conclusion>Conclusion</a>\n",
    "\n",
    "Les résultats de cette section montrent que l'identification des valeurs aberrantes n'est pas une opération triviale.\n",
    "Les méthodes de la forêt d'isolation et du facteur d'aberration local performent le mieux avec les jeux de données utilisés ici. Il va donc falloir souvent tester plusieurs méthodes avec vos problèmes pour voir laquelle performe le mieux entre deux ou trois méthodes.\n",
    "<!-- #endregion -->\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
