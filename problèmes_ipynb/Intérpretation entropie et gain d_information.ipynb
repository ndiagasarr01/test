{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73df9cb1",
   "metadata": {},
   "source": [
    "---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.16.0\n",
    "  kernelspec:\n",
    "    display_name: Python 3 (ipykernel)\n",
    "    language: python\n",
    "    name: python3\n",
    "---\n",
    "\n",
    "<!-- #region tags=[\"problem-title\"] id=\"1354ff31\" -->\n",
    "# Interprétation du gain d'information et de l'entropie\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region tags=[\"problem-statement\"] id=\"c73a1c88\" -->\n",
    "Un arbre décisionnel est construit pour faire une classification à deux classes. Deux variables binaires nous intéressent pour créer les premières feuilles. L'entropie initiale est de 0,7 ($E_{\\text{noeuds}}$). La séparation par la première variable crée une entropie de 0,68 alors que la deuxième crée une entropie de 0,42. Sélectionnez l'affirmation qui correspond à cette situation.\n",
    "\n",
    "1. La variable 1 est la seule qui semble améliorer la séparation initiale pour la classification de la variable indépendante.\n",
    "2. La variable 2 est la seule qui semble améliorer la séparation initiale pour la classification de la variable indépendante.\n",
    "3. Les deux variables permettent d'améliorer la classification, mais la première semble être meilleur.\n",
    "4. Les deux variables permettent d'améliorer la classification, mais la deuxième semble être meilleur.\n",
    "5. Les deux variables ne semblent pas permettent d'améliorer la classification.\n",
    "<!-- #endregion -->\n",
    "\n",
    "<!-- #region id=\"e1106fd0\" -->\n",
    "## Entrez votre solution dans la cellule ci-dessous\n",
    "Faites votre choix en entrant son numéro dans la cellule ci-dessous, puis pressez le bouton <button class=\"btn btn-xs btn-success disabled\">Soumettre</button> dans le haut du notebook.\n",
    "<!-- #endregion -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3dce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac6b3f",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37bf49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3948d79",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9275699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable default pep-8 and code metrics verifications\n",
    "grader.enable_pep8_checks(False)\n",
    "grader.enable_code_metrics(False)\n",
    "\n",
    "# run grader on multiple choice question\n",
    "grader.log = grader.grade_multiple_choice_question(\n",
    "    [\n",
    "        {\n",
    "            \"value\": 1,\n",
    "            \"score\": 0,\n",
    "            \"feedback\": \"Le gain d'information est l'entropie de l'arbre à la hauteur du noeud auquel on soustrait l'entropie de l'arbre à la hauteur des feuilles qui sont créées par la séparation. Dans ce cas, les deux séparations seront positives, mais la deuxième sera plus grande.\",\n",
    "        },\n",
    "        {\n",
    "            \"value\": 2,\n",
    "            \"score\": 0,\n",
    "            \"feedback\": \"Le gain d'information est l'entropie de l'arbre à la hauteur du noeud auquel on soustrait l'entropie de l'arbre à la hauteur des feuilles qui sont créées par la séparation. Dans ce cas, les deux séparations seront positives, mais la deuxième sera plus grande.\",\n",
    "        },\n",
    "        {\n",
    "            \"value\": 3,\n",
    "            \"score\": 0,\n",
    "            \"feedback\": \"Le gain d'information est l'entropie de l'arbre à la hauteur du noeud auquel on soustrait l'entropie de l'arbre à la hauteur des feuilles qui sont créées par la séparation. Dans ce cas, les deux séparations seront positives, mais la deuxième sera plus grande.\",\n",
    "        },\n",
    "        {\n",
    "            \"value\": 4,\n",
    "            \"score\": 1,\n",
    "            \"feedback\": \"En effet, on essaye de maximiser le gain d'information. Le gain d'information est l'entropie de l'arbre à la hauteur du noeud auquel on soustrait l'entropie de l'arbre à la hauteur des feuilles qui sont créées par la séparation. Dans ce cas, les deux séparations seront positives, mais la deuxième sera plus grande.\",\n",
    "        },\n",
    "        {\n",
    "            \"value\": 5,\n",
    "            \"score\": 0,\n",
    "            \"feedback\": \"Le gain d'information est l'entropie de l'arbre à la hauteur du noeud auquel on soustrait l'entropie de l'arbre à la hauteur des feuilles qui sont créées par la séparation. Dans ce cas, les deux séparations seront positives, mais la deuxième sera plus grande.\",\n",
    "        },\n",
    "    ],\n",
    "    value_type=int,\n",
    "    weight=100,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
